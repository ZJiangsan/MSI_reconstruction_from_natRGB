{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "True2msi_MraeSidLoss_UAV_MaizeAll_HSCNNR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFWHFnJO4i3o",
        "outputId": "17ab1786-be48-4d75-ecee-13c25333f1e3"
      },
      "source": [
        "!pip install hdf5storage\n",
        "!pip3 install torchvision\n",
        "!pip install tensorboardX\n",
        "!pip install --pre torch -f  https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200626%2Bcu101-cp36-cp36m-linux_x86_64.whl\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hdf5storage\n",
            "  Downloading hdf5storage-0.1.18-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.1 in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.1->hdf5storage) (1.5.2)\n",
            "Installing collected packages: hdf5storage\n",
            "Successfully installed hdf5storage-0.1.18\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200626%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVdWVJYaR_kD"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuO41DNu4dBG",
        "outputId": "7d2e948e-d0ab-430c-e139-cfe1dd08ae72"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/\")\n",
        "path=os.getcwd() "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duKaP9IE4i6_"
      },
      "source": [
        "### \n",
        "from __future__ import division\n",
        "from scipy import interpolate\n",
        "import random\n",
        "import os\n",
        "import os.path\n",
        "import h5py\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "import scipy.io\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from matplotlib.figure import Figure\n",
        "from IPython.display import clear_output \n",
        "\n",
        "import torchvision.utils as utils\n",
        "from tensorboardX import SummaryWriter\n",
        "###\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as udata\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.utils as utils\n",
        "import time\n",
        "import scipy.io as sio\n",
        "import logging\n",
        "import hdf5storage\n",
        "import datetime\n",
        "from math import sqrt\n",
        "%matplotlib inline\n",
        "import scipy.io as spio\n",
        "from scipy.interpolate import PchipInterpolator\n",
        "from bisect import bisect\n",
        "from google.colab import output\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.pylab import cm\n",
        "##\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def save_checkpoint_sp(model_path, epoch, iteration, model, optimizer, evaluation):\n",
        "    \"\"\"Save the checkpoint.\"\"\"\n",
        "    state = {\n",
        "            'epoch': epoch,\n",
        "            'iter': iteration,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer' : optimizer.state_dict(),\n",
        "            }\n",
        "    \n",
        "    torch.save(state, os.path.join(model_path, 'hscnn_6layer_dim10_{}.pkl'.format(evaluation)))\n",
        "\n",
        "def plot_spectrum(real, fake, epoch, i):\n",
        "    x =np.linspace(400, 900, 5, endpoint=True) # the wavebands of the hyperspectral image\n",
        "    fig = Figure()\n",
        "    canvas = FigureCanvasAgg(fig)\n",
        "    ax = fig.gca()\n",
        "    #ax.set_ylim(0, 1)\n",
        "    plot_real,  = ax.plot(x, real, 'ko-')\n",
        "    plot_fake,  = ax.plot(x, fake, 'r.-')\n",
        "    fig.legend((plot_real,plot_fake), ('real', 'fake'))\n",
        "    canvas.draw()\n",
        "    fig.savefig(os.path.join(iteration_path, \"{}_test_{}.png\".format(epoch,i)))\n",
        "    I = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
        "    I = I.reshape(canvas.get_width_height()[::-1]+(3,))\n",
        "    I = np.transpose(I, [2,0,1])\n",
        "    return np.float32(I)\n",
        "\n",
        "\n",
        "def plotwithcolorbar(img, title=None, figsize=(10,10)):\n",
        "    ''' Plot an image with a colorbar '''\n",
        "    vmin = np.min(img)\n",
        "    vmax = np.max(img)\n",
        "    hh = img.shape[1]\n",
        "    fig, axis = plt.subplots(1, 1, figsize=figsize)\n",
        "    rad2 = axis.imshow(img, vmin=vmin, vmax=vmax)\n",
        "    axis.set_title(title)\n",
        "    divider = make_axes_locatable(axis)\n",
        "    cax = divider.append_axes(\"right\", size=\"3%\", pad=0.05)\n",
        "    fig.colorbar(rad2, cax=cax)\n",
        "    circle_b = plt.Circle((int(hh/2), int(hh/2)), 5, color='b', fill=False)\n",
        "    axis.add_artist(circle_b)\n",
        "    plt.close(fig)\n",
        "\n",
        "    return fig, axis\n",
        "\n",
        "def initialize_logger(file_dir):\n",
        "    \"\"\"Print the results in the log file.\"\"\"\n",
        "    logger = logging.getLogger()\n",
        "    fhandler = logging.FileHandler(filename=file_dir, mode='a')\n",
        "    formatter = logging.Formatter('%(asctime)s - %(message)s',\"%Y-%m-%d %H:%M:%S\")\n",
        "    fhandler.setFormatter(formatter)\n",
        "    logger.addHandler(fhandler)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    return logger\n",
        "\n",
        "def save_matv73(mat_name, var_name, var):\n",
        "    hdf5storage.savemat(mat_name, {var_name: var}, format='7.3', store_python_metadata=True)\n",
        "\n",
        "\n",
        "def mrae_loss(im_true, im_fake):\n",
        "    error = torch.abs(im_fake-im_true)/im_true\n",
        "    rrmse = torch.mean(error.reshape(-1))\n",
        "    return rrmse\n",
        "\n",
        "def sam_loss(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    nom = torch.sum( torch.mul(im_true, im_fake), dim=1)\n",
        "    denom1 = torch.sqrt( torch.sum( torch.pow(im_true,2), dim=1))\n",
        "    denom2 = torch.sqrt( torch.sum( torch.pow(im_fake,2), dim=1))\n",
        "    sam = torch.acos(torch.div(nom, torch.mul(denom1, denom2)).clamp(-1.0 + 1e-8, 1.0 - 1e-8))\n",
        "    sam = torch.mul(torch.div(sam, np.pi), 180)\n",
        "\n",
        "    sam = torch.div(torch.sum(sam), N*H*W)\n",
        "\n",
        "    return sam\n",
        "\n",
        "def sid_loss(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    denom1 = torch.sqrt( torch.sum( torch.pow(im_true,2), dim=1))\n",
        "    denom2 = torch.sqrt( torch.sum( torch.pow(im_fake,2), dim=1))\n",
        "    #\n",
        "    unit_t = torch.div(im_true, denom1.unsqueeze(1))\n",
        "    uint_f = torch.div(im_fake, denom2.unsqueeze(1))\n",
        "    #\n",
        "    sid = ((unit_t - uint_f)* (unit_t.log() - uint_f.log())).sum() / (N*H*W)\n",
        "    return sid\n",
        "####\n",
        "## new fucntion for process data\n",
        "path=os.getcwd() \n",
        "\n",
        "##. with normalization\n",
        "def normalize(data):\n",
        "    data_nl = data/np.amax(data)\n",
        "    return data_nl\n",
        "##\n",
        "def gen_random_scale_n(img, rnd=3):\n",
        "    np.random.seed(rnd)\n",
        "    scale = np.random.uniform(0.1, 1.91, (1,1, img.shape[2],img.shape[3]))   \n",
        "    return img*scale\n",
        "##\n",
        "def data_process_list_n(path=path):\n",
        "    NO_ = 1\n",
        "    hyper_f = os.path.join(path,'Maize2018_ortho_msi_cropped_04052021')\n",
        "    sub_h5_fd = next(os.walk(hyper_f))[1]\n",
        "    sub_h5_fd.sort()\n",
        "    #\n",
        "    #rgb_f = hyper_f.replace(\"h5\", \"rgb\")\n",
        "    rgb_f = os.path.join(path,'Maize2018_ortho_rgb_cropped_04052021_NEW')\n",
        "    sub_rgb_fd = next(os.walk(rgb_f))[1]\n",
        "    sub_rgb_fd.sort()\n",
        "\n",
        "    for sf in range(len(sub_h5_fd)):\n",
        "    #for sf in [0,1,2,3]:\n",
        "        id_n = sub_h5_fd[sf].split(\"_\")[2][4:]\n",
        "        #\n",
        "        filenames_hyper = glob.glob(os.path.join(hyper_f,sub_h5_fd[sf],'*.h5'))\n",
        "        filenames_rgb = glob.glob(os.path.join(rgb_f, sub_rgb_fd[sf].replace(\"h5\", \"rgb\"),'*.png'))\n",
        "        filenames_hyper.sort()\n",
        "        print(filenames_hyper)\n",
        "        filenames_rgb.sort()\n",
        "        print(filenames_rgb)\n",
        "        #h5f = h5py.File('train_uavf3.h5', 'w')  ## the file concatenated hsi and rgb in one file\n",
        "        h5f = h5py.File('train_040521_maize_NatColor_finalN_{}.h5'.format(id_n), 'w')  ## the file concatenated hsi and rgb in one file\n",
        "        \n",
        "        for i in range(len(filenames_hyper)):\n",
        "            print(\"\\n\")\n",
        "            print(filenames_hyper[i], filenames_rgb[i])\n",
        "            # load hyperspectral image\n",
        "            mat =  h5py.File(filenames_hyper[i],'r')\n",
        "            hyper = np.float32(np.array(mat['img']))\n",
        "            print((\"hyper_shape\",hyper.shape))\n",
        "            hyper = np.transpose(hyper, [2,0,1])#/32768.0 # because it 32768.0 was divided already before cropping into 512x512 squares\n",
        "            #hyper = normalize(hyper)\n",
        "            print(\"hyper_test_max\", np.amax(hyper))\n",
        "            print(\"hyper_test_min\",np.amin(hyper))\n",
        "            mat.close()\n",
        "            # load rgb image\n",
        "            rgb =  cv2.imread(filenames_rgb[i])\n",
        "            rgb=cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            rgb = np.float32(np.transpose(rgb, [2,0,1])/255.0) # change to 0-1 range\n",
        "            print(\"rgb_test_max\", np.amax(np.float32(rgb)))\n",
        "            print(\"rgb_test_min\", np.amin(np.float32(rgb)))\n",
        "            data = np.concatenate((hyper,rgb), 0)\n",
        "            h5f.create_dataset(str(NO_), data=data)\n",
        "            NO_ += 1\n",
        "        h5f.close()\n",
        "        print(\"NO. of samples: {}\".format(NO_-1))\n",
        "##\n",
        "class HyperDataset_list_maize_RGB(udata.Dataset):\n",
        "    def __init__(self, crop_size=64):\n",
        "        self.crop_size = crop_size\n",
        "        xx = [\"train_040521_maize_NatColor_finalN_1030.h5\",\"train_040521_maize_NatColor_finalN_1109.h5\", \"train_040521_maize_NatColor_finalN_1119.h5\",\"train_040521_maize_NatColor_finalN_1220.h5\"] #\"train_uavf_maize_NatColor_1109.h5\" was removed because of over exposure\n",
        "        xx.sort()\n",
        "        # print(xx)\n",
        "        key_is = []\n",
        "        for i in range(len(xx)):\n",
        "            f_i = xx[i]\n",
        "            h5f_i = h5py.File(f_i, 'r')\n",
        "            keys_i = list(h5f_i.keys())\n",
        "            keys_i.sort()\n",
        "            key_i_n  = [\"_\".join([str(i), x]) for x in keys_i]\n",
        "            key_is = key_is + key_i_n\n",
        "            h5f_i.close()\n",
        "        \n",
        "        self.keys = key_is\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "    def __getitem__(self, index):\n",
        "        xx = [\"train_040521_maize_NatColor_finalN_1030.h5\",\"train_040521_maize_NatColor_finalN_1109.h5\", \"train_040521_maize_NatColor_finalN_1119.h5\",\"train_040521_maize_NatColor_finalN_1220.h5\"] #\"train_uavf_maize_NatColor_1109.h5\" was removed because of over exposure\n",
        "        xx.sort()\n",
        "        key = str(self.keys[index])\n",
        "        key_path_i = key.split(\"_\")[0]\n",
        "        key_key = key.split(\"_\")[1]\n",
        "        h5f_a = h5py.File(xx[int(key_path_i)], 'r')\n",
        "        #\n",
        "        data_a = np.array(h5f_a[key_key])\n",
        "        data = torch.as_tensor(data_a)\n",
        "        # crop\n",
        "        w = int(data.size()[1])\n",
        "        h = int(data.size()[2])\n",
        "        th, tw = self.crop_size, self.crop_size\n",
        "        if w > tw or h > th:\n",
        "            i = 0\n",
        "            j = 0\n",
        "            data = data[:,i:i+th,j:j+tw]\n",
        "        h5f_a.close()\n",
        "        return data[0:5,:,:], data[5:8,:,:]\n",
        "\n",
        "\n",
        "def batch_MRAE(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    Itrue = im_true.clamp(0.,1.).reshape(N, C*H*W)\n",
        "    Ifake = im_fake.clamp(0.,1.).reshape(N, C*H*W)\n",
        "    mse = nn.MSELoss(reduction='none')\n",
        "    err = mse(Itrue, Ifake).sqrt_().div_(Itrue).sum(dim=1, keepdim=True).div_(C*H*W)\n",
        "    return torch.mean(err)\n",
        "\n",
        "def batch_RMSE(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    Itrue = im_true.clamp(0.,1.).reshape(N, C*H*W)\n",
        "    Ifake = im_fake.clamp(0.,1.).reshape(N, C*H*W)\n",
        "    mse = nn.MSELoss(reduction='none')\n",
        "    err = mse(Itrue, Ifake).sum(dim=1, keepdim=True).div_(C*H*W).sqrt_()\n",
        "    return torch.mean(err)\n",
        "\n",
        "def batch_SID(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    Itrue = im_true.clone().reshape(N, C, H*W)\n",
        "    Ifake = im_fake.clone().reshape(N, C, H*W)\n",
        "    #nom = torch.mul(Itrue, Ifake).sum(dim=1).reshape(N, H*W)\n",
        "    denom1 = torch.pow(Itrue,2).sum(dim=1).sqrt_().reshape(N, H*W)\n",
        "    denom2 = torch.pow(Ifake,2).sum(dim=1).sqrt_().reshape(N, H*W)\n",
        "    #\n",
        "    unit_t = torch.div(Itrue, denom1.unsqueeze(1))\n",
        "    uint_f = torch.div(Ifake, denom2.unsqueeze(1))\n",
        "    #\n",
        "    sid = ((unit_t - uint_f)* (unit_t.log() - uint_f.log())).sum(dim = 1).reshape(N, H*W)\n",
        "    sid = sid.sum() / (N*H*W)\n",
        "    return sid\n",
        "\n",
        "\n",
        "def batch_SAM(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    Itrue = im_true.clone().reshape(N, C, H*W)\n",
        "    Ifake = im_fake.clone().reshape(N, C, H*W)\n",
        "    nom = torch.mul(Itrue, Ifake).sum(dim=1).reshape(N, H*W)\n",
        "    denom1 = torch.pow(Itrue,2).sum(dim=1).sqrt_().reshape(N, H*W)\n",
        "    denom2 = torch.pow(Ifake,2).sum(dim=1).sqrt_().reshape(N, H*W)\n",
        "    sam = torch.div(nom, torch.mul(denom1, denom2)).acos_().reshape(N, H*W)\n",
        "    sam = sam / np.pi * 180\n",
        "    sam = torch.sum(sam) / (N*H*W)\n",
        "    return sam\n",
        "\n",
        "\n",
        "def weights_init_kaimingNormal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.kaiming_normal(m.weight.data, a=0.2, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.kaiming_normal(m.weight.data, a=0.2, mode='fan_in')\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal(m.weight.data, 0, 0.01)\n",
        "        nn.init.constant(m.bias.data, 0.0)\n",
        "    elif classname.find('InstanceNorm') != -1:\n",
        "        nn.init.normal(m.weight.data, 0, 0.01)\n",
        "        nn.init.constant(m.bias.data, 0.0)\n",
        "\n",
        "###\n",
        "class conv_relu_res_relu_block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(conv_relu_res_relu_block, self).__init__()\n",
        "        self.conv1 = conv3x3(64, 64)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(64, 64)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = torch.add(out,residual) \n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "    \n",
        "#####             resblock\n",
        "\n",
        "def conv3x3(in_channels, out_channels):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                     stride=1, padding=1, bias=True)\n",
        "    \n",
        "class resblock(nn.Module):\n",
        "    def __init__(self, block, block_num, input_channel, output_channel):\n",
        "        super(resblock, self).__init__()\n",
        "\n",
        "        self.in_channels = input_channel\n",
        "        self.out_channels = output_channel\n",
        "        self.input_conv = conv3x3(self.in_channels, out_channels=64)  \n",
        "        self.conv_seq = self.make_layer(block, block_num)\n",
        "        self.conv = conv3x3(64, 64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.output_conv = conv3x3(in_channels=64,  out_channels=self.out_channels)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n=m.kernel_size[0]*m.kernel_size[1]*m.out_channels\n",
        "                m.weight.data.normal_(0,sqrt(2./n))\n",
        "                \n",
        "    def make_layer(self,block,num_layers):\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(block()) # there is a () \n",
        "        return nn.Sequential(*layers)   \n",
        "    \n",
        "    def forward(self, x):\n",
        "       \n",
        "        out = self.input_conv(x)\n",
        "        residual = out\n",
        "        out = self.conv_seq(out)\n",
        "        out = self.conv(out)\n",
        "        out = torch.add(out,residual)  \n",
        "        out = self.relu(out)\n",
        "        out = self.output_conv(out)\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URuT373IXt6l"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6swvuIE-Lhm"
      },
      "source": [
        "## process the data\n",
        "#data_process_list_n(path=path) \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERfixWWgWwjh"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXBwkZw2xBA9"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_V14DTU_2du"
      },
      "source": [
        "##. Only image normalization\n",
        "\n",
        "# Training \n",
        "def train(train_data_loader, model, criterion_1, criterion_2, optimizer, iteration, init_lr ,epoch, w1, w2):\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    average_MRAE = 0.\n",
        "    average_RMSE = 0. \n",
        "    average_SAM = 0.\n",
        "    average_SID =0.\n",
        "    \n",
        "    num=len(train_data_loader)\n",
        "    print('num.{}'.format(num))\n",
        "    #optimizer.zero_grad()\n",
        "    \n",
        "    for i, (labels_gt, images_gt) in enumerate(train_data_loader):\n",
        "    #for i in range(num):\n",
        "        # Normalize the RGB vectors\n",
        "        images_gt = torch.as_tensor(images_gt).float().cuda()\n",
        "        images_gt = images_gt.clamp(1e-8, 1.0 - 1e-8)\n",
        "        # put it on GPU\n",
        "        labels_gt = torch.as_tensor(labels_gt).float().cuda() \n",
        "        labels_gt = labels_gt.clamp(1e-8, 1.0 - 1e-8)\n",
        "        ##\n",
        "        images = images_gt.cuda()\n",
        "        #\n",
        "        labels = labels_gt.cuda()\n",
        "        #\n",
        "        images = Variable(images, requires_grad=True)\n",
        "        labels = Variable(labels, requires_grad=True)    \n",
        "        \n",
        "        # Decaying Learning Rate\n",
        "        lr = init_lr \n",
        "        iteration = iteration + 1\n",
        "        #\n",
        "        optimizer.zero_grad()\n",
        "        hs_scaled = model.forward(images)\n",
        "        #print(hs)\n",
        "        hs_scaled[hs_scaled.isnan()] = 1e-8\n",
        "        hs_scaled = hs_scaled.clamp(1e-8, float(\"Inf\"))\n",
        "        # remove extremes\n",
        "        hs_scaled[labels==1e-8] = 1e-8\n",
        "        #\n",
        "        if w2 ==0:\n",
        "            loss_1 = criterion_1(labels, hs_scaled)\n",
        "            loss = loss_1*w1\n",
        "        else:\n",
        "            loss_1 = criterion_1(labels, hs_scaled)\n",
        "            loss_2 = criterion_2(labels, hs_scaled)\n",
        "            #\n",
        "            loss = loss_1*w1 + loss_2*w2\n",
        "        ##\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #\n",
        "        #####\n",
        "        MRAE = batch_MRAE(labels, hs_scaled)\n",
        "        RMSE = batch_RMSE(labels, hs_scaled)\n",
        "        SAM = batch_SAM(labels, hs_scaled)\n",
        "        SID = batch_SID(labels, hs_scaled)\n",
        "        #\n",
        "        average_MRAE    += MRAE.item()\n",
        "        average_RMSE    += RMSE.item()\n",
        "        average_SAM    += SAM.item()\n",
        "        average_SID    += SID.item()\n",
        "        #\n",
        "        losses.update(loss.item())\n",
        "    return losses.avg, average_MRAE/num, average_RMSE/num, average_SAM/num, average_SID/num, iteration, lr\n",
        "\n",
        "# Validation\n",
        "\n",
        "def validate(val_loader, model, criterion_1, criterion_2, epoch, w1, w2):\n",
        "    \n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    xxx=0\n",
        "    num = len(val_loader)\n",
        "    \n",
        "    average_MRAE    =0.\n",
        "    average_RMSE    =0.\n",
        "    average_SAM    =0.\n",
        "    average_SID    =0.\n",
        "    \n",
        "    for i, (target_gt, input_gt) in enumerate(val_loader):\n",
        "        # Normalize the RGB vectors\n",
        "        input_gt = torch.as_tensor(input_gt).float().cuda() \n",
        "        input_gt = input_gt.clamp(1e-8, 1.0 - 1e-8)\n",
        "\n",
        "        input = input_gt.cuda()\n",
        "        target = torch.as_tensor(target_gt).float().cuda() \n",
        "        target = target.clamp(1e-8, 1.0 - 1e-8)\n",
        "        #\n",
        "        target = target.cuda(non_blocking=True)\n",
        "        #\n",
        "        input_var = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "    \n",
        "        # compute hs\n",
        "        with torch.no_grad():\n",
        "            hs_scaled = model.forward(input_var)\n",
        "        # normalize the prediction vector\n",
        "        hs_scaled[hs_scaled.isnan()] = 1e-8\n",
        "        hs_scaled = hs_scaled.clamp(1e-8, float(\"Inf\"))\n",
        "        #\n",
        "        # remove extremes\n",
        "        hs_scaled[target_var==1e-8] = 1e-8\n",
        "        #\n",
        "        #loss = criterion(target_var, hs_scaled)\n",
        "        #\n",
        "        if w2 ==0:\n",
        "            loss_1 = criterion_1(target_var, hs_scaled)\n",
        "            loss = loss_1*w1\n",
        "        else:\n",
        "            loss_1 = criterion_1(target_var, hs_scaled)\n",
        "            loss_2 = criterion_2(target_var, hs_scaled)\n",
        "            #\n",
        "            loss = loss_1*1 + loss_2*1 \n",
        "        #####\n",
        "        MRAE = batch_MRAE(target_var, hs_scaled)\n",
        "        RMSE = batch_RMSE(target_var, hs_scaled)\n",
        "        SAM = batch_SAM(target_var, hs_scaled)\n",
        "        SID = batch_SID(target_var, hs_scaled)\n",
        "        #\n",
        "        average_MRAE    += MRAE.item()\n",
        "        average_RMSE    += RMSE.item()\n",
        "        average_SAM    += SAM.item()\n",
        "        average_SID    += SID.item()\n",
        "        #####\n",
        "        ## generate a figure compare the reconstructed spectra and ground truth, every epoch\n",
        "        if epoch%1==0:\n",
        "            if (i+1)%10==0:\n",
        "                xxx += 1\n",
        "                H = target_var.size()[2]\n",
        "                W = target_var.size()[3]\n",
        "                real_spectrum = target_var.data.cpu().numpy()[0,:,int(H/2),int(W/2)]\n",
        "                fake_spectrum = hs_scaled.data.cpu().numpy()[0,:,int(H/2),int(W/2)]\n",
        "                I_spectrum = plot_spectrum(real_spectrum, fake_spectrum, \"val_\"+str(epoch),i)\n",
        "                ##\n",
        "                input_gt_plot = input_gt[0,:,:,:].squeeze().permute(1,2,0).data.cpu().numpy()\n",
        "                fig, axis = plotwithcolorbar(input_gt_plot, \"fake image\" )\n",
        "                fig.savefig(os.path.join(iteration_path, \"{}_test_fake_img_{}.png\".format(epoch,i)))\n",
        "                ##\n",
        "                print(\"val_num: \" + str(xxx))\n",
        "        #  record loss\n",
        "        losses.update(loss.item())\n",
        "    return losses.avg, average_MRAE/num, average_RMSE/num, average_SAM/num, average_SID/num\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh0EVJZ8obC8",
        "outputId": "27b3a544-ad74-4e1f-ac31-4b8c0bbdfe5c"
      },
      "source": [
        "###               the best one at the moment\n",
        "# on cases of combined loss functions, try one first and them add the other one \n",
        "\n",
        "##                 the main training part (splitting data to trainng and testing)\n",
        "loss_type = [\"MraeWSid\"]#[\"mrae\", \"rmse\"]\n",
        "for lt in loss_type:\n",
        "    iteration_folder = \"UAV_MaizeAll_jsZ_{}Loss_BV_tcRGB2msi_HSCNNR_iteration\".format(lt)\n",
        "    model_folder = \"UAV_MaizeAll_jsZ_{}Loss_BV_tcRGB2msi_HSCNNR_models\".format(lt)\n",
        "    #loss_type = \"mrae\" #\"mrae\"\n",
        "    iteration_path = os.path.join(os.getcwd(), iteration_folder)\n",
        "    if not os.path.exists(iteration_path):\n",
        "        os.makedirs(iteration_path)\n",
        "    ## new plot function with customed saving path\n",
        "    \n",
        "    cudnn.benchmark = True\n",
        "    ## network architecture\n",
        "    rgb_features = 3\n",
        "    hyper_features = 5\n",
        "    ## load dataset\n",
        "    print(\"\\nloading dataset ...\\n\")\n",
        "    #\n",
        "    trainDataset = HyperDataset_list_maize_RGB(crop_size=600)  ## here not the training data but the whole data set for this work\n",
        "    # set the ratio of training and validation set\n",
        "    validation_split = (1/3)\n",
        "    \n",
        "    dataset_len = len(trainDataset) #trainDataset\n",
        "    indices = list(range(dataset_len))\n",
        "    \n",
        "    # Randomly splitting indices:\n",
        "    val_len = int(np.ceil(validation_split * dataset_len))\n",
        "    validation_idx = np.random.choice(indices, size=val_len, replace=False)\n",
        "    train_idx = list(set(indices) - set(validation_idx))\n",
        "    \n",
        "    ## Defining the samplers for each phase based on the random indices:\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    validation_sampler = SubsetRandomSampler(validation_idx)\n",
        "    \n",
        "    # Data Loader (Input Pipeline)\n",
        "    train_data_loader = DataLoader(dataset=trainDataset,\n",
        "                                    sampler=train_sampler,\n",
        "                                    num_workers=1,  \n",
        "                                    batch_size=1,\n",
        "                                    shuffle=False,\n",
        "                                    pin_memory=True)\n",
        "    \n",
        "    val_loader = DataLoader(dataset=trainDataset,\n",
        "                            sampler=validation_sampler,\n",
        "                            num_workers=1, \n",
        "                            batch_size=1,\n",
        "                            shuffle=False,\n",
        "                            pin_memory=True)\n",
        "    # Model               \n",
        "    model = resblock(conv_relu_res_relu_block, 3, 3,5)\n",
        "    #\n",
        "    model.apply(weights_init_kaimingNormal)\n",
        "    # check whether it is possible to do multi-GPU processing\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    \n",
        "    # Parameters, Loss and Optimizer\n",
        "    start_epoch = 0\n",
        "    end_epoch = 10000\n",
        "    init_lr = 1e-4\n",
        "    \n",
        "    iteration = 0\n",
        "    \n",
        "    ## set the original values for the evaluation\n",
        "    #\n",
        "    record_ts_loss= np.inf\n",
        "    record_ts_MRAE = np.inf\n",
        "    record_ts_RMSE = np.inf\n",
        "    record_ts_SAM = np.inf\n",
        "    record_ts_SID = np.inf #np.inf\n",
        "\n",
        "    criterion_1 = mrae_loss #nn.MSELoss()\n",
        "    criterion_2 = sid_loss #nn.MSELoss()\n",
        "    #\n",
        "    w1 = 1\n",
        "    w2 = 0\n",
        "    optimizer=torch.optim.Adamax(model.parameters(), lr=init_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n",
        "    \n",
        "    model_path = os.path.join(os.getcwd(), model_folder)\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "    loss_csv = open(os.path.join(model_path, '{}_loss.csv'.format(lt)), 'w+')\n",
        "    \n",
        "    log_dir = os.path.join(model_path,'{}_train.log'.format(lt))\n",
        "    logger = initialize_logger(log_dir)\n",
        "    # load the trained model weights if you have\n",
        "    #resume_file = os.path.join(\"UAV_MaizeAll_jsZ_MraeWSidLoss_BV_tcRGB2msi_HSCNNR_models\", \"hscnn_6layer_dim10_ts_loss.pkl\")\n",
        "    resume_file = \"\" \n",
        "    #\n",
        "    if resume_file:\n",
        "        print(\"=> loading checkpoint '{}'\".format(resume_file))\n",
        "        checkpoint = torch.load(resume_file)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        iteration = checkpoint['iter']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        \n",
        "    for epoch in range(start_epoch + 1, end_epoch): ## specify the number of epochs to run\n",
        "        \n",
        "        start_time = time.time()         \n",
        "        tr_loss, tr_MRAE, tr_RMSE, tr_SAM, tr_SID, iteration, lr = train(train_data_loader, model, criterion_1, criterion_2, optimizer, iteration, init_lr, epoch, w1,w2)\n",
        "        #\n",
        "        end_time = time.time() # only record the training time\n",
        "        epoch_time = end_time - start_time\n",
        "        print(datetime.datetime.now())\n",
        "        #lr=init_lr\n",
        "        ts_loss, ts_MRAE, ts_RMSE,ts_SAM, ts_SID = validate(val_loader, model, criterion_1, criterion_2, epoch, w1,w2)\n",
        "        #\n",
        "        #           save a model based on total loss\n",
        "        if ts_loss < record_ts_loss: \n",
        "            record_ts_loss = ts_loss\n",
        "            save_checkpoint_sp(model_path, epoch, iteration, model, optimizer, \"ts_loss\")\n",
        "            record_ts_MRAE_loss = ts_MRAE\n",
        "            record_ts_RMSE_loss = ts_RMSE\n",
        "            record_ts_SAM_loss = ts_SAM\n",
        "            record_ts_SID_loss = ts_SID\n",
        "            lr_loss = init_lr\n",
        "            print(\"ts_loss\")\n",
        "            print(\"Saving total loss\", \"updated_ts_loss\", record_ts_loss,\"updated_ts_MRAE\", record_ts_MRAE_loss, \"updated_ts_RMSE\",record_ts_RMSE_loss, \"updated_ts_SAM\",record_ts_SAM_loss, \"updated_ts_SID\",record_ts_SID_loss, \"LR\", lr_loss)\n",
        "        #           save a model based on MRAE loss\n",
        "        if ts_MRAE < record_ts_MRAE:  \n",
        "            record_ts_MRAE = ts_MRAE\n",
        "            save_checkpoint_sp(model_path, epoch, iteration, model, optimizer, \"ts_mrae\")\n",
        "            record_ts_loss_MRAE = ts_loss\n",
        "            record_ts_RMSE = ts_RMSE\n",
        "            record_ts_SAM_MRAE = ts_SAM\n",
        "            record_ts_SID_MRAE = ts_SID\n",
        "            lr_mrae = init_lr\n",
        "            print(\"ts_MRAE\")\n",
        "            print(\"Saving MRAE\", \"updated_ts_loss\", record_ts_loss_MRAE,\"updated_ts_MRAE\", record_ts_MRAE, \"updated_ts_RMSE\",record_ts_RMSE, \"updated_ts_SAM\",record_ts_SAM_MRAE, \"updated_ts_SID\",record_ts_SID_MRAE, \"LR\", lr_mrae)\n",
        "        #           save a model based on SID loss\n",
        "        if ts_SID < record_ts_SID:  \n",
        "            record_ts_SID = ts_SID\n",
        "            save_checkpoint_sp(model_path, epoch, iteration, model, optimizer, \"ts_sid\")\n",
        "            record_ts_loss_SID = ts_loss\n",
        "            record_ts_MRAE_SID = ts_MRAE\n",
        "            record_ts_RMSE_SID = ts_RMSE\n",
        "            record_ts_SAM = ts_SAM\n",
        "            lr_sid = init_lr\n",
        "            print(\"ts_SID\")\n",
        "            print(\"Saving SID\", \"updated_ts_loss\", record_ts_loss_SID,\"updated_ts_MRAE\", record_ts_MRAE_SID, \"updated_ts_RMSE\",record_ts_RMSE_SID, \"updated_ts_SAM\",record_ts_SAM, \"updated_ts_SID\",record_ts_SID, \"LR\", lr_sid)\n",
        "        #\n",
        "        # (1) ts_loss evaluation\n",
        "        print (\"Epoch:{}, Iter:{}, Time:{}, learning rate:{}, Train loss:{}, Train MRAE:{}, Train RMSE:{}, Train SAM:{}, Train SID:{}\".format(\n",
        "            epoch, iteration, epoch_time, lr, tr_loss, tr_MRAE, tr_RMSE, tr_SAM, tr_SID))\n",
        "        print (\"Test loss:{}, Test MRAE:{}, Test RMSE:{}, Test SAM:{}, Test SID:{}\".format(ts_loss, ts_MRAE, ts_RMSE, ts_SAM, ts_SID))\n",
        "\n",
        "        logger.info(\"Epoch{}, Iter{}, Time:{}, learning rate:{}, Train loss:{},Train MRAE:{},Train RMSE:{}, Train SAM:{}, Train SID:{}, Test loss:{},Test MRAE:{},Test RMSE:{}, Test SAM:{}, Test SID:{}\".format(\n",
        "            epoch, iteration, epoch_time, lr, tr_loss, tr_MRAE,tr_RMSE, tr_SAM, tr_SID, ts_loss, ts_MRAE,ts_RMSE,ts_SAM, ts_SID))\n",
        "        ###\n",
        "        print(\"w1: {}, w2: {}\".format(w1,w2))\n",
        "        if (epoch +1)%50 ==0:\n",
        "            init_lr = init_lr*0.98\n",
        "            print((\"updated learning rate\", init_lr))\n",
        "        if (epoch+1) % 600 ==0:\n",
        "            clear_output()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "loading dataset ...\n",
            "\n",
            "num.101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:324: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-05 18:16:48.565337\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8988710489927554 updated_ts_MRAE 0.8987465652765012 updated_ts_RMSE 0.15386322988014595 updated_ts_SAM 41.386617286532534 updated_ts_SID 20.123267884347953 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8988710489927554 updated_ts_MRAE 0.8987465652765012 updated_ts_RMSE 0.15386322988014595 updated_ts_SAM 41.386617286532534 updated_ts_SID 20.123267884347953 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.8988710489927554 updated_ts_MRAE 0.8987465652765012 updated_ts_RMSE 0.15386322988014595 updated_ts_SAM 41.386617286532534 updated_ts_SID 20.123267884347953 LR 0.0001\n",
            "Epoch:1, Iter:101, Time:6.68281626701355, learning rate:0.0001, Train loss:1.001799486061134, Train MRAE:0.9753742259327728, Train RMSE:0.18247600460406577, Train SAM:37.494059043355506, Train SID:14.3355236383948\n",
            "Test loss:0.8988710489927554, Test MRAE:0.8987465652765012, Test RMSE:0.15386322988014595, Test SAM:41.386617286532534, Test SID:20.123267884347953\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:16:58.649727\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8610942819539238 updated_ts_MRAE 0.8610917820649988 updated_ts_RMSE 0.1505582088641092 updated_ts_SAM 41.208588731055166 updated_ts_SID 19.18508026646633 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8610942819539238 updated_ts_MRAE 0.8610917820649988 updated_ts_RMSE 0.1505582088641092 updated_ts_SAM 41.208588731055166 updated_ts_SID 19.18508026646633 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.8610942819539238 updated_ts_MRAE 0.8610917820649988 updated_ts_RMSE 0.1505582088641092 updated_ts_SAM 41.208588731055166 updated_ts_SID 19.18508026646633 LR 0.0001\n",
            "Epoch:2, Iter:202, Time:6.407695770263672, learning rate:0.0001, Train loss:0.8707429046678071, Train MRAE:0.8707248509520351, Train RMSE:0.15035590861398396, Train SAM:42.38646820748206, Train SID:20.004518782738412\n",
            "Test loss:0.8610942819539238, Test MRAE:0.8610917820649988, Test RMSE:0.1505582088641092, Test SAM:41.208588731055166, Test SID:19.18508026646633\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:17:09.100831\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8560603050624623 updated_ts_MRAE 0.8560576053226695 updated_ts_RMSE 0.14831041018752492 updated_ts_SAM 41.3448709974102 updated_ts_SID 19.381368001302082 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8560603050624623 updated_ts_MRAE 0.8560576053226695 updated_ts_RMSE 0.14831041018752492 updated_ts_SAM 41.3448709974102 updated_ts_SID 19.381368001302082 LR 0.0001\n",
            "Epoch:3, Iter:303, Time:6.399947166442871, learning rate:0.0001, Train loss:0.8633891698157433, Train MRAE:0.8633847643833349, Train RMSE:0.14804249101936226, Train SAM:42.56613162956616, Train SID:20.193145374260325\n",
            "Test loss:0.8560603050624623, Test MRAE:0.8560576053226695, Test RMSE:0.14831041018752492, Test SAM:41.3448709974102, Test SID:19.381368001302082\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:17:19.658455\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:4, Iter:404, Time:6.6686015129089355, learning rate:0.0001, Train loss:0.8570138493386825, Train MRAE:0.8570090974911605, Train RMSE:0.14615994986921255, Train SAM:42.62803812310247, Train SID:20.252987257324822\n",
            "Test loss:0.8857202974020266, Test MRAE:0.8857052665130765, Test RMSE:0.1485319800821005, Test SAM:41.45426802541695, Test SID:20.22200668559355\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:17:30.107850\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.848357548900679 updated_ts_MRAE 0.8483540088522667 updated_ts_RMSE 0.14281920796515896 updated_ts_SAM 41.45300195731369 updated_ts_SID 19.941282384535846 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.848357548900679 updated_ts_MRAE 0.8483540088522667 updated_ts_RMSE 0.14281920796515896 updated_ts_SAM 41.45300195731369 updated_ts_SID 19.941282384535846 LR 0.0001\n",
            "Epoch:5, Iter:505, Time:6.611299514770508, learning rate:0.0001, Train loss:0.8611283561970928, Train MRAE:0.8611254503231237, Train RMSE:0.1473778959843192, Train SAM:42.614817893151006, Train SID:20.205652935670155\n",
            "Test loss:0.848357548900679, Test MRAE:0.8483540088522667, Test RMSE:0.14281920796515896, Test SAM:41.45300195731369, Test SID:19.941282384535846\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:17:40.051788\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8406007815809811 updated_ts_MRAE 0.8405988590390074 updated_ts_RMSE 0.14367061663492053 updated_ts_SAM 41.447527866737516 updated_ts_SID 19.708641557132495 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8406007815809811 updated_ts_MRAE 0.8405988590390074 updated_ts_RMSE 0.14367061663492053 updated_ts_SAM 41.447527866737516 updated_ts_SID 19.708641557132495 LR 0.0001\n",
            "Epoch:6, Iter:606, Time:6.3983330726623535, learning rate:0.0001, Train loss:0.8537706207520891, Train MRAE:0.8537661911237358, Train RMSE:0.14518596239314221, Train SAM:42.669145281952204, Train SID:20.29639702032108\n",
            "Test loss:0.8406007815809811, Test MRAE:0.8405988590390074, Test RMSE:0.14367061663492053, Test SAM:41.447527866737516, Test SID:19.708641557132495\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:17:50.439982\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.8809304225678537 updated_ts_MRAE 0.8809300497466442 updated_ts_RMSE 0.15566814135687024 updated_ts_SAM 41.38693981544644 updated_ts_SID 19.082682946149042 LR 0.0001\n",
            "Epoch:7, Iter:707, Time:6.3031182289123535, learning rate:0.0001, Train loss:0.8602879448692398, Train MRAE:0.8602853588538595, Train RMSE:0.1465758558104534, Train SAM:42.67481675478491, Train SID:20.312431278795298\n",
            "Test loss:0.8809304225678537, Test MRAE:0.8809300497466442, Test RMSE:0.15566814135687024, Test SAM:41.38693981544644, Test SID:19.082682946149042\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:18:00.607075\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:8, Iter:808, Time:6.279194116592407, learning rate:0.0001, Train loss:0.8544218121188702, Train MRAE:0.8544189209985261, Train RMSE:0.14548315823373228, Train SAM:42.66549104747206, Train SID:20.30327597703084\n",
            "Test loss:0.8707594053418029, Test MRAE:0.8707541823387146, Test RMSE:0.14536145811571793, Test SAM:41.44986470540365, Test SID:20.14338929045434\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:18:10.960530\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.837270249338711 updated_ts_MRAE 0.8372689427114001 updated_ts_RMSE 0.14322761753026178 updated_ts_SAM 41.43686945298139 updated_ts_SID 19.72836503795549 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.837270249338711 updated_ts_MRAE 0.8372689427114001 updated_ts_RMSE 0.14322761753026178 updated_ts_SAM 41.43686945298139 updated_ts_SID 19.72836503795549 LR 0.0001\n",
            "Epoch:9, Iter:909, Time:6.4057135581970215, learning rate:0.0001, Train loss:0.8492149515907363, Train MRAE:0.8492122764634614, Train RMSE:0.14451328502728208, Train SAM:42.67806202350277, Train SID:20.3424230235638\n",
            "Test loss:0.837270249338711, Test MRAE:0.8372689427114001, Test RMSE:0.14322761753026178, Test SAM:41.43686945298139, Test SID:19.72836503795549\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:18:21.139812\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8368475238482157 updated_ts_MRAE 0.8368458853048437 updated_ts_RMSE 0.14240001127416013 updated_ts_SAM 41.42089035931755 updated_ts_SID 19.799548766192267 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8368475238482157 updated_ts_MRAE 0.8368458853048437 updated_ts_RMSE 0.14240001127416013 updated_ts_SAM 41.42089035931755 updated_ts_SID 19.799548766192267 LR 0.0001\n",
            "Epoch:10, Iter:1010, Time:6.326525926589966, learning rate:0.0001, Train loss:0.8555908486394599, Train MRAE:0.8555886892989131, Train RMSE:0.14605959431074633, Train SAM:42.65374019358418, Train SID:20.280901625604912\n",
            "Test loss:0.8368475238482157, Test MRAE:0.8368458853048437, Test RMSE:0.14240001127416013, Test SAM:41.42089035931755, Test SID:19.799548766192267\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:18:31.513856\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:11, Iter:1111, Time:6.37673544883728, learning rate:0.0001, Train loss:0.8517796532942516, Train MRAE:0.8517766854550579, Train RMSE:0.14477875009916796, Train SAM:42.65162617145199, Train SID:20.308138224157958\n",
            "Test loss:0.8463684647691017, Test MRAE:0.8463679400144839, Test RMSE:0.14704888547752418, Test SAM:41.414126265282725, Test SID:19.513924823087805\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:18:41.878047\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:12, Iter:1212, Time:6.358338117599487, learning rate:0.0001, Train loss:0.8447895958872125, Train MRAE:0.8447866658172986, Train RMSE:0.1435565894635597, Train SAM:42.64318063943693, Train SID:20.316661466466318\n",
            "Test loss:0.8605679355415643, Test MRAE:0.8605676001193476, Test RMSE:0.15003775030958885, Test SAM:41.402024699192424, Test SID:19.32132079554539\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:18:52.134942\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8343249956766764 updated_ts_MRAE 0.8343235920457279 updated_ts_RMSE 0.14247294676070119 updated_ts_SAM 41.39967331231809 updated_ts_SID 19.76053351981967 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8343249956766764 updated_ts_MRAE 0.8343235920457279 updated_ts_RMSE 0.14247294676070119 updated_ts_SAM 41.39967331231809 updated_ts_SID 19.76053351981967 LR 0.0001\n",
            "Epoch:13, Iter:1313, Time:6.261166334152222, learning rate:0.0001, Train loss:0.8433853193084793, Train MRAE:0.8433834933998561, Train RMSE:0.1436444952493847, Train SAM:42.638246668447366, Train SID:20.305235381173617\n",
            "Test loss:0.8343249956766764, Test MRAE:0.8343235920457279, Test RMSE:0.14247294676070119, Test SAM:41.39967331231809, Test SID:19.76053351981967\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:19:02.067674\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8334758679072062 updated_ts_MRAE 0.8334749095580157 updated_ts_RMSE 0.14293655519391976 updated_ts_SAM 41.39902907726812 updated_ts_SID 19.67966143290202 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8334758679072062 updated_ts_MRAE 0.8334749095580157 updated_ts_RMSE 0.14293655519391976 updated_ts_SAM 41.39902907726812 updated_ts_SID 19.67966143290202 LR 0.0001\n",
            "Epoch:14, Iter:1414, Time:6.32476282119751, learning rate:0.0001, Train loss:0.8388940284747889, Train MRAE:0.8388915380628983, Train RMSE:0.14277912860754693, Train SAM:42.63590282024723, Train SID:20.311495695963945\n",
            "Test loss:0.8334758679072062, Test MRAE:0.8334749095580157, Test RMSE:0.14293655519391976, Test SAM:41.39902907726812, Test SID:19.67966143290202\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:19:12.410432\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:15, Iter:1515, Time:6.340653896331787, learning rate:0.0001, Train loss:0.8469220293630467, Train MRAE:0.8469179036593674, Train RMSE:0.14366947948047432, Train SAM:42.629903358988244, Train SID:20.304531975547867\n",
            "Test loss:0.8451515459546856, Test MRAE:0.8451511824832243, Test RMSE:0.14663013231520558, Test SAM:41.394819222244564, Test SID:19.499632835388184\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:19:22.479275\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8326300826727175 updated_ts_MRAE 0.8326285598324794 updated_ts_RMSE 0.14189918572996177 updated_ts_SAM 41.39394707773246 updated_ts_SID 19.74330486970789 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8326300826727175 updated_ts_MRAE 0.8326285598324794 updated_ts_RMSE 0.14189918572996177 updated_ts_SAM 41.39394707773246 updated_ts_SID 19.74330486970789 LR 0.0001\n",
            "Epoch:16, Iter:1616, Time:6.410262823104858, learning rate:0.0001, Train loss:0.8424872425523134, Train MRAE:0.842486051049563, Train RMSE:0.1435239568795308, Train SAM:42.63139849370069, Train SID:20.286484368956916\n",
            "Test loss:0.8326300826727175, Test MRAE:0.8326285598324794, Test RMSE:0.14189918572996177, Test SAM:41.39394707773246, Test SID:19.74330486970789\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:19:32.674431\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:17, Iter:1717, Time:6.337057828903198, learning rate:0.0001, Train loss:0.84147045399883, Train MRAE:0.8414682285620434, Train RMSE:0.14320281029927848, Train SAM:42.62738911468204, Train SID:20.314527351077242\n",
            "Test loss:0.8561965192065519, Test MRAE:0.8561928470929464, Test RMSE:0.1426528857326975, Test SAM:41.382319207284965, Test SID:20.014531902238435\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:19:43.003597\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8314128073991514 updated_ts_MRAE 0.8314116982852712 updated_ts_RMSE 0.1414268216958233 updated_ts_SAM 41.38337797277114 updated_ts_SID 19.742324922599046 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8314128073991514 updated_ts_MRAE 0.8314116982852712 updated_ts_RMSE 0.1414268216958233 updated_ts_SAM 41.38337797277114 updated_ts_SID 19.742324922599046 LR 0.0001\n",
            "Epoch:18, Iter:1818, Time:6.338378667831421, learning rate:0.0001, Train loss:0.840753967809205, Train MRAE:0.8407522599295815, Train RMSE:0.14274197278341444, Train SAM:42.620559994537054, Train SID:20.31089976282403\n",
            "Test loss:0.8314128073991514, Test MRAE:0.8314116982852712, Test RMSE:0.1414268216958233, Test SAM:41.38337797277114, Test SID:19.742324922599046\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:19:53.685001\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:19, Iter:1919, Time:6.296817302703857, learning rate:0.0001, Train loss:0.8465407431715786, Train MRAE:0.8465391981719744, Train RMSE:0.14365405198371056, Train SAM:42.61809891049224, Train SID:20.289833569290614\n",
            "Test loss:0.8349432337517831, Test MRAE:0.8349428620992922, Test RMSE:0.14176998944843516, Test SAM:41.37433994517607, Test SID:19.792927162320005\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:20:04.090783\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:20, Iter:2020, Time:6.728605508804321, learning rate:0.0001, Train loss:0.8392406023374879, Train MRAE:0.8392391913007982, Train RMSE:0.1429090614218523, Train SAM:42.61453515232199, Train SID:20.305668830871582\n",
            "Test loss:0.8553701008067411, Test MRAE:0.8553700166590074, Test RMSE:0.14967219677625918, Test SAM:41.38571196911382, Test SID:19.359483793670055\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:20:14.161485\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:21, Iter:2121, Time:6.396497964859009, learning rate:0.0001, Train loss:0.8392245344596334, Train MRAE:0.8392228678901597, Train RMSE:0.14317287895644065, Train SAM:42.60253604095761, Train SID:20.27824840923347\n",
            "Test loss:0.8481644356951994, Test MRAE:0.8481637029086843, Test RMSE:0.14286025379802667, Test SAM:41.37346525753246, Test SID:19.912239504795448\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:20:24.579642\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 1.0065854565770018 updated_ts_MRAE 1.0065851527101852 updated_ts_RMSE 0.1468516629116208 updated_ts_SAM 43.891123752967985 updated_ts_SID 17.610334919948205 LR 0.0001\n",
            "Epoch:22, Iter:2222, Time:6.356296539306641, learning rate:0.0001, Train loss:0.8341030891578977, Train MRAE:0.8341017784458575, Train RMSE:0.14217673433889258, Train SAM:42.58068862763962, Train SID:20.243919920213152\n",
            "Test loss:1.0065854565770018, Test MRAE:1.0065851527101852, Test RMSE:0.1468516629116208, Test SAM:43.891123752967985, Test SID:17.610334919948205\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:20:34.796647\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.7478420979836408 updated_ts_MRAE 0.7478415346613118 updated_ts_RMSE 0.12091850927647423 updated_ts_SAM 34.05094760071997 updated_ts_SID 13.135291520287009 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.7478420979836408 updated_ts_MRAE 0.7478415346613118 updated_ts_RMSE 0.12091850927647423 updated_ts_SAM 34.05094760071997 updated_ts_SID 13.135291520287009 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.7478420979836408 updated_ts_MRAE 0.7478415346613118 updated_ts_RMSE 0.12091850927647423 updated_ts_SAM 34.05094760071997 updated_ts_SID 13.135291520287009 LR 0.0001\n",
            "Epoch:23, Iter:2323, Time:6.302728652954102, learning rate:0.0001, Train loss:0.7909691894408499, Train MRAE:0.7909680823288342, Train RMSE:0.13144759546117027, Train SAM:38.41976838064666, Train SID:16.84249546032141\n",
            "Test loss:0.7478420979836408, Test MRAE:0.7478415346613118, Test RMSE:0.12091850927647423, Test SAM:34.05094760071997, Test SID:13.135291520287009\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:20:45.205809\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.709648001427744 updated_ts_MRAE 0.7096476589932161 updated_ts_RMSE 0.11592416027012993 updated_ts_SAM 32.169890198053096 updated_ts_SID 13.01413995144414 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.709648001427744 updated_ts_MRAE 0.7096476589932161 updated_ts_RMSE 0.11592416027012993 updated_ts_SAM 32.169890198053096 updated_ts_SID 13.01413995144414 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.709648001427744 updated_ts_MRAE 0.7096476589932161 updated_ts_RMSE 0.11592416027012993 updated_ts_SAM 32.169890198053096 updated_ts_SID 13.01413995144414 LR 0.0001\n",
            "Epoch:24, Iter:2424, Time:6.391445875167847, learning rate:0.0001, Train loss:0.7352988826166286, Train MRAE:0.7352978817307123, Train RMSE:0.11961491837507428, Train SAM:34.138814397377544, Train SID:14.051014522514722\n",
            "Test loss:0.709648001427744, Test MRAE:0.7096476589932161, Test RMSE:0.11592416027012993, Test SAM:32.169890198053096, Test SID:13.01413995144414\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:20:55.030623\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.6846488457100064 updated_ts_MRAE 0.6846485617114049 updated_ts_RMSE 0.11316261966438855 updated_ts_SAM 30.814698911180685 updated_ts_SID 12.93620423709645 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.6846488457100064 updated_ts_MRAE 0.6846485617114049 updated_ts_RMSE 0.11316261966438855 updated_ts_SAM 30.814698911180685 updated_ts_SID 12.93620423709645 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.6846488457100064 updated_ts_MRAE 0.6846485617114049 updated_ts_RMSE 0.11316261966438855 updated_ts_SAM 30.814698911180685 updated_ts_SID 12.93620423709645 LR 0.0001\n",
            "Epoch:25, Iter:2525, Time:6.393321514129639, learning rate:0.0001, Train loss:0.6994209366269631, Train MRAE:0.6994204727729948, Train RMSE:0.11391862485520911, Train SAM:32.2514395383325, Train SID:13.414132316513816\n",
            "Test loss:0.6846488457100064, Test MRAE:0.6846485617114049, Test RMSE:0.11316261966438855, Test SAM:30.814698911180685, Test SID:12.93620423709645\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:21:05.742872\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.6931645391034145 updated_ts_MRAE 0.6931644082069397 updated_ts_RMSE 0.11678406260177202 updated_ts_SAM 31.17560644710765 updated_ts_SID 12.841291006873636 LR 0.0001\n",
            "Epoch:26, Iter:2626, Time:6.3577635288238525, learning rate:0.0001, Train loss:0.7000939267696721, Train MRAE:0.7000933354443842, Train RMSE:0.11348191851584039, Train SAM:32.24064058360487, Train SID:13.410656315265316\n",
            "Test loss:0.6931645391034145, Test MRAE:0.6931644082069397, Test RMSE:0.11678406260177202, Test SAM:31.17560644710765, Test SID:12.841291006873636\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:21:15.834558\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.6933835452678156 updated_ts_MRAE 0.6933833816472221 updated_ts_RMSE 0.11490415971653134 updated_ts_SAM 30.863641664093617 updated_ts_SID 12.828576835931516 LR 0.0001\n",
            "Epoch:27, Iter:2727, Time:6.394757032394409, learning rate:0.0001, Train loss:0.7038663283433064, Train MRAE:0.7038658379328133, Train RMSE:0.11417416309808741, Train SAM:32.23202142621031, Train SID:13.38624917870701\n",
            "Test loss:0.6933835452678156, Test MRAE:0.6933833816472221, Test RMSE:0.11490415971653134, Test SAM:30.863641664093617, Test SID:12.828576835931516\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:21:26.166235\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.6190179235794965 updated_ts_MRAE 0.6190173999935973 updated_ts_RMSE 0.09947880423244308 updated_ts_SAM 26.406866709391277 updated_ts_SID 8.71524052993924 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.6190179235794965 updated_ts_MRAE 0.6190173999935973 updated_ts_RMSE 0.09947880423244308 updated_ts_SAM 26.406866709391277 updated_ts_SID 8.71524052993924 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.6190179235794965 updated_ts_MRAE 0.6190173999935973 updated_ts_RMSE 0.09947880423244308 updated_ts_SAM 26.406866709391277 updated_ts_SID 8.71524052993924 LR 0.0001\n",
            "Epoch:28, Iter:2828, Time:6.3698155879974365, learning rate:0.0001, Train loss:0.6551741048841193, Train MRAE:0.6551736056214512, Train RMSE:0.1056440540469519, Train SAM:28.978935515526498, Train SID:10.765636198591478\n",
            "Test loss:0.6190179235794965, Test MRAE:0.6190173999935973, Test RMSE:0.09947880423244308, Test SAM:26.406866709391277, Test SID:8.71524052993924\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:21:36.455123\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5927697244812461 updated_ts_MRAE 0.5927696298150456 updated_ts_RMSE 0.09410836028994299 updated_ts_SAM 24.5115816078934 updated_ts_SID 7.872171537548888 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5927697244812461 updated_ts_MRAE 0.5927696298150456 updated_ts_RMSE 0.09410836028994299 updated_ts_SAM 24.5115816078934 updated_ts_SID 7.872171537548888 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.5927697244812461 updated_ts_MRAE 0.5927696298150456 updated_ts_RMSE 0.09410836028994299 updated_ts_SAM 24.5115816078934 updated_ts_SID 7.872171537548888 LR 0.0001\n",
            "Epoch:29, Iter:2929, Time:6.189945697784424, learning rate:0.0001, Train loss:0.6343164337743626, Train MRAE:0.6343155031157012, Train RMSE:0.1002440561014827, Train SAM:27.587641055041022, Train SID:9.86709465838895\n",
            "Test loss:0.5927697244812461, Test MRAE:0.5927696298150456, Test RMSE:0.09410836028994299, Test SAM:24.5115816078934, Test SID:7.872171537548888\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:21:46.312872\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5803436929104375 updated_ts_MRAE 0.5803434591667325 updated_ts_RMSE 0.09027968507771399 updated_ts_SAM 23.88265329248765 updated_ts_SID 7.8011560907550885 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5803436929104375 updated_ts_MRAE 0.5803434591667325 updated_ts_RMSE 0.09027968507771399 updated_ts_SAM 23.88265329248765 updated_ts_SID 7.8011560907550885 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.5803436929104375 updated_ts_MRAE 0.5803434591667325 updated_ts_RMSE 0.09027968507771399 updated_ts_SAM 23.88265329248765 updated_ts_SID 7.8011560907550885 LR 0.0001\n",
            "Epoch:30, Iter:3030, Time:6.153059959411621, learning rate:0.0001, Train loss:0.5860128125341812, Train MRAE:0.5860120996390239, Train RMSE:0.0909861257982136, Train SAM:24.684304246808043, Train SID:8.230071353440238\n",
            "Test loss:0.5803436929104375, Test MRAE:0.5803434591667325, Test RMSE:0.09027968507771399, Test SAM:23.88265329248765, Test SID:7.8011560907550885\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:21:55.836573\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:31, Iter:3131, Time:6.172739267349243, learning rate:0.0001, Train loss:0.5926045043633716, Train MRAE:0.5926037354044394, Train RMSE:0.09219674758686877, Train SAM:25.283519735430726, Train SID:8.516056438483814\n",
            "Test loss:0.5825300076428581, Test MRAE:0.5825297937673681, Test RMSE:0.09368348399213716, Test SAM:24.537400376562978, Test SID:7.978041522643146\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:22:05.640052\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5718875597505009 updated_ts_MRAE 0.5718869239676232 updated_ts_RMSE 0.08780555836125917 updated_ts_SAM 23.408797806384516 updated_ts_SID 7.774080823449528 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5718875597505009 updated_ts_MRAE 0.5718869239676232 updated_ts_RMSE 0.08780555836125917 updated_ts_SAM 23.408797806384516 updated_ts_SID 7.774080823449528 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.5718875597505009 updated_ts_MRAE 0.5718869239676232 updated_ts_RMSE 0.08780555836125917 updated_ts_SAM 23.408797806384516 updated_ts_SID 7.774080823449528 LR 0.0001\n",
            "Epoch:32, Iter:3232, Time:6.187664985656738, learning rate:0.0001, Train loss:0.5755179757528966, Train MRAE:0.5755169134919006, Train RMSE:0.08878878810175575, Train SAM:24.23121708218414, Train SID:8.181236009786625\n",
            "Test loss:0.5718875597505009, Test MRAE:0.5718869239676232, Test RMSE:0.08780555836125917, Test SAM:23.408797806384516, Test SID:7.774080823449528\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:22:15.460122\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:33, Iter:3333, Time:6.152146100997925, learning rate:0.0001, Train loss:0.5898897228854718, Train MRAE:0.5898886505920108, Train RMSE:0.09134594771531548, Train SAM:25.157442565011507, Train SID:8.44765838302008\n",
            "Test loss:0.6014021576619616, Test MRAE:0.6014016854996774, Test RMSE:0.09358652797984142, Test SAM:24.659695382211723, Test SID:8.197927937788123\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:22:25.415554\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:34, Iter:3434, Time:6.172314882278442, learning rate:0.0001, Train loss:0.5809410509496632, Train MRAE:0.5809401592405716, Train RMSE:0.08948773171494503, Train SAM:24.600366252483706, Train SID:8.278464798880096\n",
            "Test loss:0.6367525937510472, Test MRAE:0.6367519287502065, Test RMSE:0.09800278504981715, Test SAM:26.023386936561735, Test SID:8.676573996450387\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:22:35.176245\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5610264750087962 updated_ts_MRAE 0.5610262120471281 updated_ts_RMSE 0.09032406007834509 updated_ts_SAM 23.653427647609337 updated_ts_SID 7.727530049342735 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5610264750087962 updated_ts_MRAE 0.5610262120471281 updated_ts_RMSE 0.09032406007834509 updated_ts_SAM 23.653427647609337 updated_ts_SID 7.727530049342735 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.5610264750087962 updated_ts_MRAE 0.5610262120471281 updated_ts_RMSE 0.09032406007834509 updated_ts_SAM 23.653427647609337 updated_ts_SID 7.727530049342735 LR 0.0001\n",
            "Epoch:35, Iter:3535, Time:6.158650159835815, learning rate:0.0001, Train loss:0.5641295021713371, Train MRAE:0.5641277087206887, Train RMSE:0.08718698047617875, Train SAM:23.854254665941294, Train SID:8.072956451094976\n",
            "Test loss:0.5610264750087962, Test MRAE:0.5610262120471281, Test RMSE:0.09032406007834509, Test SAM:23.653427647609337, Test SID:7.727530049342735\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:22:45.082199\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:36, Iter:3636, Time:6.5714123249053955, learning rate:0.0001, Train loss:0.5576485265599619, Train MRAE:0.557647444234036, Train RMSE:0.08675496479367266, Train SAM:23.635881622238916, Train SID:8.0622665244754\n",
            "Test loss:0.5918942876890594, Test MRAE:0.5918934602363437, Test RMSE:0.09192550664438921, Test SAM:24.259379910487755, Test SID:8.108759613598094\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:22:54.561878\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:37, Iter:3737, Time:6.217891216278076, learning rate:0.0001, Train loss:0.5642615527209669, Train MRAE:0.5642596288482742, Train RMSE:0.08755201680382879, Train SAM:24.102042292604352, Train SID:8.151929128288042\n",
            "Test loss:0.570753309656592, Test MRAE:0.5707529041112638, Test RMSE:0.08872815849734288, Test SAM:23.596635930678424, Test SID:7.741297777961282\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:23:04.356514\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:38, Iter:3838, Time:6.1799156665802, learning rate:0.0001, Train loss:0.5661378440880539, Train MRAE:0.5661366030131236, Train RMSE:0.0878249882250139, Train SAM:24.067115188825248, Train SID:8.144313873630939\n",
            "Test loss:0.6397307082718494, Test MRAE:0.6397300362586975, Test RMSE:0.09247267976695416, Test SAM:25.23223061655082, Test SID:7.799753544377346\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:23:13.865598\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "Epoch:39, Iter:3939, Time:6.227326154708862, learning rate:0.0001, Train loss:0.5551689438300558, Train MRAE:0.5551671084791127, Train RMSE:0.08630743427294316, Train SAM:23.68510119749768, Train SID:7.995919971182795\n",
            "Test loss:0.5689222263354882, Test MRAE:0.5689205159159267, Test RMSE:0.08552951986591022, Test SAM:22.73553622002695, Test SID:7.7340472819758395\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 18:23:23.803284\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5465376184267157 updated_ts_MRAE 0.5465364859384649 updated_ts_RMSE 0.08683418913506995 updated_ts_SAM 22.82149148454853 updated_ts_SID 7.725739469715193 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5465376184267157 updated_ts_MRAE 0.5465364859384649 updated_ts_RMSE 0.08683418913506995 updated_ts_SAM 22.82149148454853 updated_ts_SID 7.725739469715193 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.5465376184267157 updated_ts_MRAE 0.5465364859384649 updated_ts_RMSE 0.08683418913506995 updated_ts_SAM 22.82149148454853 updated_ts_SID 7.725739469715193 LR 0.0001\n",
            "Epoch:40, Iter:4040, Time:6.221310615539551, learning rate:0.0001, Train loss:0.5546726976880932, Train MRAE:0.5546705896901613, Train RMSE:0.08633201292688304, Train SAM:23.557892619973362, Train SID:8.003655780660043\n",
            "Test loss:0.5465376184267157, Test MRAE:0.5465364859384649, Test RMSE:0.08683418913506995, Test SAM:22.82149148454853, Test SID:7.725739469715193\n",
            "w1: 1, w2: 0\n",
            "num.101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7sakvJCYwiQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xDSJQBg8GxY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}