{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "True2msi_MraeSidLoss_UAV_MaizeAll_HSCNNR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFWHFnJO4i3o",
        "outputId": "d812e53a-b9ff-4cb4-d835-5e04966ac440"
      },
      "source": [
        "#!pip install --no-cache-dir -I pillow\n",
        "!pip install hdf5storage\n",
        "#!pip install http://download.pytorch.org/whl/cu92/torch-1.6.0-cp36-cp36m-linux_x86_64.whl\n",
        "#!pip install torch\n",
        "!pip3 install torchvision\n",
        "#!git clone https://github.com/lanpa/tensorboardX && cd tensorboardX && python setup.py install\n",
        "!pip install tensorboardX\n",
        "!pip install --pre torch -f  https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200626%2Bcu101-cp36-cp36m-linux_x86_64.whl\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hdf5storage\n",
            "  Downloading hdf5storage-0.1.18-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 988 kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.1 in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.1->hdf5storage) (1.5.2)\n",
            "Installing collected packages: hdf5storage\n",
            "Successfully installed hdf5storage-0.1.18\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cu101/torch-1.7.0.dev20200626%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVdWVJYaR_kD"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuO41DNu4dBG",
        "outputId": "f37bde87-9600-4bf4-9ee4-586912d489ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "#os.chdir(\"/content/drive/My Drive/maize_season3_RededgeMultispectral_20181030_10m_flight2/\") # folder containing your images used for training and testing \n",
        "#os.chdir(\"/content/drive/My Drive/maize_season3_RededgeMultispectral_20181119_10m/\") # folder containing your images used for training and testing \n",
        "os.chdir(\"/content/drive/My Drive/\")\n",
        "path=os.getcwd() "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duKaP9IE4i6_"
      },
      "source": [
        "### \n",
        "from __future__ import division\n",
        "from scipy import interpolate\n",
        "import random\n",
        "import os\n",
        "import os.path\n",
        "import h5py\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "\n",
        "import scipy.io\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from matplotlib.figure import Figure\n",
        "from IPython.display import clear_output \n",
        "\n",
        "import torchvision.utils as utils\n",
        "from tensorboardX import SummaryWriter\n",
        "###\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as udata\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.utils as utils\n",
        "import time\n",
        "import scipy.io as sio\n",
        "import logging\n",
        "import hdf5storage\n",
        "import datetime\n",
        "from math import sqrt\n",
        "%matplotlib inline\n",
        "import scipy.io as spio\n",
        "from scipy.interpolate import PchipInterpolator\n",
        "from bisect import bisect\n",
        "from google.colab import output\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.pylab import cm\n",
        "##\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def save_checkpoint_sp(model_path, epoch, iteration, model, optimizer, evaluation):\n",
        "    \"\"\"Save the checkpoint.\"\"\"\n",
        "    state = {\n",
        "            'epoch': epoch,\n",
        "            'iter': iteration,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer' : optimizer.state_dict(),\n",
        "            }\n",
        "    \n",
        "    torch.save(state, os.path.join(model_path, 'hscnn_6layer_dim10_{}.pkl'.format(evaluation)))\n",
        "\n",
        "def plot_spectrum(real, fake, epoch, i):\n",
        "    x =np.linspace(400, 900, 5, endpoint=True) # the wavebands of the hyperspectral image\n",
        "    fig = Figure()\n",
        "    canvas = FigureCanvasAgg(fig)\n",
        "    ax = fig.gca()\n",
        "    #ax.set_ylim(0, 1)\n",
        "    plot_real,  = ax.plot(x, real, 'ko-')\n",
        "    plot_fake,  = ax.plot(x, fake, 'r.-')\n",
        "    fig.legend((plot_real,plot_fake), ('real', 'fake'))\n",
        "    canvas.draw()\n",
        "    fig.savefig(os.path.join(iteration_path, \"{}_test_{}.png\".format(epoch,i)))\n",
        "    I = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
        "    I = I.reshape(canvas.get_width_height()[::-1]+(3,))\n",
        "    I = np.transpose(I, [2,0,1])\n",
        "    return np.float32(I)\n",
        "\n",
        "\n",
        "def plotwithcolorbar(img, title=None, figsize=(10,10)):\n",
        "    ''' Plot an image with a colorbar '''\n",
        "    vmin = np.min(img)\n",
        "    vmax = np.max(img)\n",
        "    hh = img.shape[1]\n",
        "    fig, axis = plt.subplots(1, 1, figsize=figsize)\n",
        "    rad2 = axis.imshow(img, vmin=vmin, vmax=vmax)\n",
        "    axis.set_title(title)\n",
        "    divider = make_axes_locatable(axis)\n",
        "    cax = divider.append_axes(\"right\", size=\"3%\", pad=0.05)\n",
        "    fig.colorbar(rad2, cax=cax)\n",
        "    circle_b = plt.Circle((int(hh/2), int(hh/2)), 5, color='b', fill=False)\n",
        "    axis.add_artist(circle_b)\n",
        "    plt.close(fig)\n",
        "    #fig.savefig(os.path.join(iteration_path, \"{}_test_fake_img_{}.png\".format(epoch,i)))\n",
        "    #plt.tight_layout()\n",
        "    #plt.show()\n",
        "    return fig, axis\n",
        "\n",
        "def initialize_logger(file_dir):\n",
        "    \"\"\"Print the results in the log file.\"\"\"\n",
        "    logger = logging.getLogger()\n",
        "    fhandler = logging.FileHandler(filename=file_dir, mode='a')\n",
        "    formatter = logging.Formatter('%(asctime)s - %(message)s',\"%Y-%m-%d %H:%M:%S\")\n",
        "    fhandler.setFormatter(formatter)\n",
        "    logger.addHandler(fhandler)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    return logger\n",
        "\n",
        "def save_matv73(mat_name, var_name, var):\n",
        "    hdf5storage.savemat(mat_name, {var_name: var}, format='7.3', store_python_metadata=True)\n",
        "\n",
        "\n",
        "def mrae_loss(im_true, im_fake):\n",
        "    error = torch.abs(im_fake-im_true)/im_true\n",
        "    rrmse = torch.mean(error.reshape(-1))\n",
        "    return rrmse\n",
        "\n",
        "def sam_loss(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    nom = torch.sum( torch.mul(im_true, im_fake), dim=1)\n",
        "    denom1 = torch.sqrt( torch.sum( torch.pow(im_true,2), dim=1))\n",
        "    denom2 = torch.sqrt( torch.sum( torch.pow(im_fake,2), dim=1))\n",
        "    sam = torch.acos(torch.div(nom, torch.mul(denom1, denom2)).clamp(-1.0 + 1e-8, 1.0 - 1e-8))\n",
        "    #print(sam.size())\n",
        "    sam = torch.mul(torch.div(sam, np.pi), 180)\n",
        "    #print((\"sam_max\", sam.max()))\n",
        "    #print((\"sam_min\", sam.min()))\n",
        "    sam = torch.div(torch.sum(sam), N*H*W)\n",
        "    #print(\"\")\n",
        "    #print((\"sam_final\", sam.item()))\n",
        "    #print(\"\")\n",
        "    return sam\n",
        "\n",
        "def sid_loss(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    denom1 = torch.sqrt( torch.sum( torch.pow(im_true,2), dim=1))\n",
        "    denom2 = torch.sqrt( torch.sum( torch.pow(im_fake,2), dim=1))\n",
        "    #\n",
        "    unit_t = torch.div(im_true, denom1.unsqueeze(1))\n",
        "    uint_f = torch.div(im_fake, denom2.unsqueeze(1))\n",
        "    #\n",
        "    sid = ((unit_t - uint_f)* (unit_t.log() - uint_f.log())).sum() / (N*H*W)\n",
        "    return sid\n",
        "####\n",
        "## new fucntion for process data\n",
        "path=os.getcwd() \n",
        "\n",
        "##. with normalization\n",
        "def normalize(data):\n",
        "    #data_nl = (data-np.amin(data[:,:,:]))/(np.amax(data[:,:,:])-np.amin(data[:,:,:]))\n",
        "    data_nl = data/np.amax(data)\n",
        "    #data_nl[data_nl<=0.0] = 0.0\n",
        "    #data_nl[data_nl>1.0] = 1.0\n",
        "    return data_nl\n",
        "##\n",
        "def gen_random_scale_n(img, rnd=3):\n",
        "    np.random.seed(rnd)\n",
        "    scale = np.random.uniform(0.1, 1.91, (1,1, img.shape[2],img.shape[3]))   \n",
        "    return img*scale\n",
        "##\n",
        "def data_process_list_n(path=path):\n",
        "    NO_ = 1\n",
        "    hyper_f = os.path.join(path,'Maize2018_ortho_msi_cropped_04052021')\n",
        "    sub_h5_fd = next(os.walk(hyper_f))[1]\n",
        "    sub_h5_fd.sort()\n",
        "    #\n",
        "    #rgb_f = hyper_f.replace(\"h5\", \"rgb\")\n",
        "    rgb_f = os.path.join(path,'Maize2018_ortho_rgb_cropped_04052021_NEW')\n",
        "    sub_rgb_fd = next(os.walk(rgb_f))[1]\n",
        "    sub_rgb_fd.sort()\n",
        "\n",
        "    for sf in range(len(sub_h5_fd)):\n",
        "    #for sf in [0,1,2,3]:\n",
        "        id_n = sub_h5_fd[sf].split(\"_\")[2][4:]\n",
        "        #\n",
        "        filenames_hyper = glob.glob(os.path.join(hyper_f,sub_h5_fd[sf],'*.h5'))\n",
        "        filenames_rgb = glob.glob(os.path.join(rgb_f, sub_rgb_fd[sf].replace(\"h5\", \"rgb\"),'*.png'))\n",
        "        filenames_hyper.sort()\n",
        "        print(filenames_hyper)\n",
        "        filenames_rgb.sort()\n",
        "        print(filenames_rgb)\n",
        "        #h5f = h5py.File('train_uavf3.h5', 'w')  ## the file concatenated hsi and rgb in one file\n",
        "        h5f = h5py.File('train_040521_maize_NatColor_finalN_{}.h5'.format(id_n), 'w')  ## the file concatenated hsi and rgb in one file\n",
        "        #filenames_hyper = glob.glob(os.path.join(path,'Rice_ortho_h5_cropped_noco/Exported_rice_20181124_h5_cropped_2','*.h5'))\n",
        "        \n",
        "        \n",
        "        for i in range(len(filenames_hyper)):\n",
        "            print(\"\\n\")\n",
        "            print(filenames_hyper[i], filenames_rgb[i])\n",
        "            # load hyperspectral image\n",
        "            mat =  h5py.File(filenames_hyper[i],'r')\n",
        "            hyper = np.float32(np.array(mat['img']))\n",
        "            print((\"hyper_shape\",hyper.shape))\n",
        "            hyper = np.transpose(hyper, [2,0,1])#/32768.0 # because it 32768.0 was divided already before cropping into 512x512 squares\n",
        "            #print(\"hyper_or_max\", np.amax(hyper))\n",
        "            #print(\"hyper_or_min\",np.amin(hyper))\n",
        "            #hyper[hyper<0] = 0\n",
        "            #hyper[hyper>1.0] = 1.0\n",
        "            #hyper = normalize(hyper)\n",
        "            print(\"hyper_test_max\", np.amax(hyper))\n",
        "            print(\"hyper_test_min\",np.amin(hyper))\n",
        "            mat.close()\n",
        "            # load rgb image\n",
        "            rgb =  cv2.imread(filenames_rgb[i])\n",
        "            rgb=cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            rgb = np.float32(np.transpose(rgb, [2,0,1])/255.0) # change to 0-1 range\n",
        "            #print(\"rgb_or_max\", np.amax(np.float32(rgb)))\n",
        "            #print(\"rgb_or_min\", np.amin(np.float32(rgb)))\n",
        "            #rgb = normalize(np.float32(rgb))\n",
        "            print(\"rgb_test_max\", np.amax(np.float32(rgb)))\n",
        "            print(\"rgb_test_min\", np.amin(np.float32(rgb)))\n",
        "    #            mat.close()\n",
        "                # creat patches\n",
        "            data = np.concatenate((hyper,rgb), 0)\n",
        "            h5f.create_dataset(str(NO_), data=data)\n",
        "            NO_ += 1\n",
        "        h5f.close()\n",
        "        print(\"NO. of samples: {}\".format(NO_-1))\n",
        "##\n",
        "class HyperDataset_list_maize_RGB(udata.Dataset):\n",
        "    def __init__(self, crop_size=64):\n",
        "        self.crop_size = crop_size\n",
        "        xx = [\"train_040521_maize_NatColor_finalN_1030.h5\",\"train_040521_maize_NatColor_finalN_1109.h5\", \"train_040521_maize_NatColor_finalN_1119.h5\",\"train_040521_maize_NatColor_finalN_1220.h5\"] #\"train_uavf_maize_NatColor_1109.h5\" was removed because of over exposure\n",
        "        xx.sort()\n",
        "        # print(xx)\n",
        "        key_is = []\n",
        "        for i in range(len(xx)):\n",
        "            f_i = xx[i]\n",
        "            h5f_i = h5py.File(f_i, 'r')\n",
        "            keys_i = list(h5f_i.keys())\n",
        "            keys_i.sort()\n",
        "            key_i_n  = [\"_\".join([str(i), x]) for x in keys_i]\n",
        "            key_is = key_is + key_i_n\n",
        "            #print((\"key_is\", key_is))\n",
        "            h5f_i.close()\n",
        "        \n",
        "        self.keys = key_is\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "    def __getitem__(self, index):\n",
        "        #xx = glob.glob(os.path.join(path,'train_uavf_maize_natlk_2_shuffle_?*.h5'))\n",
        "        xx = [\"train_040521_maize_NatColor_finalN_1030.h5\",\"train_040521_maize_NatColor_finalN_1109.h5\", \"train_040521_maize_NatColor_finalN_1119.h5\",\"train_040521_maize_NatColor_finalN_1220.h5\"] #\"train_uavf_maize_NatColor_1109.h5\" was removed because of over exposure\n",
        "        xx.sort()\n",
        "        key = str(self.keys[index])\n",
        "        #print((\"key\", key))\n",
        "        key_path_i = key.split(\"_\")[0]\n",
        "        #print((\"key_path_i\",key_path_i))\n",
        "        key_key = key.split(\"_\")[1]\n",
        "        #print((\"key_key\",key_key))\n",
        "        #print(xx[int(key_path_i)])\n",
        "        h5f_a = h5py.File(xx[int(key_path_i)], 'r')\n",
        "        #\n",
        "        #print((\"list\",list(h5f_a.keys())))\n",
        "        #print(key_key)\n",
        "        data_a = np.array(h5f_a[key_key])\n",
        "        data = torch.as_tensor(data_a)\n",
        "        #data[data>1.0] = 1.0\n",
        "        #data[data<0.0] = 0.0\n",
        "        #print(data.size())\n",
        "        # crop\n",
        "        w = int(data.size()[1])\n",
        "        h = int(data.size()[2])\n",
        "        th, tw = self.crop_size, self.crop_size\n",
        "        if w > tw or h > th:\n",
        "            i = 0\n",
        "            j = 0\n",
        "            data = data[:,i:i+th,j:j+tw]\n",
        "        h5f_a.close()\n",
        "        return data[0:5,:,:], data[5:8,:,:]\n",
        "\n",
        "class HyperDataset_list_rice_RGB(udata.Dataset):\n",
        "    def __init__(self, crop_size=64):\n",
        "        self.crop_size = crop_size\n",
        "        #xx = glob.glob(os.path.join(path,'train_uavf_maize_natlk_2_shuffle_?*.h5'))\n",
        "        xx = [\"train_uavf_rice_NatColor_final_0821.h5\",\"train_uavf_rice_NatColor_final_1026.h5\",\"train_uavf_rice_NatColor_final_1124.h5\"] #\"train_uavf_rice_NatColor_final_0927.h5\" was removed due to over exposure\n",
        "        xx.sort()\n",
        "        # print(xx)\n",
        "        key_is = []\n",
        "        for i in range(len(xx)):\n",
        "            f_i = xx[i]\n",
        "            h5f_i = h5py.File(f_i, 'r')\n",
        "            keys_i = list(h5f_i.keys())\n",
        "            keys_i.sort()\n",
        "            key_i_n  = [\"_\".join([str(i), x]) for x in keys_i]\n",
        "            key_is = key_is + key_i_n\n",
        "            #print((\"key_is\", key_is))\n",
        "            h5f_i.close()\n",
        "        \n",
        "        self.keys = key_is\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "    def __getitem__(self, index):\n",
        "        #xx = glob.glob(os.path.join(path,'train_uavf_maize_natlk_2_shuffle_?*.h5'))\n",
        "        xx = [\"train_uavf_rice_NatColor_final_0821.h5\",\"train_uavf_rice_NatColor_final_1026.h5\",\"train_uavf_rice_NatColor_final_1124.h5\"] #\"train_uavf_rice_NatColor_final_0927.h5\" was removed due to over exposure\n",
        "        xx.sort()\n",
        "        key = str(self.keys[index])\n",
        "        #print((\"key\", key))\n",
        "        key_path_i = key.split(\"_\")[0]\n",
        "        #print((\"key_path_i\",key_path_i))\n",
        "        key_key = key.split(\"_\")[1]\n",
        "        #print((\"key_key\",key_key))\n",
        "        #print(xx[int(key_path_i)])\n",
        "        h5f_a = h5py.File(xx[int(key_path_i)], 'r')\n",
        "        #\n",
        "        #print((\"list\",list(h5f_a.keys())))\n",
        "        #print(key_key)\n",
        "        data_a = np.array(h5f_a[key_key])\n",
        "        data = torch.as_tensor(data_a)\n",
        "        #data[data>1.0] = 1.0\n",
        "        #data[data<0.0] = 0.0\n",
        "        #print(data.size())\n",
        "        # crop\n",
        "        w = int(data.size()[1])\n",
        "        h = int(data.size()[2])\n",
        "        th, tw = self.crop_size, self.crop_size\n",
        "        if w > tw or h > th:\n",
        "            i = 0\n",
        "            j = 0\n",
        "            data = data[:,i:i+th,j:j+tw]\n",
        "        h5f_a.close()\n",
        "        return data[0:5,:,:], data[5:8,:,:]\n",
        "\n",
        "def batch_MRAE(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    Itrue = im_true.clamp(0.,1.).reshape(N, C*H*W)\n",
        "    Ifake = im_fake.clamp(0.,1.).reshape(N, C*H*W)\n",
        "    mse = nn.MSELoss(reduction='none')\n",
        "    err = mse(Itrue, Ifake).sqrt_().div_(Itrue).sum(dim=1, keepdim=True).div_(C*H*W)\n",
        "    return torch.mean(err)\n",
        "\n",
        "def batch_RMSE(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    Itrue = im_true.clamp(0.,1.).reshape(N, C*H*W)\n",
        "    Ifake = im_fake.clamp(0.,1.).reshape(N, C*H*W)\n",
        "    mse = nn.MSELoss(reduction='none')\n",
        "    err = mse(Itrue, Ifake).sum(dim=1, keepdim=True).div_(C*H*W).sqrt_()\n",
        "    return torch.mean(err)\n",
        "\n",
        "def batch_SID(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    Itrue = im_true.clone().reshape(N, C, H*W)\n",
        "    Ifake = im_fake.clone().reshape(N, C, H*W)\n",
        "    #nom = torch.mul(Itrue, Ifake).sum(dim=1).reshape(N, H*W)\n",
        "    denom1 = torch.pow(Itrue,2).sum(dim=1).sqrt_().reshape(N, H*W)\n",
        "    denom2 = torch.pow(Ifake,2).sum(dim=1).sqrt_().reshape(N, H*W)\n",
        "    #\n",
        "    unit_t = torch.div(Itrue, denom1.unsqueeze(1))\n",
        "    uint_f = torch.div(Ifake, denom2.unsqueeze(1))\n",
        "    #\n",
        "    sid = ((unit_t - uint_f)* (unit_t.log() - uint_f.log())).sum(dim = 1).reshape(N, H*W)\n",
        "    sid = sid.sum() / (N*H*W)\n",
        "    return sid\n",
        "\n",
        "\n",
        "def batch_SAM(im_true, im_fake):\n",
        "    N = im_true.size()[0]\n",
        "    C = im_true.size()[1]\n",
        "    H = im_true.size()[2]\n",
        "    W = im_true.size()[3]\n",
        "    Itrue = im_true.clone().reshape(N, C, H*W)\n",
        "    Ifake = im_fake.clone().reshape(N, C, H*W)\n",
        "    nom = torch.mul(Itrue, Ifake).sum(dim=1).reshape(N, H*W)\n",
        "    denom1 = torch.pow(Itrue,2).sum(dim=1).sqrt_().reshape(N, H*W)\n",
        "    denom2 = torch.pow(Ifake,2).sum(dim=1).sqrt_().reshape(N, H*W)\n",
        "    sam = torch.div(nom, torch.mul(denom1, denom2)).acos_().reshape(N, H*W)\n",
        "    sam = sam / np.pi * 180\n",
        "    sam = torch.sum(sam) / (N*H*W)\n",
        "    return sam\n",
        "\n",
        "\n",
        "def weights_init_kaimingNormal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.kaiming_normal(m.weight.data, a=0.2, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.kaiming_normal(m.weight.data, a=0.2, mode='fan_in')\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal(m.weight.data, 0, 0.01)\n",
        "        nn.init.constant(m.bias.data, 0.0)\n",
        "    elif classname.find('InstanceNorm') != -1:\n",
        "        nn.init.normal(m.weight.data, 0, 0.01)\n",
        "        nn.init.constant(m.bias.data, 0.0)\n",
        "\n",
        "###\n",
        "class conv_relu_res_relu_block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(conv_relu_res_relu_block, self).__init__()\n",
        "        self.conv1 = conv3x3(64, 64)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        #self.relu1 = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.conv2 = conv3x3(64, 64)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        #self.relu2 = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = torch.add(out,residual) \n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "    \n",
        "#####             resblock\n",
        "\n",
        "def conv3x3(in_channels, out_channels):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
        "                     stride=1, padding=1, bias=True)\n",
        "    \n",
        "class resblock(nn.Module):\n",
        "    def __init__(self, block, block_num, input_channel, output_channel):\n",
        "        super(resblock, self).__init__()\n",
        "\n",
        "        self.in_channels = input_channel\n",
        "        self.out_channels = output_channel\n",
        "        self.input_conv = conv3x3(self.in_channels, out_channels=64)  \n",
        "        self.conv_seq = self.make_layer(block, block_num)\n",
        "        self.conv = conv3x3(64, 64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.output_conv = conv3x3(in_channels=64,  out_channels=self.out_channels)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n=m.kernel_size[0]*m.kernel_size[1]*m.out_channels\n",
        "                m.weight.data.normal_(0,sqrt(2./n))# the devide  2./n  carefully  \n",
        "                \n",
        "    def make_layer(self,block,num_layers):\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(block()) # there is a () \n",
        "        return nn.Sequential(*layers)   \n",
        "    \n",
        "    def forward(self, x):\n",
        "       \n",
        "        out = self.input_conv(x)\n",
        "        residual = out\n",
        "        out = self.conv_seq(out)\n",
        "        out = self.conv(out)\n",
        "        out = torch.add(out,residual)  \n",
        "        out = self.relu(out)\n",
        "        out = self.output_conv(out)\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URuT373IXt6l"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6swvuIE-Lhm"
      },
      "source": [
        "## process the data\n",
        "#data_process_list_n(path=path) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERfixWWgWwjh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXBwkZw2xBA9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_V14DTU_2du"
      },
      "source": [
        "##. Only image normalization\n",
        "\n",
        "# Training \n",
        "def train(train_data_loader, model, criterion_1, criterion_2, optimizer, iteration, init_lr ,epoch, w1, w2):\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    average_MRAE = 0.\n",
        "    average_RMSE = 0. \n",
        "    average_SAM = 0.\n",
        "    average_SID =0.\n",
        "    \n",
        "    num=len(train_data_loader)\n",
        "    print('num.{}'.format(num))\n",
        "    #optimizer.zero_grad()\n",
        "    \n",
        "    for i, (labels_gt, images_gt) in enumerate(train_data_loader):\n",
        "    #for i in range(num):\n",
        "        # Normalize the RGB vectors\n",
        "        images_gt = torch.as_tensor(images_gt).float().cuda()\n",
        "        images_gt = images_gt.clamp(1e-8, 1.0 - 1e-8)\n",
        "        # put it on GPU\n",
        "        labels_gt = torch.as_tensor(labels_gt).float().cuda() \n",
        "        labels_gt = labels_gt.clamp(1e-8, 1.0 - 1e-8)\n",
        "        ##\n",
        "        images = images_gt.cuda()\n",
        "        #\n",
        "        labels = labels_gt.cuda()\n",
        "        #\n",
        "        images = Variable(images, requires_grad=True)\n",
        "        labels = Variable(labels, requires_grad=True)    \n",
        "        \n",
        "        # Decaying Learning Rate\n",
        "        lr = init_lr #poly_lr_scheduler(optimizer, init_lr, epoch, max_iter=968000, power=1.5) \n",
        "        iteration = iteration + 1\n",
        "        #\n",
        "        optimizer.zero_grad()\n",
        "        hs_scaled = model.forward(images)\n",
        "        #print(hs)\n",
        "        hs_scaled[hs_scaled.isnan()] = 1e-8\n",
        "        hs_scaled = hs_scaled.clamp(1e-8, float(\"Inf\"))\n",
        "        # remove extremes\n",
        "        hs_scaled[labels==1e-8] = 1e-8\n",
        "        #\n",
        "        if w2 ==0:\n",
        "            loss_1 = criterion_1(labels, hs_scaled)\n",
        "            loss = loss_1*w1\n",
        "        else:\n",
        "            loss_1 = criterion_1(labels, hs_scaled)\n",
        "            loss_2 = criterion_2(labels, hs_scaled)\n",
        "            #\n",
        "            loss = loss_1*w1 + loss_2*w2\n",
        "        ##\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #\n",
        "        #####\n",
        "        MRAE = batch_MRAE(labels, hs_scaled)\n",
        "        RMSE = batch_RMSE(labels, hs_scaled)\n",
        "        SAM = batch_SAM(labels, hs_scaled)\n",
        "        SID = batch_SID(labels, hs_scaled)\n",
        "        #\n",
        "        average_MRAE    += MRAE.item()\n",
        "        average_RMSE    += RMSE.item()\n",
        "        average_SAM    += SAM.item()\n",
        "        average_SID    += SID.item()\n",
        "        #\n",
        "        losses.update(loss.item())\n",
        "    return losses.avg, average_MRAE/num, average_RMSE/num, average_SAM/num, average_SID/num, iteration, lr\n",
        "\n",
        "# Validation\n",
        "\n",
        "def validate(val_loader, model, criterion_1, criterion_2, epoch, w1, w2):\n",
        "    \n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    xxx=0\n",
        "    num = len(val_loader)\n",
        "    \n",
        "    average_MRAE    =0.\n",
        "    average_RMSE    =0.\n",
        "    average_SAM    =0.\n",
        "    average_SID    =0.\n",
        "    \n",
        "    for i, (target_gt, input_gt) in enumerate(val_loader):\n",
        "        # Normalize the RGB vectors\n",
        "        input_gt = torch.as_tensor(input_gt).float().cuda() \n",
        "        input_gt = input_gt.clamp(1e-8, 1.0 - 1e-8)\n",
        "\n",
        "        input = input_gt.cuda()\n",
        "        target = torch.as_tensor(target_gt).float().cuda() \n",
        "        target = target.clamp(1e-8, 1.0 - 1e-8)\n",
        "        #\n",
        "        target = target.cuda(non_blocking=True)\n",
        "        #\n",
        "        input_var = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "    \n",
        "        # compute hs\n",
        "        with torch.no_grad():\n",
        "            hs_scaled = model.forward(input_var)\n",
        "        # normalize the prediction vector\n",
        "        hs_scaled[hs_scaled.isnan()] = 1e-8\n",
        "        hs_scaled = hs_scaled.clamp(1e-8, float(\"Inf\"))\n",
        "        #\n",
        "        # remove extremes\n",
        "        hs_scaled[target_var==1e-8] = 1e-8\n",
        "        #\n",
        "        #loss = criterion(target_var, hs_scaled)\n",
        "        #\n",
        "        if w2 ==0:\n",
        "            loss_1 = criterion_1(target_var, hs_scaled)\n",
        "            loss = loss_1*w1\n",
        "        else:\n",
        "            loss_1 = criterion_1(target_var, hs_scaled)\n",
        "            loss_2 = criterion_2(target_var, hs_scaled)\n",
        "            #\n",
        "            loss = loss_1*1 + loss_2*1 \n",
        "        #####\n",
        "        MRAE = batch_MRAE(target_var, hs_scaled)\n",
        "        RMSE = batch_RMSE(target_var, hs_scaled)\n",
        "        SAM = batch_SAM(target_var, hs_scaled)\n",
        "        SID = batch_SID(target_var, hs_scaled)\n",
        "        #\n",
        "        average_MRAE    += MRAE.item()\n",
        "        average_RMSE    += RMSE.item()\n",
        "        average_SAM    += SAM.item()\n",
        "        average_SID    += SID.item()\n",
        "        #####\n",
        "        ## generate a figure compare the reconstructed spectra and ground truth, every epoch\n",
        "        if epoch%1==0:\n",
        "            if (i+1)%10==0:\n",
        "                xxx += 1\n",
        "                H = target_var.size()[2]\n",
        "                W = target_var.size()[3]\n",
        "                real_spectrum = target_var.data.cpu().numpy()[0,:,int(H/2),int(W/2)]\n",
        "                fake_spectrum = hs_scaled.data.cpu().numpy()[0,:,int(H/2),int(W/2)]\n",
        "                I_spectrum = plot_spectrum(real_spectrum, fake_spectrum, \"val_\"+str(epoch),i)\n",
        "                ##\n",
        "                input_gt_plot = input_gt[0,:,:,:].squeeze().permute(1,2,0).data.cpu().numpy()\n",
        "                fig, axis = plotwithcolorbar(input_gt_plot, \"fake image\" )\n",
        "                fig.savefig(os.path.join(iteration_path, \"{}_test_fake_img_{}.png\".format(epoch,i)))\n",
        "                ##\n",
        "                print(\"val_num: \" + str(xxx))\n",
        "        #  record loss\n",
        "        losses.update(loss.item())\n",
        "    return losses.avg, average_MRAE/num, average_RMSE/num, average_SAM/num, average_SID/num\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh0EVJZ8obC8",
        "outputId": "8a2888f2-99c2-40c0-b156-9d10c61cabe6"
      },
      "source": [
        "###               the best one at the moment\n",
        "# on cases of combined loss functions, try one first and them add the other one \n",
        "\n",
        "##                 the main training part (splitting data to trainng and testing)\n",
        "loss_type = [\"MraeWSid\"]#[\"mrae\", \"rmse\"]\n",
        "for lt in loss_type:\n",
        "    iteration_folder = \"UAV_MaizeAll_jsZ_{}Loss_BV_tcRGB2msi_HSCNNR_iteration\".format(lt)\n",
        "    model_folder = \"UAV_MaizeAll_jsZ_{}Loss_BV_tcRGB2msi_HSCNNR_models\".format(lt)\n",
        "    #loss_type = \"mrae\" #\"mrae\"\n",
        "    iteration_path = os.path.join(os.getcwd(), iteration_folder)\n",
        "    if not os.path.exists(iteration_path):\n",
        "        os.makedirs(iteration_path)\n",
        "    ## new plot function with customed saving path\n",
        "    \n",
        "    cudnn.benchmark = True\n",
        "    ## network architecture\n",
        "    rgb_features = 3\n",
        "    hyper_features = 5\n",
        "    ## load dataset\n",
        "    print(\"\\nloading dataset ...\\n\")\n",
        "    #\n",
        "    trainDataset = HyperDataset_list_maize_RGB(crop_size=600)  ## here not the training data but the whole data set for this work\n",
        "    # set the ratio of training and validation set\n",
        "    validation_split = (1/3)\n",
        "    \n",
        "    dataset_len = len(trainDataset) #trainDataset\n",
        "    indices = list(range(dataset_len))\n",
        "    \n",
        "    # Randomly splitting indices:\n",
        "    val_len = int(np.ceil(validation_split * dataset_len))\n",
        "    validation_idx = np.random.choice(indices, size=val_len, replace=False)\n",
        "    train_idx = list(set(indices) - set(validation_idx))\n",
        "    \n",
        "    ## Defining the samplers for each phase based on the random indices:\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    validation_sampler = SubsetRandomSampler(validation_idx)\n",
        "    \n",
        "    # Data Loader (Input Pipeline)\n",
        "    train_data_loader = DataLoader(dataset=trainDataset,\n",
        "                                    sampler=train_sampler,\n",
        "                                    num_workers=1,  \n",
        "                                    batch_size=1,\n",
        "                                    shuffle=False,\n",
        "                                    pin_memory=True)\n",
        "    \n",
        "    val_loader = DataLoader(dataset=trainDataset,\n",
        "                            sampler=validation_sampler,\n",
        "                            num_workers=1, \n",
        "                            batch_size=1,\n",
        "                            shuffle=False,\n",
        "                            pin_memory=True)\n",
        "    # Model               \n",
        "    model = resblock(conv_relu_res_relu_block, 3, 3,5)\n",
        "    #\n",
        "    model.apply(weights_init_kaimingNormal)\n",
        "    # check whether it is possible to do multi-GPU processing\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    \n",
        "    # Parameters, Loss and Optimizer\n",
        "    start_epoch = 0\n",
        "    end_epoch = 10000\n",
        "    init_lr = 1e-4\n",
        "    \n",
        "    iteration = 0\n",
        "    \n",
        "    ## set the original values for the evaluation\n",
        "    #\n",
        "    record_ts_loss= np.inf\n",
        "    record_ts_MRAE = np.inf\n",
        "    record_ts_RMSE = np.inf\n",
        "    record_ts_SAM = np.inf\n",
        "    record_ts_SID = np.inf #np.inf\n",
        "\n",
        "    # decide which loss function to use\n",
        "    #criterion = nn.MSELoss()\n",
        "    #if lt == \"mrae\":\n",
        "    #    criterion = mrae_loss\n",
        "    #elif lt ==\"rmse\":\n",
        "    #    criterion = nn.MSELoss()\n",
        "    #else:\n",
        "    #    criterion = sid_loss\n",
        "    \n",
        "    criterion_1 = mrae_loss #nn.MSELoss()\n",
        "    criterion_2 = sid_loss #nn.MSELoss()\n",
        "    #\n",
        "    w1 = 1\n",
        "    w2 = 0\n",
        "    optimizer=torch.optim.Adamax(model.parameters(), lr=init_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n",
        "    \n",
        "    model_path = os.path.join(os.getcwd(), model_folder)\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "    loss_csv = open(os.path.join(model_path, '{}_loss_{}.csv'.format(lt, str(f))), 'w+')\n",
        "    \n",
        "    log_dir = os.path.join(model_path,'{}_train_{}.log'.format(lt,str(f)))\n",
        "    logger = initialize_logger(log_dir)\n",
        "    # load the trained model weights if you have\n",
        "    #resume_file = os.path.join(\"UAV_MaizeAll_jsZ_MraeWSidLoss_BV_tcRGB2msi_HSCNNR_models\", \"hscnn_6layer_dim10_ts_loss.pkl\")\n",
        "    resume_file = \"\" \n",
        "    #\n",
        "    if resume_file:\n",
        "        print(\"=> loading checkpoint '{}'\".format(resume_file))\n",
        "        checkpoint = torch.load(resume_file)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        iteration = checkpoint['iter']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        \n",
        "    for epoch in range(start_epoch + 1, end_epoch): ## specify the number of epochs to run\n",
        "        \n",
        "        start_time = time.time()         \n",
        "        tr_loss, tr_MRAE, tr_RMSE, tr_SAM, tr_SID, iteration, lr = train(train_data_loader, model, criterion_1, criterion_2, optimizer, iteration, init_lr, epoch, w1,w2)\n",
        "        #\n",
        "        end_time = time.time() # only record the training time\n",
        "        epoch_time = end_time - start_time\n",
        "        print(datetime.datetime.now())\n",
        "        #lr=init_lr\n",
        "        ts_loss, ts_MRAE, ts_RMSE,ts_SAM, ts_SID = validate(val_loader, model, criterion_1, criterion_2, epoch, w1,w2)\n",
        "        #\n",
        "        #           save a model based on total loss\n",
        "        if ts_loss < record_ts_loss: \n",
        "            record_ts_loss = ts_loss\n",
        "            save_checkpoint_sp(model_path, epoch, iteration, model, optimizer, \"ts_loss\")\n",
        "            record_ts_MRAE_loss = ts_MRAE\n",
        "            record_ts_RMSE_loss = ts_RMSE\n",
        "            record_ts_SAM_loss = ts_SAM\n",
        "            record_ts_SID_loss = ts_SID\n",
        "            lr_loss = init_lr\n",
        "            print(\"ts_loss\")\n",
        "            print(\"Saving total loss\", \"updated_ts_loss\", record_ts_loss,\"updated_ts_MRAE\", record_ts_MRAE_loss, \"updated_ts_RMSE\",record_ts_RMSE_loss, \"updated_ts_SAM\",record_ts_SAM_loss, \"updated_ts_SID\",record_ts_SID_loss, \"LR\", lr_loss)\n",
        "        #           save a model based on MRAE loss\n",
        "        if ts_MRAE < record_ts_MRAE:  \n",
        "            record_ts_MRAE = ts_MRAE\n",
        "            save_checkpoint_sp(model_path, epoch, iteration, model, optimizer, \"ts_mrae\")\n",
        "            record_ts_loss_MRAE = ts_loss\n",
        "            record_ts_RMSE = ts_RMSE\n",
        "            record_ts_SAM_MRAE = ts_SAM\n",
        "            record_ts_SID_MRAE = ts_SID\n",
        "            lr_mrae = init_lr\n",
        "            print(\"ts_MRAE\")\n",
        "            print(\"Saving MRAE\", \"updated_ts_loss\", record_ts_loss_MRAE,\"updated_ts_MRAE\", record_ts_MRAE, \"updated_ts_RMSE\",record_ts_RMSE, \"updated_ts_SAM\",record_ts_SAM_MRAE, \"updated_ts_SID\",record_ts_SID_MRAE, \"LR\", lr_mrae)\n",
        "        #           save a model based on SID loss\n",
        "        if ts_SID < record_ts_SID:  \n",
        "            record_ts_SID = ts_SID\n",
        "            save_checkpoint_sp(model_path, epoch, iteration, model, optimizer, \"ts_sid\")\n",
        "            record_ts_loss_SID = ts_loss\n",
        "            record_ts_MRAE_SID = ts_MRAE\n",
        "            record_ts_RMSE_SID = ts_RMSE\n",
        "            record_ts_SAM = ts_SAM\n",
        "            lr_sid = init_lr\n",
        "            print(\"ts_SID\")\n",
        "            print(\"Saving SID\", \"updated_ts_loss\", record_ts_loss_SID,\"updated_ts_MRAE\", record_ts_MRAE_SID, \"updated_ts_RMSE\",record_ts_RMSE_SID, \"updated_ts_SAM\",record_ts_SAM, \"updated_ts_SID\",record_ts_SID, \"LR\", lr_sid)\n",
        "        # print loss\n",
        "############################################### save model on rice validation \n",
        "        #\n",
        "        # (1) ts_loss evaluation\n",
        "        print (\"Epoch:{}, Iter:{}, Time:{}, learning rate:{}, Train loss:{}, Train MRAE:{}, Train RMSE:{}, Train SAM:{}, Train SID:{}\".format(\n",
        "            epoch, iteration, epoch_time, lr, tr_loss, tr_MRAE, tr_RMSE, tr_SAM, tr_SID))\n",
        "        print (\"Test loss:{}, Test MRAE:{}, Test RMSE:{}, Test SAM:{}, Test SID:{}\".format(ts_loss, ts_MRAE, ts_RMSE, ts_SAM, ts_SID))\n",
        "\n",
        "        logger.info(\"Epoch{}, Iter{}, Time:{}, learning rate:{}, Train loss:{},Train MRAE:{},Train RMSE:{}, Train SAM:{}, Train SID:{}, Test loss:{},Test MRAE:{},Test RMSE:{}, Test SAM:{}, Test SID:{}\".format(\n",
        "            epoch, iteration, epoch_time, lr, tr_loss, tr_MRAE,tr_RMSE, tr_SAM, tr_SID, ts_loss, ts_MRAE,ts_RMSE,ts_SAM, ts_SID))\n",
        "        ###\n",
        "        print(\"w1: {}, w2: {}\".format(w1,w2))\n",
        "        if (epoch +1)%50 ==0:\n",
        "            init_lr = init_lr*0.98\n",
        "            print((\"updated learning rate\", init_lr))\n",
        "        if (epoch+1) % 600 ==0:\n",
        "            clear_output()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "loading dataset ...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:407: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num.101\n",
            "2022-08-05 17:16:22.797693\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.9661073684692383 updated_ts_MRAE 0.9661070681085774 updated_ts_RMSE 0.1977150437294268 updated_ts_SAM 51.198976180132696 updated_ts_SID 15.669321920357499 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.9661073684692383 updated_ts_MRAE 0.9661070681085774 updated_ts_RMSE 0.1977150437294268 updated_ts_SAM 51.198976180132696 updated_ts_SID 15.669321920357499 LR 0.0001\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.9661073684692383 updated_ts_MRAE 0.9661070681085774 updated_ts_RMSE 0.1977150437294268 updated_ts_SAM 51.198976180132696 updated_ts_SID 15.669321920357499 LR 0.0001\n",
            "fold: 0, Epoch:1, Iter:101, Time:14.014204978942871, learning rate:0.0001, Train loss:1.1264142877984755, Train MRAE:1.0768019261926707, Train RMSE:0.21460151067464658, Train SAM:33.4704206296713, Train SID:4.601732792535631\n",
            "Test loss:0.9661073684692383, Test MRAE:0.9661070681085774, Test RMSE:0.1977150437294268, Test SAM:51.198976180132696, Test SID:15.669321920357499\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:16:33.386560\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.9997557132851844 updated_ts_MRAE 0.9997555017471313 updated_ts_RMSE 0.20133638995535233 updated_ts_SAM 24.389921263152477 updated_ts_SID 0.6578697083043117 LR 0.0001\n",
            "fold: 0, Epoch:2, Iter:202, Time:6.386991262435913, learning rate:0.0001, Train loss:0.9974455721307509, Train MRAE:0.9974453177782568, Train RMSE:0.2035326296740239, Train SAM:30.837320308874148, Train SID:3.2090007380978895\n",
            "Test loss:0.9997557132851844, Test MRAE:0.9997555017471313, Test RMSE:0.20133638995535233, Test SAM:24.389921263152477, Test SID:0.6578697083043117\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:16:43.655114\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:3, Iter:303, Time:6.285025596618652, learning rate:0.0001, Train loss:0.9997329227995164, Train MRAE:0.9997327935577619, Train RMSE:0.20408344947465576, Train SAM:26.58867377101785, Train SID:0.8067720240295524\n",
            "Test loss:0.999677932729908, Test MRAE:0.9996777913149666, Test RMSE:0.2013226545908872, Test SAM:24.416887489019654, Test SID:0.6745726933666304\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:16:54.211803\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:4, Iter:404, Time:6.47960090637207, learning rate:0.0001, Train loss:0.9995702652647944, Train MRAE:0.9995701743824648, Train RMSE:0.20405750330721978, Train SAM:26.6455555339851, Train SID:0.842236190444172\n",
            "Test loss:0.9994998784626231, Test MRAE:0.9994998060020746, Test RMSE:0.2012977167671802, Test SAM:24.468271760379565, Test SID:0.7061697040118423\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:17:04.768764\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:5, Iter:505, Time:6.44767951965332, learning rate:0.0001, Train loss:0.9994947438192839, Train MRAE:0.9994947007386992, Train RMSE:0.2040508933881722, Train SAM:26.662076638476684, Train SID:0.8517781617027698\n",
            "Test loss:0.9994337769115672, Test MRAE:0.999433738343856, Test RMSE:0.20129207477850072, Test SAM:24.48824224285051, Test SID:0.7182479354680753\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:17:14.826522\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:6, Iter:606, Time:6.263044357299805, learning rate:0.0001, Train loss:0.9994272855248781, Train MRAE:0.9994272666402383, Train RMSE:0.2040444472343615, Train SAM:26.689016521567165, Train SID:0.8670647811181474\n",
            "Test loss:0.999426287763259, Test MRAE:0.9994262714011997, Test RMSE:0.20129216769162347, Test SAM:24.500474443622664, Test SID:0.7227873697000391\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:17:25.234734\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:7, Iter:707, Time:6.383986711502075, learning rate:0.0001, Train loss:0.9993056885086664, Train MRAE:0.9993056058883667, Train RMSE:0.20403420556299756, Train SAM:26.738525919394917, Train SID:0.8944453195770188\n",
            "Test loss:0.999209439053255, Test MRAE:0.9992092263464835, Test RMSE:0.20127324584652395, Test SAM:24.571312193777047, Test SID:0.7621527045380836\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:17:35.637010\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:8, Iter:808, Time:6.345240592956543, learning rate:0.0001, Train loss:0.9991688362442621, Train MRAE:0.9991687011010576, Train RMSE:0.20402373476783828, Train SAM:26.787917533723434, Train SID:0.9216229499566673\n",
            "Test loss:0.9990999792136398, Test MRAE:0.9990997443012163, Test RMSE:0.20126240861182118, Test SAM:24.61159350825291, Test SID:0.7849713797662773\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:17:45.807052\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:9, Iter:909, Time:6.233046531677246, learning rate:0.0001, Train loss:0.9990759521427721, Train MRAE:0.9990758683421824, Train RMSE:0.20401494824650265, Train SAM:26.81368349094202, Train SID:0.9362221380861679\n",
            "Test loss:0.9989542376761343, Test MRAE:0.9989540530186073, Test RMSE:0.2012360440749748, Test SAM:24.668609245150698, Test SID:0.8211259286777646\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:17:56.188263\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:10, Iter:1010, Time:6.262177467346191, learning rate:0.0001, Train loss:0.9987677123286937, Train MRAE:0.9987674733199695, Train RMSE:0.2039684138380655, Train SAM:26.96831225404645, Train SID:1.0310539703557986\n",
            "Test loss:0.996629887936162, Test MRAE:0.9966273506482443, Test RMSE:0.20093507889439077, Test SAM:26.075542898739087, Test SID:1.6763625624133092\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:18:06.037332\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.9158385279131871 updated_ts_MRAE 0.9158383035192302 updated_ts_RMSE 0.1885706639172984 updated_ts_SAM 55.25667138193168 updated_ts_SID 19.304559595444623 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.9158385279131871 updated_ts_MRAE 0.9158383035192302 updated_ts_RMSE 0.1885706639172984 updated_ts_SAM 55.25667138193168 updated_ts_SID 19.304559595444623 LR 0.0001\n",
            "fold: 0, Epoch:11, Iter:1111, Time:6.257299184799194, learning rate:0.0001, Train loss:0.9516198835750618, Train MRAE:0.951590355670098, Train RMSE:0.19679421215954393, Train SAM:49.52431210432903, Train SID:15.074563319140141\n",
            "Test loss:0.9158385279131871, Test MRAE:0.9158383035192302, Test RMSE:0.1885706639172984, Test SAM:55.25667138193168, Test SID:19.304559595444623\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:18:16.109099\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:12, Iter:1212, Time:6.472316026687622, learning rate:0.0001, Train loss:0.9681226215740242, Train MRAE:0.9681224763983547, Train RMSE:0.19973124991549124, Train SAM:37.98560641071584, Train SID:7.92514306070781\n",
            "Test loss:0.9989041767868341, Test MRAE:0.9989039535615959, Test RMSE:0.20123490369787403, Test SAM:24.711081785314224, Test SID:0.8406131980465907\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:18:26.236935\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:13, Iter:1313, Time:6.54601263999939, learning rate:0.0001, Train loss:0.9988354108121136, Train MRAE:0.9988353169790589, Train RMSE:0.20398678357648378, Train SAM:26.92156423436533, Train SID:0.9977773773198081\n",
            "Test loss:0.9987270750251471, Test MRAE:0.9987269394537982, Test RMSE:0.20120555220865735, Test SAM:24.753012862859986, Test SID:0.866100400686264\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:18:36.818754\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:14, Iter:1414, Time:6.994088649749756, learning rate:0.0001, Train loss:0.9983993951636966, Train MRAE:0.9983993208054269, Train RMSE:0.20391462758035944, Train SAM:27.081952671013255, Train SID:1.0960122621295476\n",
            "Test loss:0.9977788142129487, Test MRAE:0.9977786868226295, Test RMSE:0.2010099826490178, Test SAM:25.17026830187031, Test SID:1.122419536113739\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:18:46.689015\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8791281545863432 updated_ts_MRAE 0.8791277583907632 updated_ts_RMSE 0.18276657280968686 updated_ts_SAM 62.741959740133844 updated_ts_SID 25.032655603745404 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8791281545863432 updated_ts_MRAE 0.8791277583907632 updated_ts_RMSE 0.18276657280968686 updated_ts_SAM 62.741959740133844 updated_ts_SID 25.032655603745404 LR 0.0001\n",
            "fold: 0, Epoch:15, Iter:1515, Time:6.529514312744141, learning rate:0.0001, Train loss:0.9408998926087181, Train MRAE:0.9408971088947636, Train RMSE:0.19548930272017376, Train SAM:47.61595267116434, Train SID:13.932647905727425\n",
            "Test loss:0.8791281545863432, Test MRAE:0.8791277583907632, Test RMSE:0.18276657280968686, Test SAM:62.741959740133844, Test SID:25.032655603745404\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:18:56.907793\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:16, Iter:1616, Time:6.494641304016113, learning rate:0.0001, Train loss:0.922870121970035, Train MRAE:0.9228699720732056, Train RMSE:0.19220584704734311, Train SAM:52.369659489924366, Train SID:16.913303428947337\n",
            "Test loss:0.9990490230859495, Test MRAE:0.9990489073828155, Test RMSE:0.20125475964125464, Test SAM:24.602474998025333, Test SID:0.7733202413016674\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:19:06.721553\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:17, Iter:1717, Time:6.256519556045532, learning rate:0.0001, Train loss:0.9987563514473414, Train MRAE:0.9987562936131317, Train RMSE:0.2039848431797311, Train SAM:26.83880416945656, Train SID:0.9482043633366576\n",
            "Test loss:0.9985538089976591, Test MRAE:0.9985537038129919, Test RMSE:0.20119381067799588, Test SAM:24.698112039005053, Test SID:0.8331086261599672\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:19:17.121661\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:18, Iter:1818, Time:6.492181301116943, learning rate:0.0001, Train loss:0.9983042797239701, Train MRAE:0.9983042307419352, Train RMSE:0.2039302422268556, Train SAM:26.948489576283066, Train SID:1.0177373107117\n",
            "Test loss:0.998117626882067, Test MRAE:0.9981175053353403, Test RMSE:0.2011331056847292, Test SAM:24.773915683521945, Test SID:0.884115664397969\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:19:27.944269\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:19, Iter:1919, Time:6.419848680496216, learning rate:0.0001, Train loss:0.9980782600912718, Train MRAE:0.9980782099289469, Train RMSE:0.20387913434222193, Train SAM:26.93700588339626, Train SID:1.0168279689727444\n",
            "Test loss:0.9979782349923078, Test MRAE:0.9979781683753518, Test RMSE:0.20108800045415468, Test SAM:24.726687599630917, Test SID:0.862715972404854\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:19:39.066597\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:20, Iter:2020, Time:6.951090574264526, learning rate:0.0001, Train loss:0.99789828239101, Train MRAE:0.9978982410808601, Train RMSE:0.20383553501993124, Train SAM:26.91042454407947, Train SID:1.0077979611878347\n",
            "Test loss:0.9977854326659558, Test MRAE:0.9977853625428443, Test RMSE:0.20105776687463126, Test SAM:24.73030299766391, Test SID:0.8676150055492625\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:19:49.370458\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:21, Iter:2121, Time:6.749228000640869, learning rate:0.0001, Train loss:0.9977268948413358, Train MRAE:0.997726853531186, Train RMSE:0.20380901272344118, Train SAM:26.912750263025266, Train SID:1.0124393601228696\n",
            "Test loss:0.9976499980571223, Test MRAE:0.9976499466335073, Test RMSE:0.20104033514565112, Test SAM:24.732189066269818, Test SID:0.8711366045708749\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:19:59.819024\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:22, Iter:2222, Time:6.583849668502808, learning rate:0.0001, Train loss:0.9976624622203336, Train MRAE:0.9976624279919237, Train RMSE:0.2038021698446557, Train SAM:26.913257183414874, Train SID:1.0134551873301516\n",
            "Test loss:0.9975016350839653, Test MRAE:0.9975015813229131, Test RMSE:0.2010208847475987, Test SAM:24.734799515967275, Test SID:0.8757385196639043\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:20:09.810984\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:23, Iter:2323, Time:6.375835657119751, learning rate:0.0001, Train loss:0.9975777652003978, Train MRAE:0.9975777421847428, Train RMSE:0.2037941980184895, Train SAM:26.914080006061216, Train SID:1.0150317576262031\n",
            "Test loss:0.9974581260307163, Test MRAE:0.9974580827881309, Test RMSE:0.20101721204963385, Test SAM:24.73653632519292, Test SID:0.8763921710790372\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:20:20.018985\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:24, Iter:2424, Time:6.295922040939331, learning rate:0.0001, Train loss:0.997558328774896, Train MRAE:0.9975583134311261, Train RMSE:0.20378673607760137, Train SAM:26.913190841674805, Train SID:1.0155703986045157\n",
            "Test loss:0.9975215967963723, Test MRAE:0.9975215745907203, Test RMSE:0.20102663513492136, Test SAM:24.736157211602904, Test SID:0.8741214462355071\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:20:30.046180\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:25, Iter:2525, Time:6.118183374404907, learning rate:0.0001, Train loss:0.9974691112442772, Train MRAE:0.9974690964906523, Train RMSE:0.20378087652791846, Train SAM:26.91404931852133, Train SID:1.0164599111764738\n",
            "Test loss:0.9974171192038292, Test MRAE:0.9974171016730514, Test RMSE:0.20101036014510135, Test SAM:24.735214719585343, Test SID:0.8764320004220102\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:20:41.245379\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:26, Iter:2626, Time:6.604517698287964, learning rate:0.0001, Train loss:0.9974925287879339, Train MRAE:0.9974925199357589, Train RMSE:0.20378092698531575, Train SAM:26.914515788012213, Train SID:1.016974027499114\n",
            "Test loss:0.9973547482023052, Test MRAE:0.9973547283340903, Test RMSE:0.20100418463641523, Test SAM:24.73672533970253, Test SID:0.8781878714467964\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:20:51.597642\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:27, Iter:2727, Time:6.4010608196258545, learning rate:0.0001, Train loss:0.9973872239046758, Train MRAE:0.9973872150525008, Train RMSE:0.2037731347107651, Train SAM:26.915351546636902, Train SID:1.0183388198366259\n",
            "Test loss:0.9972652325443193, Test MRAE:0.9972652173509785, Test RMSE:0.20099187306329316, Test SAM:24.739882001689836, Test SID:0.8828170790391809\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:21:01.846956\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:28, Iter:2828, Time:6.3789215087890625, learning rate:0.0001, Train loss:0.9973907571027775, Train MRAE:0.9973907500210375, Train RMSE:0.20376939894539295, Train SAM:26.91897998469891, Train SID:1.0204059782594737\n",
            "Test loss:0.9973457116706699, Test MRAE:0.9973457069957957, Test RMSE:0.2010004803830502, Test SAM:24.742800282497033, Test SID:0.8810467334354625\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:21:12.144659\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:29, Iter:2929, Time:6.427145957946777, learning rate:0.0001, Train loss:0.9973483923638221, Train MRAE:0.9973483864623721, Train RMSE:0.20376558454319982, Train SAM:26.92705513227104, Train SID:1.0242558930179861\n",
            "Test loss:0.9971726057576198, Test MRAE:0.9971725893955604, Test RMSE:0.20097259563558242, Test SAM:24.772968666226255, Test SID:0.8997444381900862\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:21:22.625498\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:30, Iter:3030, Time:6.979211091995239, learning rate:0.0001, Train loss:0.9968965820746847, Train MRAE:0.9968965785338146, Train RMSE:0.20367382702851058, Train SAM:27.12833304452424, Train SID:1.1453354600632544\n",
            "Test loss:0.9939953123821932, Test MRAE:0.9939952971888524, Test RMSE:0.20014133085222804, Test SAM:26.728206784117457, Test SID:2.0578940241944554\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:21:32.495523\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:31, Iter:3131, Time:6.444714069366455, learning rate:0.0001, Train loss:0.8915172561560527, Train MRAE:0.8915168867252841, Train RMSE:0.18758856999402, Train SAM:61.24538472619387, Train SID:22.52455704283006\n",
            "Test loss:0.8866390223596611, Test MRAE:0.8866390106724757, Test RMSE:0.18221484183096417, Test SAM:62.961943458108344, Test SID:25.480996113197477\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:21:43.739636\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8677138440749225 updated_ts_MRAE 0.8677138405687669 updated_ts_RMSE 0.18084624438893562 updated_ts_SAM 62.971802356196385 updated_ts_SID 25.360184987386067 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8677138440749225 updated_ts_MRAE 0.8677138405687669 updated_ts_RMSE 0.18084624438893562 updated_ts_SAM 62.971802356196385 updated_ts_SID 25.360184987386067 LR 0.0001\n",
            "fold: 0, Epoch:32, Iter:3232, Time:7.045895099639893, learning rate:0.0001, Train loss:0.879115367289817, Train MRAE:0.879115364339092, Train RMSE:0.18620430405187136, Train SAM:63.47489868768371, Train SID:23.953361964461827\n",
            "Test loss:0.8677138440749225, Test MRAE:0.8677138405687669, Test RMSE:0.18084624438893562, Test SAM:62.971802356196385, Test SID:25.360184987386067\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:21:53.699743\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8509127930098889 updated_ts_MRAE 0.8509127930098889 updated_ts_RMSE 0.1801860662651997 updated_ts_SAM 62.94994137333889 updated_ts_SID 25.004691928040746 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8509127930098889 updated_ts_MRAE 0.8509127930098889 updated_ts_RMSE 0.1801860662651997 updated_ts_SAM 62.94994137333889 updated_ts_SID 25.004691928040746 LR 0.0001\n",
            "fold: 0, Epoch:33, Iter:3333, Time:6.473876237869263, learning rate:0.0001, Train loss:0.8924296403875446, Train MRAE:0.8924296327156596, Train RMSE:0.18725207861107174, Train SAM:61.52456699031414, Train SID:22.68812597388088\n",
            "Test loss:0.8509127930098889, Test MRAE:0.8509127930098889, Test RMSE:0.1801860662651997, Test SAM:62.94994137333889, Test SID:25.004691928040746\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:22:03.941596\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:34, Iter:3434, Time:6.234080076217651, learning rate:0.0001, Train loss:0.861237102215833, Train MRAE:0.861237095724238, Train RMSE:0.18477830394069747, Train SAM:64.0858853783938, Train SID:24.410560249101998\n",
            "Test loss:0.8822422343141892, Test MRAE:0.8822422343141892, Test RMSE:0.1827773182999854, Test SAM:62.590447968127684, Test SID:23.932707954855527\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:22:13.826341\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:35, Iter:3535, Time:6.1779725551605225, learning rate:0.0001, Train loss:0.8606358105593389, Train MRAE:0.8606358105593389, Train RMSE:0.18464327079824883, Train SAM:64.20431941570622, Train SID:24.477382924297068\n",
            "Test loss:0.8757251956883598, Test MRAE:0.8757251956883598, Test RMSE:0.18214023434648327, Test SAM:62.74265468821806, Test SID:24.133782742070217\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:22:23.684172\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8420736544272479 updated_ts_MRAE 0.8420736544272479 updated_ts_RMSE 0.17960758448815814 updated_ts_SAM 62.95621542837105 updated_ts_SID 24.987980001112994 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8420736544272479 updated_ts_MRAE 0.8420736544272479 updated_ts_RMSE 0.17960758448815814 updated_ts_SAM 62.95621542837105 updated_ts_SID 24.987980001112994 LR 0.0001\n",
            "fold: 0, Epoch:36, Iter:3636, Time:6.170754909515381, learning rate:0.0001, Train loss:0.860384205190262, Train MRAE:0.860384205190262, Train RMSE:0.18468328808793927, Train SAM:64.18591187732055, Train SID:24.482082253635518\n",
            "Test loss:0.8420736544272479, Test MRAE:0.8420736544272479, Test RMSE:0.17960758448815814, Test SAM:62.95621542837105, Test SID:24.987980001112994\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:22:33.493909\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:37, Iter:3737, Time:6.395095109939575, learning rate:0.0001, Train loss:0.8558715269117072, Train MRAE:0.8558715269117072, Train RMSE:0.18426875696323886, Train SAM:64.2423804632508, Train SID:24.54525090208148\n",
            "Test loss:0.8488229045680925, Test MRAE:0.8488229045680925, Test RMSE:0.17980427806283913, Test SAM:62.95706475949755, Test SID:25.22884739146513\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:22:42.964360\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8389989441516352 updated_ts_MRAE 0.8389989441516352 updated_ts_RMSE 0.1793370281948763 updated_ts_SAM 62.95349375406901 updated_ts_SID 25.070246677772673 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8389989441516352 updated_ts_MRAE 0.8389989441516352 updated_ts_RMSE 0.1793370281948763 updated_ts_SAM 62.95349375406901 updated_ts_SID 25.070246677772673 LR 0.0001\n",
            "fold: 0, Epoch:38, Iter:3838, Time:6.131787300109863, learning rate:0.0001, Train loss:0.8475879194712875, Train MRAE:0.8475879194712875, Train RMSE:0.18378439914472033, Train SAM:64.2632210986449, Train SID:24.600756767952795\n",
            "Test loss:0.8389989441516352, Test MRAE:0.8389989441516352, Test RMSE:0.1793370281948763, Test SAM:62.95349375406901, Test SID:25.070246677772673\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:22:53.113108\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.8385493930648354 updated_ts_MRAE 0.8385493930648354 updated_ts_RMSE 0.17929508113393597 updated_ts_SAM 62.93874426449047 updated_ts_SID 25.0379965913062 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.8385493930648354 updated_ts_MRAE 0.8385493930648354 updated_ts_RMSE 0.17929508113393597 updated_ts_SAM 62.93874426449047 updated_ts_SID 25.0379965913062 LR 0.0001\n",
            "fold: 0, Epoch:39, Iter:3939, Time:6.26633095741272, learning rate:0.0001, Train loss:0.8491735948194371, Train MRAE:0.8491735948194371, Train RMSE:0.1838492911936033, Train SAM:64.25532939646504, Train SID:24.560526252973197\n",
            "Test loss:0.8385493930648354, Test MRAE:0.8385493930648354, Test RMSE:0.17929508113393597, Test SAM:62.93874426449047, Test SID:25.0379965913062\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:23:03.942737\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.7893485976200477 updated_ts_MRAE 0.7893485976200477 updated_ts_RMSE 0.17538019138223984 updated_ts_SAM 59.67781418445063 updated_ts_SID 21.81648317972819 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.7893485976200477 updated_ts_MRAE 0.7893485976200477 updated_ts_RMSE 0.17538019138223984 updated_ts_SAM 59.67781418445063 updated_ts_SID 21.81648317972819 LR 0.0001\n",
            "fold: 0, Epoch:40, Iter:4040, Time:5.867874622344971, learning rate:0.0001, Train loss:0.8465674124141731, Train MRAE:0.8465674118240281, Train RMSE:0.18361765205269992, Train SAM:63.942208205119215, Train SID:24.238895775067924\n",
            "Test loss:0.7893485976200477, Test MRAE:0.7893485976200477, Test RMSE:0.17538019138223984, Test SAM:59.67781418445063, Test SID:21.81648317972819\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:23:13.990518\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.7597418976765052 updated_ts_MRAE 0.7597418976765052 updated_ts_RMSE 0.1728174426392013 updated_ts_SAM 58.34751069312002 updated_ts_SID 20.437137192370844 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.7597418976765052 updated_ts_MRAE 0.7597418976765052 updated_ts_RMSE 0.1728174426392013 updated_ts_SAM 58.34751069312002 updated_ts_SID 20.437137192370844 LR 0.0001\n",
            "fold: 0, Epoch:41, Iter:4141, Time:6.209107398986816, learning rate:0.0001, Train loss:0.7867392218939149, Train MRAE:0.7867392207136249, Train RMSE:0.17773230167308657, Train SAM:60.245801151388946, Train SID:20.82780928658967\n",
            "Test loss:0.7597418976765052, Test MRAE:0.7597418976765052, Test RMSE:0.1728174426392013, Test SAM:58.34751069312002, Test SID:20.437137192370844\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:23:24.289453\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.6443051216649074 updated_ts_MRAE 0.6443051204961889 updated_ts_RMSE 0.155774195988973 updated_ts_SAM 50.70918565637925 updated_ts_SID 15.558381716410318 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.6443051216649074 updated_ts_MRAE 0.6443051204961889 updated_ts_RMSE 0.155774195988973 updated_ts_SAM 50.70918565637925 updated_ts_SID 15.558381716410318 LR 0.0001\n",
            "fold: 0, Epoch:42, Iter:4242, Time:6.222846984863281, learning rate:0.0001, Train loss:0.7023120404470085, Train MRAE:0.7023120404470085, Train RMSE:0.17040389584432733, Train SAM:56.63271920987875, Train SID:18.026942338093672\n",
            "Test loss:0.6443051216649074, Test MRAE:0.6443051204961889, Test RMSE:0.155774195988973, Test SAM:50.70918565637925, Test SID:15.558381716410318\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:23:34.637271\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.6314898156652263 updated_ts_MRAE 0.6314898156652263 updated_ts_RMSE 0.15541850658608417 updated_ts_SAM 51.23161315917969 updated_ts_SID 15.71897938672234 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.6314898156652263 updated_ts_MRAE 0.6314898156652263 updated_ts_RMSE 0.15541850658608417 updated_ts_SAM 51.23161315917969 updated_ts_SID 15.71897938672234 LR 0.0001\n",
            "fold: 0, Epoch:43, Iter:4343, Time:6.188103437423706, learning rate:0.0001, Train loss:0.6823824685398895, Train MRAE:0.6823824685398895, Train RMSE:0.1668581764886875, Train SAM:55.509018850798654, Train SID:17.285706019637608\n",
            "Test loss:0.6314898156652263, Test MRAE:0.6314898156652263, Test RMSE:0.15541850658608417, Test SAM:51.23161315917969, Test SID:15.71897938672234\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:23:44.620016\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:44, Iter:4444, Time:6.190550088882446, learning rate:0.0001, Train loss:0.635214655706198, Train MRAE:0.635214655706198, Train RMSE:0.16152293721933175, Train SAM:53.327886449228416, Train SID:15.936161872183922\n",
            "Test loss:0.6630355061269274, Test MRAE:0.6630355061269274, Test RMSE:0.15839114317706987, Test SAM:52.130170709946576, Test SID:15.859698015100816\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:23:54.578206\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5854606254428041 updated_ts_MRAE 0.5854606254428041 updated_ts_RMSE 0.15284648756770527 updated_ts_SAM 50.284202351289636 updated_ts_SID 15.3287312002743 LR 0.0001\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5854606254428041 updated_ts_MRAE 0.5854606254428041 updated_ts_RMSE 0.15284648756770527 updated_ts_SAM 50.284202351289636 updated_ts_SID 15.3287312002743 LR 0.0001\n",
            "fold: 0, Epoch:45, Iter:4545, Time:6.17593240737915, learning rate:0.0001, Train loss:0.6335564586195616, Train MRAE:0.6335564586195616, Train RMSE:0.16088063137070968, Train SAM:53.03708773320264, Train SID:15.80125111872607\n",
            "Test loss:0.5854606254428041, Test MRAE:0.5854606254428041, Test RMSE:0.15284648756770527, Test SAM:50.284202351289636, Test SID:15.3287312002743\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:24:04.847546\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:46, Iter:4646, Time:6.261260747909546, learning rate:0.0001, Train loss:0.6195412083427505, Train MRAE:0.6195412083427505, Train RMSE:0.16007319085373736, Train SAM:52.78533421884669, Train SID:15.738940833818795\n",
            "Test loss:0.7122041816804924, Test MRAE:0.7122041816804924, Test RMSE:0.16845948672762104, Test SAM:56.790616802140775, Test SID:19.22952169530532\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:24:15.141586\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:47, Iter:4747, Time:6.41293740272522, learning rate:0.0001, Train loss:0.6769571888564837, Train MRAE:0.6769571888564837, Train RMSE:0.1677321461167666, Train SAM:56.28173129393323, Train SID:18.286590595056516\n",
            "Test loss:0.5894691780501721, Test MRAE:0.5894691780501721, Test RMSE:0.15213247213293524, Test SAM:49.873699936212276, Test SID:15.249803561790317\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:24:24.948231\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:48, Iter:4848, Time:6.2204225063323975, learning rate:0.0001, Train loss:0.6110802003652742, Train MRAE:0.6110802003652742, Train RMSE:0.15940059020672695, Train SAM:52.56791230003432, Train SID:15.663419931241782\n",
            "Test loss:0.6466851970728706, Test MRAE:0.6466851970728706, Test RMSE:0.15849906542137557, Test SAM:52.97561278997683, Test SID:16.282726867526186\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:24:35.268396\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:49, Iter:4949, Time:6.162189483642578, learning rate:0.0001, Train loss:0.6206184956106809, Train MRAE:0.6206184956106809, Train RMSE:0.16051300128202628, Train SAM:53.06889449015702, Train SID:15.85423497870417\n",
            "Test loss:0.608375608921051, Test MRAE:0.608375608921051, Test RMSE:0.15468408079708323, Test SAM:51.29001131244734, Test SID:15.51111389608944\n",
            "w1: 1, w2: 0\n",
            "('updated learning rate', 9.8e-05)\n",
            "num.101\n",
            "2022-08-05 17:24:45.246849\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5713066248332753 updated_ts_MRAE 0.5713066248332753 updated_ts_RMSE 0.15148852823996076 updated_ts_SAM 49.710579068053 updated_ts_SID 15.115698365604176 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5713066248332753 updated_ts_MRAE 0.5713066248332753 updated_ts_RMSE 0.15148852823996076 updated_ts_SAM 49.710579068053 updated_ts_SID 15.115698365604176 LR 9.8e-05\n",
            "fold: 0, Epoch:50, Iter:5050, Time:6.127903938293457, learning rate:9.8e-05, Train loss:0.6253259848840166, Train MRAE:0.6253259848840166, Train RMSE:0.16007160653572272, Train SAM:52.74194286837436, Train SID:15.646941883729236\n",
            "Test loss:0.5713066248332753, Test MRAE:0.5713066248332753, Test RMSE:0.15148852823996076, Test SAM:49.710579068053, Test SID:15.115698365604176\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:24:55.330183\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:51, Iter:5151, Time:6.483074426651001, learning rate:9.8e-05, Train loss:0.5874719578440827, Train MRAE:0.5874719578440827, Train RMSE:0.1582663816095579, Train SAM:52.16206167239954, Train SID:15.51847240712383\n",
            "Test loss:0.5713668816229877, Test MRAE:0.5713668816229877, Test RMSE:0.15238619610375048, Test SAM:49.98931630452474, Test SID:15.157417185166302\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:25:04.887889\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:52, Iter:5252, Time:6.17449951171875, learning rate:9.8e-05, Train loss:0.5975789389397839, Train MRAE:0.5975789389397839, Train RMSE:0.15869050716409588, Train SAM:52.28728269822527, Train SID:15.556383812781608\n",
            "Test loss:0.5933337036301108, Test MRAE:0.5933337036301108, Test RMSE:0.15144779460102903, Test SAM:49.67559672336952, Test SID:15.216335726719276\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:25:14.779474\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:53, Iter:5353, Time:6.1154608726501465, learning rate:9.8e-05, Train loss:0.5854307655060645, Train MRAE:0.5854307655060645, Train RMSE:0.15790214596113356, Train SAM:52.154233347071276, Train SID:15.502288978878815\n",
            "Test loss:0.5793104849609674, Test MRAE:0.5793104849609674, Test RMSE:0.1522658908484029, Test SAM:49.78190149045458, Test SID:15.032980395298377\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:25:24.411140\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5561362832200294 updated_ts_MRAE 0.5561362832200294 updated_ts_RMSE 0.15117505908596748 updated_ts_SAM 49.44580766266468 updated_ts_SID 15.02836244246539 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5561362832200294 updated_ts_MRAE 0.5561362832200294 updated_ts_RMSE 0.15117505908596748 updated_ts_SAM 49.44580766266468 updated_ts_SID 15.02836244246539 LR 9.8e-05\n",
            "fold: 0, Epoch:54, Iter:5454, Time:6.22691798210144, learning rate:9.8e-05, Train loss:0.577970150378671, Train MRAE:0.577970150378671, Train RMSE:0.15773245065224054, Train SAM:51.89274344113794, Train SID:15.45067569052819\n",
            "Test loss:0.5561362832200294, Test MRAE:0.5561362832200294, Test RMSE:0.15117505908596748, Test SAM:49.44580766266468, Test SID:15.02836244246539\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:25:35.159728\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:55, Iter:5555, Time:6.164079666137695, learning rate:9.8e-05, Train loss:0.5765961565003537, Train MRAE:0.5765961565003537, Train RMSE:0.15778014757255515, Train SAM:52.01931834456944, Train SID:15.472692631258823\n",
            "Test loss:0.5943559828926536, Test MRAE:0.5943559828926536, Test RMSE:0.1536931765137934, Test SAM:50.50486022350835, Test SID:15.220275916305242\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:25:45.137302\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:56, Iter:5656, Time:6.603759765625, learning rate:9.8e-05, Train loss:0.6085527819572109, Train MRAE:0.6085527819572109, Train RMSE:0.15895706112727082, Train SAM:52.54024131699364, Train SID:15.647927057625044\n",
            "Test loss:0.6023283314471152, Test MRAE:0.6023283314471152, Test RMSE:0.15134215997714623, Test SAM:49.07553070666743, Test SID:15.259549215728162\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:25:54.714295\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5427969571422128 updated_ts_MRAE 0.5427969571422128 updated_ts_RMSE 0.1499237925106404 updated_ts_SAM 49.154690387202244 updated_ts_SID 15.098668902528052 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5427969571422128 updated_ts_MRAE 0.5427969571422128 updated_ts_RMSE 0.1499237925106404 updated_ts_SAM 49.154690387202244 updated_ts_SID 15.098668902528052 LR 9.8e-05\n",
            "fold: 0, Epoch:57, Iter:5757, Time:6.177402496337891, learning rate:9.8e-05, Train loss:0.5806740298129545, Train MRAE:0.5806740298129545, Train RMSE:0.15762808955837004, Train SAM:51.980746694130474, Train SID:15.478069447054722\n",
            "Test loss:0.5427969571422128, Test MRAE:0.5427969571422128, Test RMSE:0.1499237925106404, Test SAM:49.154690387202244, Test SID:15.098668902528052\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:26:04.942981\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:58, Iter:5858, Time:6.2330169677734375, learning rate:9.8e-05, Train loss:0.5660859431960795, Train MRAE:0.5660859431960795, Train RMSE:0.15711833285813284, Train SAM:51.76873968143274, Train SID:15.421878323696628\n",
            "Test loss:0.564452096527698, Test MRAE:0.564452096527698, Test RMSE:0.1518142267185099, Test SAM:49.822808583577476, Test SID:15.079071774202234\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:26:14.663022\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.5104964720267876 updated_ts_MRAE 0.5104964720267876 updated_ts_RMSE 0.15165744327446995 updated_ts_SAM 48.49864825080423 updated_ts_SID 12.544013546962365 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.5104964720267876 updated_ts_MRAE 0.5104964720267876 updated_ts_RMSE 0.15165744327446995 updated_ts_SAM 48.49864825080423 updated_ts_SID 12.544013546962365 LR 9.8e-05\n",
            "fold: 0, Epoch:59, Iter:5959, Time:5.8887951374053955, learning rate:9.8e-05, Train loss:0.5393587398056937, Train MRAE:0.5393587398056937, Train RMSE:0.15580564929117072, Train SAM:51.07628586268661, Train SID:14.119256992151241\n",
            "Test loss:0.5104964720267876, Test MRAE:0.5104964720267876, Test RMSE:0.15165744327446995, Test SAM:48.49864825080423, Test SID:12.544013546962365\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:26:24.584558\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.49087798829172175 updated_ts_MRAE 0.49087798829172175 updated_ts_RMSE 0.14936195898289775 updated_ts_SAM 48.04962150723326 updated_ts_SID 12.39591069314994 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.49087798829172175 updated_ts_MRAE 0.49087798829172175 updated_ts_RMSE 0.14936195898289775 updated_ts_SAM 48.04962150723326 updated_ts_SID 12.39591069314994 LR 9.8e-05\n",
            "fold: 0, Epoch:60, Iter:6060, Time:6.198780059814453, learning rate:9.8e-05, Train loss:0.48185515669312806, Train MRAE:0.48185515669312806, Train RMSE:0.15393732753720615, Train SAM:50.26561204513701, Train SID:13.02284068872433\n",
            "Test loss:0.49087798829172175, Test MRAE:0.49087798829172175, Test RMSE:0.14936195898289775, Test SAM:48.04962150723326, Test SID:12.39591069314994\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:26:34.865526\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.43100928149971307 updated_ts_MRAE 0.43100928149971307 updated_ts_RMSE 0.1461302413075578 updated_ts_SAM 47.34894053141276 updated_ts_SID 12.254063494065228 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.43100928149971307 updated_ts_MRAE 0.43100928149971307 updated_ts_RMSE 0.1461302413075578 updated_ts_SAM 47.34894053141276 updated_ts_SID 12.254063494065228 LR 9.8e-05\n",
            "fold: 0, Epoch:61, Iter:6161, Time:6.432716131210327, learning rate:9.8e-05, Train loss:0.4707798352926084, Train MRAE:0.4707798352926084, Train RMSE:0.15340697839118467, Train SAM:50.09860573192634, Train SID:12.89656948807216\n",
            "Test loss:0.43100928149971307, Test MRAE:0.43100928149971307, Test RMSE:0.1461302413075578, Test SAM:47.34894053141276, Test SID:12.254063494065228\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:26:44.666125\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:62, Iter:6262, Time:6.22239351272583, learning rate:9.8e-05, Train loss:0.4727310889428205, Train MRAE:0.4727310889428205, Train RMSE:0.15357672718196813, Train SAM:50.033286292954244, Train SID:12.932146214022495\n",
            "Test loss:0.44218967124527575, Test MRAE:0.44218967124527575, Test RMSE:0.1462809022735147, Test SAM:47.56715639899759, Test SID:12.43224723666322\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:26:54.791645\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:63, Iter:6363, Time:6.219882965087891, learning rate:9.8e-05, Train loss:0.4459488250241421, Train MRAE:0.4459488250241421, Train RMSE:0.15260994574516126, Train SAM:49.78659646817953, Train SID:12.770883182487866\n",
            "Test loss:0.4322516970774707, Test MRAE:0.4322516970774707, Test RMSE:0.14525856574376425, Test SAM:47.08834008609547, Test SID:12.241488494125067\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:27:04.402326\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.4128805784618153 updated_ts_MRAE 0.4128805784618153 updated_ts_RMSE 0.14502317341519336 updated_ts_SAM 47.03058930939319 updated_ts_SID 12.190494387757544 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.4128805784618153 updated_ts_MRAE 0.4128805784618153 updated_ts_RMSE 0.14502317341519336 updated_ts_SAM 47.03058930939319 updated_ts_SID 12.190494387757544 LR 9.8e-05\n",
            "fold: 0, Epoch:64, Iter:6464, Time:6.257542610168457, learning rate:9.8e-05, Train loss:0.4531180374693162, Train MRAE:0.4531180374693162, Train RMSE:0.15276455421849053, Train SAM:49.806039451372506, Train SID:12.750757670638585\n",
            "Test loss:0.4128805784618153, Test MRAE:0.4128805784618153, Test RMSE:0.14502317341519336, Test SAM:47.03058930939319, Test SID:12.190494387757544\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:27:14.536032\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:65, Iter:6565, Time:6.195782899856567, learning rate:9.8e-05, Train loss:0.44310129396986253, Train MRAE:0.44310129396986253, Train RMSE:0.15245236685075383, Train SAM:49.681687609984145, Train SID:12.71774172074724\n",
            "Test loss:0.4620509691098157, Test MRAE:0.4620509691098157, Test RMSE:0.14597533365675047, Test SAM:46.717453077727676, Test SID:12.27755963568594\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:27:24.188769\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.40532353812573 updated_ts_MRAE 0.40532353812573 updated_ts_RMSE 0.14458258508467206 updated_ts_SAM 46.79312537698185 updated_ts_SID 12.206317247128954 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.40532353812573 updated_ts_MRAE 0.40532353812573 updated_ts_RMSE 0.14458258508467206 updated_ts_SAM 46.79312537698185 updated_ts_SID 12.206317247128954 LR 9.8e-05\n",
            "fold: 0, Epoch:66, Iter:6666, Time:6.238814830780029, learning rate:9.8e-05, Train loss:0.43983471393585205, Train MRAE:0.43983471393585205, Train RMSE:0.15247552923046717, Train SAM:49.689651036026454, Train SID:12.728515058460802\n",
            "Test loss:0.40532353812573, Test MRAE:0.40532353812573, Test RMSE:0.14458258508467206, Test SAM:46.79312537698185, Test SID:12.206317247128954\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:27:34.558730\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:67, Iter:6767, Time:6.251483678817749, learning rate:9.8e-05, Train loss:0.449423618835978, Train MRAE:0.449423618835978, Train RMSE:0.15291649671179233, Train SAM:49.76791853951936, Train SID:12.724956219739253\n",
            "Test loss:0.40764561821432677, Test MRAE:0.40764561821432677, Test RMSE:0.14504551127845167, Test SAM:46.98391020531748, Test SID:12.161002664005055\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:27:44.553401\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.40049293228224214 updated_ts_MRAE 0.40049293228224214 updated_ts_RMSE 0.14455855798487569 updated_ts_SAM 46.77161773980833 updated_ts_SID 12.174879466786104 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.40049293228224214 updated_ts_MRAE 0.40049293228224214 updated_ts_RMSE 0.14455855798487569 updated_ts_SAM 46.77161773980833 updated_ts_SID 12.174879466786104 LR 9.8e-05\n",
            "fold: 0, Epoch:68, Iter:6868, Time:6.253933668136597, learning rate:9.8e-05, Train loss:0.43446597663482817, Train MRAE:0.43446597663482817, Train RMSE:0.1522759742075854, Train SAM:49.594988341378695, Train SID:12.70172112531001\n",
            "Test loss:0.40049293228224214, Test MRAE:0.40049293228224214, Test RMSE:0.14455855798487569, Test SAM:46.77161773980833, Test SID:12.174879466786104\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:27:54.847124\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:69, Iter:6969, Time:6.2045464515686035, learning rate:9.8e-05, Train loss:0.4257870003728583, Train MRAE:0.4257870003728583, Train RMSE:0.15200123058097198, Train SAM:49.51046647175704, Train SID:12.670574141020822\n",
            "Test loss:0.41236863825835435, Test MRAE:0.41236863825835435, Test RMSE:0.14571561810432695, Test SAM:47.03037082447725, Test SID:12.084432433633243\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:28:06.685810\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:70, Iter:7070, Time:6.9480507373809814, learning rate:9.8e-05, Train loss:0.4357873543654338, Train MRAE:0.4357873543654338, Train RMSE:0.1523314184776627, Train SAM:49.59274730115834, Train SID:12.692066579762072\n",
            "Test loss:0.4194311774244495, Test MRAE:0.4194311774244495, Test RMSE:0.14454210695682787, Test SAM:46.77624392041973, Test SID:12.20209888383454\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:28:16.164195\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:71, Iter:7171, Time:5.902253866195679, learning rate:9.8e-05, Train loss:0.4209531566294113, Train MRAE:0.4209531566294113, Train RMSE:0.15178923714574022, Train SAM:49.44051096699025, Train SID:12.67930571867688\n",
            "Test loss:0.4095499439566743, Test MRAE:0.4095499439566743, Test RMSE:0.14507915795433754, Test SAM:47.044917985504746, Test SID:12.156082658206715\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:28:25.693044\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.3895690359321295 updated_ts_MRAE 0.3895690359321295 updated_ts_RMSE 0.14407039053884207 updated_ts_SAM 46.66678933536305 updated_ts_SID 12.154159022312538 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.3895690359321295 updated_ts_MRAE 0.3895690359321295 updated_ts_RMSE 0.14407039053884207 updated_ts_SAM 46.66678933536305 updated_ts_SID 12.154159022312538 LR 9.8e-05\n",
            "fold: 0, Epoch:72, Iter:7272, Time:5.929851055145264, learning rate:9.8e-05, Train loss:0.45036489833699594, Train MRAE:0.45036489833699594, Train RMSE:0.15222166906489004, Train SAM:49.61409200536143, Train SID:12.730287457456683\n",
            "Test loss:0.3895690359321295, Test MRAE:0.3895690359321295, Test RMSE:0.14407039053884207, Test SAM:46.66678933536305, Test SID:12.154159022312538\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:28:35.088260\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:73, Iter:7373, Time:5.901916742324829, learning rate:9.8e-05, Train loss:0.4156560042116902, Train MRAE:0.4156560042116902, Train RMSE:0.15160162201022157, Train SAM:49.40336809063902, Train SID:12.65316119052396\n",
            "Test loss:0.401034428208482, Test MRAE:0.401034428208482, Test RMSE:0.14482698005204106, Test SAM:46.967306772867836, Test SID:12.134903552485447\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:28:44.468912\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.3857104953597574 updated_ts_MRAE 0.3857104953597574 updated_ts_RMSE 0.14420298969044404 updated_ts_SAM 46.7126532161937 updated_ts_SID 12.106296371011172 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.3857104953597574 updated_ts_MRAE 0.3857104953597574 updated_ts_RMSE 0.14420298969044404 updated_ts_SAM 46.7126532161937 updated_ts_SID 12.106296371011172 LR 9.8e-05\n",
            "fold: 0, Epoch:74, Iter:7474, Time:5.919611692428589, learning rate:9.8e-05, Train loss:0.41287299577552494, Train MRAE:0.41287299577552494, Train RMSE:0.1515500049779911, Train SAM:49.37492023128094, Train SID:12.64133094560982\n",
            "Test loss:0.3857104953597574, Test MRAE:0.3857104953597574, Test RMSE:0.14420298969044404, Test SAM:46.7126532161937, Test SID:12.106296371011172\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:28:54.263211\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:75, Iter:7575, Time:5.899359941482544, learning rate:9.8e-05, Train loss:0.4118590720809332, Train MRAE:0.4118590720809332, Train RMSE:0.15146180220169597, Train SAM:49.32237209659992, Train SID:12.636948462760094\n",
            "Test loss:0.38665751674596, Test MRAE:0.38665751674596, Test RMSE:0.1443525114480187, Test SAM:46.70165559357288, Test SID:12.091214759677063\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:29:03.651328\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:76, Iter:7676, Time:5.908416986465454, learning rate:9.8e-05, Train loss:0.429502792877726, Train MRAE:0.429502792877726, Train RMSE:0.15194831072989076, Train SAM:49.521171720901336, Train SID:12.686984742041862\n",
            "Test loss:0.3865446495074852, Test MRAE:0.3865446495074852, Test RMSE:0.1444496160336569, Test SAM:46.845466314577585, Test SID:12.090034671858245\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:29:13.097752\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:77, Iter:7777, Time:5.903580188751221, learning rate:9.8e-05, Train loss:0.41041345377959826, Train MRAE:0.41041345377959826, Train RMSE:0.15143894212375772, Train SAM:49.31427005730053, Train SID:12.631793116578962\n",
            "Test loss:0.4244294990511501, Test MRAE:0.4244294990511501, Test RMSE:0.14457564771759743, Test SAM:46.49146629782284, Test SID:12.222575112885119\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:29:22.738230\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:78, Iter:7878, Time:5.917295694351196, learning rate:9.8e-05, Train loss:0.41727845060943375, Train MRAE:0.41727845060943375, Train RMSE:0.15154766254495866, Train SAM:49.32012452229415, Train SID:12.637617791053092\n",
            "Test loss:0.42439357556548774, Test MRAE:0.42439357556548774, Test RMSE:0.14613980390861922, Test SAM:47.2185624814501, Test SID:12.092970810684504\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:29:32.099717\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:79, Iter:7979, Time:5.910886287689209, learning rate:9.8e-05, Train loss:0.41593451488136063, Train MRAE:0.41593451488136063, Train RMSE:0.1514687496836823, Train SAM:49.340963118147144, Train SID:12.63409938434563\n",
            "Test loss:0.38795482644847795, Test MRAE:0.38795482644847795, Test RMSE:0.14379341257553474, Test SAM:46.58820028866039, Test SID:12.165309270222982\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:29:41.536110\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:80, Iter:8080, Time:5.946467161178589, learning rate:9.8e-05, Train loss:0.4318588370143777, Train MRAE:0.4318588370143777, Train RMSE:0.15208961054830267, Train SAM:49.426024786316525, Train SID:12.661017285715236\n",
            "Test loss:0.38646404825004876, Test MRAE:0.38646404825004876, Test RMSE:0.1445514797872188, Test SAM:46.927062389897365, Test SID:12.098848772983924\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:29:51.022379\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.3800857008672228 updated_ts_MRAE 0.3800857008672228 updated_ts_RMSE 0.14409322583792256 updated_ts_SAM 46.7525832232307 updated_ts_SID 12.125921511182598 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.3800857008672228 updated_ts_MRAE 0.3800857008672228 updated_ts_RMSE 0.14409322583792256 updated_ts_SAM 46.7525832232307 updated_ts_SID 12.125921511182598 LR 9.8e-05\n",
            "fold: 0, Epoch:81, Iter:8181, Time:5.938925266265869, learning rate:9.8e-05, Train loss:0.4099696641511256, Train MRAE:0.4099696641511256, Train RMSE:0.15130601845460362, Train SAM:49.32610989561175, Train SID:12.64283040490481\n",
            "Test loss:0.3800857008672228, Test MRAE:0.3800857008672228, Test RMSE:0.14409322583792256, Test SAM:46.7525832232307, Test SID:12.125921511182598\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:30:00.621401\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.3758673913338605 updated_ts_MRAE 0.3758673913338605 updated_ts_RMSE 0.1440758843924485 updated_ts_SAM 46.664720049091414 updated_ts_SID 12.076369173386517 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.3758673913338605 updated_ts_MRAE 0.3758673913338605 updated_ts_RMSE 0.1440758843924485 updated_ts_SAM 46.664720049091414 updated_ts_SID 12.076369173386517 LR 9.8e-05\n",
            "fold: 0, Epoch:82, Iter:8282, Time:5.9200146198272705, learning rate:9.8e-05, Train loss:0.39599085414763724, Train MRAE:0.39599085414763724, Train RMSE:0.15110735528834976, Train SAM:49.21263269858785, Train SID:12.613374426813408\n",
            "Test loss:0.3758673913338605, Test MRAE:0.3758673913338605, Test RMSE:0.1440758843924485, Test SAM:46.664720049091414, Test SID:12.076369173386517\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:30:10.050949\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:83, Iter:8383, Time:5.907881259918213, learning rate:9.8e-05, Train loss:0.39726534338280706, Train MRAE:0.39726534338280706, Train RMSE:0.1511897111293113, Train SAM:49.238826109631226, Train SID:12.61118713227829\n",
            "Test loss:0.3769088510204764, Test MRAE:0.3769088510204764, Test RMSE:0.14357083086289613, Test SAM:46.55085642197553, Test SID:12.134833279777975\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:30:19.313863\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.37205931310560186 updated_ts_MRAE 0.37205931310560186 updated_ts_RMSE 0.1438182827596571 updated_ts_SAM 46.61090566597733 updated_ts_SID 12.067115204007019 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.37205931310560186 updated_ts_MRAE 0.37205931310560186 updated_ts_RMSE 0.1438182827596571 updated_ts_SAM 46.61090566597733 updated_ts_SID 12.067115204007019 LR 9.8e-05\n",
            "fold: 0, Epoch:84, Iter:8484, Time:5.900663375854492, learning rate:9.8e-05, Train loss:0.40029825461973056, Train MRAE:0.40029825461973056, Train RMSE:0.15098098333519283, Train SAM:49.171399749151554, Train SID:12.604469167123927\n",
            "Test loss:0.37205931310560186, Test MRAE:0.37205931310560186, Test RMSE:0.1438182827596571, Test SAM:46.61090566597733, Test SID:12.067115204007019\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:30:28.699433\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:85, Iter:8585, Time:5.9179441928863525, learning rate:9.8e-05, Train loss:0.39138981257334793, Train MRAE:0.39138981257334793, Train RMSE:0.1508196002923616, Train SAM:49.146382095790145, Train SID:12.59988805563143\n",
            "Test loss:0.37339316045536713, Test MRAE:0.37339316045536713, Test RMSE:0.14411865101725446, Test SAM:46.607779858159084, Test SID:12.054175414291082\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:30:38.289960\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:86, Iter:8686, Time:5.91227388381958, learning rate:9.8e-05, Train loss:0.3913451846283261, Train MRAE:0.3913451846283261, Train RMSE:0.1508793535799083, Train SAM:49.153393924826446, Train SID:12.596347280067972\n",
            "Test loss:0.38896968960762024, Test MRAE:0.38896968960762024, Test RMSE:0.14372071989026725, Test SAM:46.50501011867149, Test SID:12.157417129067813\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:30:48.904688\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:87, Iter:8787, Time:5.906840562820435, learning rate:9.8e-05, Train loss:0.39062115815606446, Train MRAE:0.39062115815606446, Train RMSE:0.1509495466062338, Train SAM:49.15871055527489, Train SID:12.607523710420816\n",
            "Test loss:0.3769454453505722, Test MRAE:0.3769454453505722, Test RMSE:0.14390049407295152, Test SAM:46.52968013987822, Test SID:12.063516785116757\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:30:58.274067\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:88, Iter:8888, Time:5.960703611373901, learning rate:9.8e-05, Train loss:0.39578075692205145, Train MRAE:0.39578075692205145, Train RMSE:0.15092084512557133, Train SAM:49.16621300725654, Train SID:12.597447282016867\n",
            "Test loss:0.3774959602776696, Test MRAE:0.3774959602776696, Test RMSE:0.1433510390274665, Test SAM:46.44430803785137, Test SID:12.153494778801413\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:31:07.660613\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:89, Iter:8989, Time:5.91396164894104, learning rate:9.8e-05, Train loss:0.3977736568096841, Train MRAE:0.3977736568096841, Train RMSE:0.15076272728124468, Train SAM:49.135879629909404, Train SID:12.609430341437312\n",
            "Test loss:0.3984691646753573, Test MRAE:0.3984691646753573, Test RMSE:0.14485447561624004, Test SAM:46.85346423878389, Test SID:12.030214421889362\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:31:17.203180\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.36090726945914475 updated_ts_MRAE 0.36090726945914475 updated_ts_RMSE 0.14345823053051443 updated_ts_SAM 46.456901999080884 updated_ts_SID 12.076331811792711 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.36090726945914475 updated_ts_MRAE 0.36090726945914475 updated_ts_RMSE 0.14345823053051443 updated_ts_SAM 46.456901999080884 updated_ts_SID 12.076331811792711 LR 9.8e-05\n",
            "fold: 0, Epoch:90, Iter:9090, Time:5.904301643371582, learning rate:9.8e-05, Train loss:0.3967122205413214, Train MRAE:0.3967122205413214, Train RMSE:0.1510726060755182, Train SAM:49.15145934454285, Train SID:12.59714436295009\n",
            "Test loss:0.36090726945914475, Test MRAE:0.36090726945914475, Test RMSE:0.14345823053051443, Test SAM:46.456901999080884, Test SID:12.076331811792711\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:31:26.912007\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.36034369351817114 updated_ts_MRAE 0.36034369293381185 updated_ts_RMSE 0.14346019426981607 updated_ts_SAM 46.505537070480045 updated_ts_SID 12.086468864889707 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.36034369351817114 updated_ts_MRAE 0.36034369293381185 updated_ts_RMSE 0.14346019426981607 updated_ts_SAM 46.505537070480045 updated_ts_SID 12.086468864889707 LR 9.8e-05\n",
            "fold: 0, Epoch:91, Iter:9191, Time:5.912661790847778, learning rate:9.8e-05, Train loss:0.3847853364330707, Train MRAE:0.3847853364330707, Train RMSE:0.15054137650692817, Train SAM:49.09700340799766, Train SID:12.595126189807853\n",
            "Test loss:0.36034369351817114, Test MRAE:0.36034369293381185, Test RMSE:0.14346019426981607, Test SAM:46.505537070480045, Test SID:12.086468864889707\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:31:36.372436\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:92, Iter:9292, Time:5.941600799560547, learning rate:9.8e-05, Train loss:0.38254315605258, Train MRAE:0.38254315605258, Train RMSE:0.15069625305362266, Train SAM:49.081358012586534, Train SID:12.589028622844431\n",
            "Test loss:0.3648901757072, Test MRAE:0.3648901757072, Test RMSE:0.14359482363158582, Test SAM:46.57145264569451, Test SID:12.096740030774884\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:31:45.852701\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:93, Iter:9393, Time:5.912883043289185, learning rate:9.8e-05, Train loss:0.3878381795222216, Train MRAE:0.3878381795222216, Train RMSE:0.1507641832427223, Train SAM:49.10031856876789, Train SID:12.589183816815368\n",
            "Test loss:0.36265917034710154, Test MRAE:0.36265917034710154, Test RMSE:0.1436633556789043, Test SAM:46.56000316844267, Test SID:12.074331358367322\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:31:55.231941\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:94, Iter:9494, Time:5.943877935409546, learning rate:9.8e-05, Train loss:0.3758113995046899, Train MRAE:0.3758113995046899, Train RMSE:0.15045683617048924, Train SAM:49.0707218434551, Train SID:12.584895719396005\n",
            "Test loss:0.3638199310676724, Test MRAE:0.3638199310676724, Test RMSE:0.14349734929262423, Test SAM:46.56260209925034, Test SID:12.109051891401702\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:32:04.608154\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:95, Iter:9595, Time:5.91262674331665, learning rate:9.8e-05, Train loss:0.382427970961769, Train MRAE:0.382427970961769, Train RMSE:0.15075366781784758, Train SAM:49.0832257034755, Train SID:12.583275341751552\n",
            "Test loss:0.37405039983637195, Test MRAE:0.37405039983637195, Test RMSE:0.14378317255599826, Test SAM:46.61956113927505, Test SID:12.11075522852879\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:32:14.001919\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:96, Iter:9696, Time:5.917175769805908, learning rate:9.8e-05, Train loss:0.3773285522319303, Train MRAE:0.3773285522319303, Train RMSE:0.15044730739428266, Train SAM:49.058672045717145, Train SID:12.58667258460923\n",
            "Test loss:0.3635982847681232, Test MRAE:0.363598284183764, Test RMSE:0.14341116141454846, Test SAM:46.49262357225605, Test SID:12.11072618821088\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:32:23.346091\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:97, Iter:9797, Time:5.9183313846588135, learning rate:9.8e-05, Train loss:0.3712704476743641, Train MRAE:0.3712704476743641, Train RMSE:0.1503328661812414, Train SAM:49.02116839720471, Train SID:12.582907790004617\n",
            "Test loss:0.3757130719867407, Test MRAE:0.37571307140238147, Test RMSE:0.14407174055482827, Test SAM:46.52081059474571, Test SID:12.009934163561054\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:32:32.679226\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:98, Iter:9898, Time:5.914047718048096, learning rate:9.8e-05, Train loss:0.37343907828378203, Train MRAE:0.37343907828378203, Train RMSE:0.15041260598319592, Train SAM:49.01626137459632, Train SID:12.580707512279549\n",
            "Test loss:0.3742409927003524, Test MRAE:0.3742409921159931, Test RMSE:0.14339130576334747, Test SAM:46.40335255043179, Test SID:12.148342039070878\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:32:42.006628\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.3547527661510542 updated_ts_MRAE 0.3547527649823357 updated_ts_RMSE 0.1433750702177777 updated_ts_SAM 46.4702688478956 updated_ts_SID 12.07205248814003 LR 9.8e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.3547527661510542 updated_ts_MRAE 0.3547527649823357 updated_ts_RMSE 0.1433750702177777 updated_ts_SAM 46.4702688478956 updated_ts_SID 12.07205248814003 LR 9.8e-05\n",
            "fold: 0, Epoch:99, Iter:9999, Time:5.912475109100342, learning rate:9.8e-05, Train loss:0.3723307499791136, Train MRAE:0.3723307499791136, Train RMSE:0.15040466853297582, Train SAM:49.01567228713838, Train SID:12.581616694384282\n",
            "Test loss:0.3547527661510542, Test MRAE:0.3547527649823357, Test RMSE:0.1433750702177777, Test SAM:46.4702688478956, Test SID:12.07205248814003\n",
            "w1: 1, w2: 0\n",
            "('updated learning rate', 9.604e-05)\n",
            "num.101\n",
            "2022-08-05 17:32:51.428673\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:100, Iter:10100, Time:5.922044277191162, learning rate:9.604e-05, Train loss:0.3787529152808803, Train MRAE:0.3787529152808803, Train RMSE:0.15044564886553452, Train SAM:49.04492036422881, Train SID:12.58802090068855\n",
            "Test loss:0.3600154437270819, Test MRAE:0.36001544314272266, Test RMSE:0.1437296096016379, Test SAM:46.48560826918658, Test SID:12.01769224802653\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:33:00.771242\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:101, Iter:10201, Time:5.931452512741089, learning rate:9.604e-05, Train loss:0.3681380754650229, Train MRAE:0.3681380754650229, Train RMSE:0.15021274873230717, Train SAM:48.98376087150952, Train SID:12.580366502894034\n",
            "Test loss:0.4034704840650745, Test MRAE:0.4034704823119968, Test RMSE:0.14535997559626898, Test SAM:46.78667808981503, Test SID:11.97352289686016\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:33:10.145374\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:102, Iter:10302, Time:5.925312280654907, learning rate:9.604e-05, Train loss:0.39224990936789184, Train MRAE:0.39224990936789184, Train RMSE:0.15068139666968053, Train SAM:49.05298312347714, Train SID:12.586912693363605\n",
            "Test loss:0.36579236972565743, Test MRAE:0.3657923685569389, Test RMSE:0.14377415267860189, Test SAM:46.47525174010034, Test SID:12.005743569018794\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:33:19.485313\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.3503587380343792 updated_ts_MRAE 0.3503587368656607 updated_ts_RMSE 0.14298400721129248 updated_ts_SAM 46.35216866287531 updated_ts_SID 12.101943558337641 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.3503587380343792 updated_ts_MRAE 0.3503587368656607 updated_ts_RMSE 0.14298400721129248 updated_ts_SAM 46.35216866287531 updated_ts_SID 12.101943558337641 LR 9.604e-05\n",
            "fold: 0, Epoch:103, Iter:10403, Time:5.92297625541687, learning rate:9.604e-05, Train loss:0.36604316281800225, Train MRAE:0.36604316281800225, Train RMSE:0.15019595202538047, Train SAM:48.97288645376073, Train SID:12.579594612121582\n",
            "Test loss:0.3503587380343792, Test MRAE:0.3503587368656607, Test RMSE:0.14298400721129248, Test SAM:46.35216866287531, Test SID:12.101943558337641\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:33:28.925096\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:104, Iter:10504, Time:5.923363208770752, learning rate:9.604e-05, Train loss:0.3836805823415813, Train MRAE:0.3836805823415813, Train RMSE:0.15059865870983294, Train SAM:49.053731672834644, Train SID:12.579564198408976\n",
            "Test loss:0.3612077943250245, Test MRAE:0.36120779315630597, Test RMSE:0.14325332743864433, Test SAM:46.38926995969286, Test SID:12.114625201505774\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:33:38.269301\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:105, Iter:10605, Time:5.933247804641724, learning rate:9.604e-05, Train loss:0.39184879400942585, Train MRAE:0.39184879400942585, Train RMSE:0.15093198477631747, Train SAM:49.060293745286394, Train SID:12.59194015276314\n",
            "Test loss:0.3594297182326223, Test MRAE:0.3594297170639038, Test RMSE:0.14358022137015475, Test SAM:46.49168807385015, Test SID:12.047409506405101\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:33:47.640096\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.34775977274950814 updated_ts_MRAE 0.3477597715807896 updated_ts_RMSE 0.14280790119778877 updated_ts_SAM 46.31207813936121 updated_ts_SID 12.10240612778009 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.34775977274950814 updated_ts_MRAE 0.3477597715807896 updated_ts_RMSE 0.14280790119778877 updated_ts_SAM 46.31207813936121 updated_ts_SID 12.10240612778009 LR 9.604e-05\n",
            "fold: 0, Epoch:106, Iter:10706, Time:5.917866230010986, learning rate:9.604e-05, Train loss:0.369131481588477, Train MRAE:0.369131481588477, Train RMSE:0.15024454603985984, Train SAM:48.974371579614015, Train SID:12.570279451880124\n",
            "Test loss:0.34775977274950814, Test MRAE:0.3477597715807896, Test RMSE:0.14280790119778877, Test SAM:46.31207813936121, Test SID:12.10240612778009\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:33:57.039124\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.34400289374239307 updated_ts_MRAE 0.3440028925736745 updated_ts_RMSE 0.14281051620548846 updated_ts_SAM 46.306093926523246 updated_ts_SID 12.08662633334889 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.34400289374239307 updated_ts_MRAE 0.3440028925736745 updated_ts_RMSE 0.14281051620548846 updated_ts_SAM 46.306093926523246 updated_ts_SID 12.08662633334889 LR 9.604e-05\n",
            "fold: 0, Epoch:107, Iter:10807, Time:5.927562952041626, learning rate:9.604e-05, Train loss:0.3684704280135655, Train MRAE:0.3684704280135655, Train RMSE:0.15032462775707245, Train SAM:48.94940797409209, Train SID:12.576088112179596\n",
            "Test loss:0.34400289374239307, Test MRAE:0.3440028925736745, Test RMSE:0.14281051620548846, Test SAM:46.306093926523246, Test SID:12.08662633334889\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:34:06.476013\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:108, Iter:10908, Time:5.933183431625366, learning rate:9.604e-05, Train loss:0.41529553125400354, Train MRAE:0.41529553125400354, Train RMSE:0.1511358871760935, Train SAM:49.102262062601525, Train SID:12.600971108615989\n",
            "Test loss:0.36591265248317345, Test MRAE:0.36591265131445494, Test RMSE:0.14389797592279957, Test SAM:46.57214318069757, Test SID:12.044740396387438\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:34:17.363449\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:109, Iter:11009, Time:5.9337944984436035, learning rate:9.604e-05, Train loss:0.3669639126499101, Train MRAE:0.3669639126499101, Train RMSE:0.1501011917054063, Train SAM:48.97084993419081, Train SID:12.57707398480708\n",
            "Test loss:0.35056990151311834, Test MRAE:0.3505698991756813, Test RMSE:0.14306414945452822, Test SAM:46.377034953996244, Test SID:12.076250637278838\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:34:26.805046\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:110, Iter:11110, Time:5.9229278564453125, learning rate:9.604e-05, Train loss:0.3711108438449331, Train MRAE:0.3711108438449331, Train RMSE:0.149418669983302, Train SAM:48.80907595039594, Train SID:12.27695893769217\n",
            "Test loss:0.35905177336113125, Test MRAE:0.3590517634270238, Test RMSE:0.13695194426120497, Test SAM:45.08297587376015, Test SID:9.80421303767784\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:34:36.222703\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.19946735746720257 updated_ts_MRAE 0.1994672893893485 updated_ts_RMSE 0.049773758605999106 updated_ts_SAM 8.572663877524581 updated_ts_SID 0.11451118790051516 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.19946735746720257 updated_ts_MRAE 0.1994672893893485 updated_ts_RMSE 0.049773758605999106 updated_ts_SAM 8.572663877524581 updated_ts_SID 0.11451118790051516 LR 9.604e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.19946735746720257 updated_ts_MRAE 0.1994672893893485 updated_ts_RMSE 0.049773758605999106 updated_ts_SAM 8.572663877524581 updated_ts_SID 0.11451118790051516 LR 9.604e-05\n",
            "fold: 0, Epoch:111, Iter:11211, Time:5.926053285598755, learning rate:9.604e-05, Train loss:0.24101864450638838, Train MRAE:0.24101860054058605, Train RMSE:0.07094403680893455, Train SAM:16.177718006738342, Train SID:1.285558017763761\n",
            "Test loss:0.19946735746720257, Test MRAE:0.1994672893893485, Test RMSE:0.049773758605999106, Test SAM:8.572663877524581, Test SID:0.11451118790051516\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:34:45.693218\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.1942698032248254 updated_ts_MRAE 0.19426975472300662 updated_ts_RMSE 0.04532761659984495 updated_ts_SAM 8.567966405083151 updated_ts_SID 0.10469941079032187 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.1942698032248254 updated_ts_MRAE 0.19426975472300662 updated_ts_RMSE 0.04532761659984495 updated_ts_SAM 8.567966405083151 updated_ts_SID 0.10469941079032187 LR 9.604e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.1942698032248254 updated_ts_MRAE 0.19426975472300662 updated_ts_RMSE 0.04532761659984495 updated_ts_SAM 8.567966405083151 updated_ts_SID 0.10469941079032187 LR 9.604e-05\n",
            "fold: 0, Epoch:112, Iter:11312, Time:5.92897629737854, learning rate:9.604e-05, Train loss:0.2151599828854646, Train MRAE:0.21515997285299962, Train RMSE:0.05084172221324822, Train SAM:8.784434970062557, Train SID:0.1298255980531178\n",
            "Test loss:0.1942698032248254, Test MRAE:0.19426975472300662, Test RMSE:0.04532761659984495, Test SAM:8.567966405083151, Test SID:0.10469941079032187\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:34:55.227409\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:113, Iter:11413, Time:5.933878660202026, learning rate:9.604e-05, Train loss:0.19686373507622446, Train MRAE:0.19686373124028198, Train RMSE:0.04654674213564042, Train SAM:7.896055353750096, Train SID:0.10315058535278433\n",
            "Test loss:0.1966454278604657, Test MRAE:0.19664539747378407, Test RMSE:0.04468646671111677, Test SAM:8.725264109817205, Test SID:0.11008767842077742\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:35:04.569934\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.1988946729723145 updated_ts_MRAE 0.19889466596000335 updated_ts_RMSE 0.04637890751017075 updated_ts_SAM 8.14628080293244 updated_ts_SID 0.09659671476658653 LR 9.604e-05\n",
            "fold: 0, Epoch:114, Iter:11514, Time:5.940172433853149, learning rate:9.604e-05, Train loss:0.1984909409933751, Train MRAE:0.19849093627221515, Train RMSE:0.04505723334922649, Train SAM:7.770634065760245, Train SID:0.10100096995287602\n",
            "Test loss:0.1988946729723145, Test MRAE:0.19889466596000335, Test RMSE:0.04637890751017075, Test SAM:8.14628080293244, Test SID:0.09659671476658653\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:35:13.967222\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.17610003112577924 updated_ts_MRAE 0.17610001826987548 updated_ts_RMSE 0.039874890855714384 updated_ts_SAM 7.256172900106392 updated_ts_SID 0.07991438899554458 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.17610003112577924 updated_ts_MRAE 0.17610001826987548 updated_ts_RMSE 0.039874890855714384 updated_ts_SAM 7.256172900106392 updated_ts_SID 0.07991438899554458 LR 9.604e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.17610003112577924 updated_ts_MRAE 0.17610001826987548 updated_ts_RMSE 0.039874890855714384 updated_ts_SAM 7.256172900106392 updated_ts_SID 0.07991438899554458 LR 9.604e-05\n",
            "fold: 0, Epoch:115, Iter:11615, Time:5.92647385597229, learning rate:9.604e-05, Train loss:0.20251961620432316, Train MRAE:0.20251960277852446, Train RMSE:0.04441297932131456, Train SAM:7.808339458881038, Train SID:0.10991023160000839\n",
            "Test loss:0.17610003112577924, Test MRAE:0.17610001826987548, Test RMSE:0.039874890855714384, Test SAM:7.256172900106392, Test SID:0.07991438899554458\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:35:23.418805\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.17430778052292617 updated_ts_MRAE 0.17430775393457973 updated_ts_RMSE 0.0388976206966475 updated_ts_SAM 6.960426919600543 updated_ts_SID 0.07625818851531721 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.17430778052292617 updated_ts_MRAE 0.17430775393457973 updated_ts_RMSE 0.0388976206966475 updated_ts_SAM 6.960426919600543 updated_ts_SID 0.07625818851531721 LR 9.604e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.17430778052292617 updated_ts_MRAE 0.17430775393457973 updated_ts_RMSE 0.0388976206966475 updated_ts_SAM 6.960426919600543 updated_ts_SID 0.07625818851531721 LR 9.604e-05\n",
            "fold: 0, Epoch:116, Iter:11716, Time:5.935123682022095, learning rate:9.604e-05, Train loss:0.19487533998666423, Train MRAE:0.19487532065941557, Train RMSE:0.04353073543619992, Train SAM:7.663404899068398, Train SID:0.1004336520218023\n",
            "Test loss:0.17430778052292617, Test MRAE:0.17430775393457973, Test RMSE:0.0388976206966475, Test SAM:6.960426919600543, Test SID:0.07625818851531721\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:35:32.802035\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.17331755950170405 updated_ts_MRAE 0.173317546061441 updated_ts_RMSE 0.03915713347640692 updated_ts_SAM 6.955391154569738 updated_ts_SID 0.07729881205687336 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.17331755950170405 updated_ts_MRAE 0.173317546061441 updated_ts_RMSE 0.03915713347640692 updated_ts_SAM 6.955391154569738 updated_ts_SID 0.07729881205687336 LR 9.604e-05\n",
            "fold: 0, Epoch:117, Iter:11817, Time:5.9389026165008545, learning rate:9.604e-05, Train loss:0.1848311630068439, Train MRAE:0.18483112405727406, Train RMSE:0.04188173363852029, Train SAM:7.201982545380545, Train SID:0.0884597100833855\n",
            "Test loss:0.17331755950170405, Test MRAE:0.173317546061441, Test RMSE:0.03915713347640692, Test SAM:6.955391154569738, Test SID:0.07729881205687336\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:35:42.200434\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:118, Iter:11918, Time:5.946730613708496, learning rate:9.604e-05, Train loss:0.1867094722272146, Train MRAE:0.18670943991677597, Train RMSE:0.04222303403928728, Train SAM:7.228708899847351, Train SID:0.0883108548967555\n",
            "Test loss:0.1975932255679486, Test MRAE:0.1975930955480127, Test RMSE:0.04135131963766089, Test SAM:8.385442126031016, Test SID:0.1053065567770425\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:35:51.564963\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:119, Iter:12019, Time:5.943258047103882, learning rate:9.604e-05, Train loss:0.19648328880862434, Train MRAE:0.1964832054506434, Train RMSE:0.04235096598541973, Train SAM:7.58956784541064, Train SID:0.0978618320262078\n",
            "Test loss:0.17488137150512023, Test MRAE:0.17488135192908494, Test RMSE:0.04021076459948923, Test SAM:7.45508461372525, Test SID:0.0843457290354897\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:36:00.966012\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:120, Iter:12120, Time:5.936804533004761, learning rate:9.604e-05, Train loss:0.19607864371915856, Train MRAE:0.19607849146174913, Train RMSE:0.04186331465987876, Train SAM:7.486752037954803, Train SID:0.09702245241934711\n",
            "Test loss:0.21233838109993466, Test MRAE:0.21233823793191536, Test RMSE:0.039258663962576906, Test SAM:7.602869576098872, Test SID:0.0869630989344681\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:36:10.384847\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.17141172625854903 updated_ts_MRAE 0.17141165175274306 updated_ts_RMSE 0.03751309183152283 updated_ts_SAM 6.817486360961316 updated_ts_SID 0.07307933161363882 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.17141172625854903 updated_ts_MRAE 0.17141165175274306 updated_ts_RMSE 0.03751309183152283 updated_ts_SAM 6.817486360961316 updated_ts_SID 0.07307933161363882 LR 9.604e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.17141172625854903 updated_ts_MRAE 0.17141165175274306 updated_ts_RMSE 0.03751309183152283 updated_ts_SAM 6.817486360961316 updated_ts_SID 0.07307933161363882 LR 9.604e-05\n",
            "fold: 0, Epoch:121, Iter:12221, Time:5.946341514587402, learning rate:9.604e-05, Train loss:0.19018103919997073, Train MRAE:0.19018088502459007, Train RMSE:0.04104352047168972, Train SAM:7.2782279477261085, Train SID:0.09249650502558981\n",
            "Test loss:0.17141172625854903, Test MRAE:0.17141165175274306, Test RMSE:0.03751309183152283, Test SAM:6.817486360961316, Test SID:0.07307933161363882\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:36:19.822977\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:122, Iter:12322, Time:5.936837911605835, learning rate:9.604e-05, Train loss:0.1896402629295198, Train MRAE:0.18964000341325704, Train RMSE:0.041301848495950796, Train SAM:7.1204187279880635, Train SID:0.08826236471091167\n",
            "Test loss:0.17546151359291637, Test MRAE:0.17546146742853463, Test RMSE:0.040629413709336634, Test SAM:6.933586700289857, Test SID:0.07593412529311928\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:36:29.166938\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.1652429952633147 updated_ts_MRAE 0.1652429073172457 updated_ts_RMSE 0.03796183336160931 updated_ts_SAM 6.769325583588843 updated_ts_SID 0.07347919339058447 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.1652429952633147 updated_ts_MRAE 0.1652429073172457 updated_ts_RMSE 0.03796183336160931 updated_ts_SAM 6.769325583588843 updated_ts_SID 0.07347919339058447 LR 9.604e-05\n",
            "fold: 0, Epoch:123, Iter:12423, Time:5.9321208000183105, learning rate:9.604e-05, Train loss:0.18161168657612092, Train MRAE:0.18161145656710803, Train RMSE:0.0400344374598843, Train SAM:6.88683800177999, Train SID:0.08354755542655982\n",
            "Test loss:0.1652429952633147, Test MRAE:0.1652429073172457, Test RMSE:0.03796183336160931, Test SAM:6.769325583588843, Test SID:0.07347919339058447\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:36:38.645363\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:124, Iter:12524, Time:5.941227436065674, learning rate:9.604e-05, Train loss:0.1972809891299446, Train MRAE:0.19728075410469925, Train RMSE:0.04058942102854795, Train SAM:7.152223823094132, Train SID:0.09115809630049337\n",
            "Test loss:0.19126289966059665, Test MRAE:0.19126262267430624, Test RMSE:0.038610501945310946, Test SAM:7.513648388432522, Test SID:0.08641943715366662\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:36:48.049284\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.16870817483640185 updated_ts_MRAE 0.1687078414594426 updated_ts_RMSE 0.03613041103908829 updated_ts_SAM 6.524785154006061 updated_ts_SID 0.06984503070513408 LR 9.604e-05\n",
            "fold: 0, Epoch:125, Iter:12625, Time:5.953604698181152, learning rate:9.604e-05, Train loss:0.1848944061906031, Train MRAE:0.18489392477982114, Train RMSE:0.04001606655961806, Train SAM:6.9377700031393825, Train SID:0.08457934380610391\n",
            "Test loss:0.16870817483640185, Test MRAE:0.1687078414594426, Test RMSE:0.03613041103908829, Test SAM:6.524785154006061, Test SID:0.06984503070513408\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:36:57.494222\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:126, Iter:12726, Time:5.956933975219727, learning rate:9.604e-05, Train loss:0.1945902818795478, Train MRAE:0.19458993295631785, Train RMSE:0.040361200468522485, Train SAM:7.137716316940761, Train SID:0.09214698250340943\n",
            "Test loss:0.1855287218795103, Test MRAE:0.18552811619113474, Test RMSE:0.038109143501987644, Test SAM:7.407920014624502, Test SID:0.08671570401273522\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:37:06.920092\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:127, Iter:12827, Time:5.943830966949463, learning rate:9.604e-05, Train loss:0.19580654025372893, Train MRAE:0.19580617355238092, Train RMSE:0.04045011186142369, Train SAM:7.327307380071961, Train SID:0.09145038508542694\n",
            "Test loss:0.17034489573801265, Test MRAE:0.1703445316821921, Test RMSE:0.0374060443730331, Test SAM:6.721693440979602, Test SID:0.07350438336531322\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:37:16.302550\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:128, Iter:12928, Time:5.956293106079102, learning rate:9.604e-05, Train loss:0.18517249608689015, Train MRAE:0.18517177249535477, Train RMSE:0.03986775023069712, Train SAM:6.985271784338621, Train SID:0.08575921653225871\n",
            "Test loss:0.17417865772457683, Test MRAE:0.17417828928606183, Test RMSE:0.03794890072415857, Test SAM:6.975722079183541, Test SID:0.07943420930235993\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:37:25.713518\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:129, Iter:13029, Time:5.9610631465911865, learning rate:9.604e-05, Train loss:0.18432492002992346, Train MRAE:0.18432433844202817, Train RMSE:0.03968059964994393, Train SAM:6.923266066182958, Train SID:0.08375155239707173\n",
            "Test loss:0.1951592184749304, Test MRAE:0.19515855493498782, Test RMSE:0.037365668106312845, Test SAM:7.017058409896552, Test SID:0.07772447928494099\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:37:35.138530\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.1737818726721932 updated_ts_MRAE 0.17378134324270136 updated_ts_RMSE 0.03560455713201972 updated_ts_SAM 6.422031598932603 updated_ts_SID 0.06733777585859392 LR 9.604e-05\n",
            "fold: 0, Epoch:130, Iter:13130, Time:5.944598197937012, learning rate:9.604e-05, Train loss:0.20089296000723791, Train MRAE:0.20089223988280439, Train RMSE:0.040262892786966695, Train SAM:7.40832765503685, Train SID:0.10152318188459566\n",
            "Test loss:0.1737818726721932, Test MRAE:0.17378134324270136, Test RMSE:0.03560455713201972, Test SAM:6.422031598932603, Test SID:0.06733777585859392\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:37:44.544137\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:131, Iter:13231, Time:5.944459438323975, learning rate:9.604e-05, Train loss:0.18920346069158894, Train MRAE:0.18920292358587285, Train RMSE:0.039750826351418356, Train SAM:7.07353152851067, Train SID:0.0881517568584716\n",
            "Test loss:0.17097734849826962, Test MRAE:0.17097618679205576, Test RMSE:0.03679755560177214, Test SAM:6.900434101329131, Test SID:0.08138927342552765\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:37:53.892330\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:132, Iter:13332, Time:5.946595191955566, learning rate:9.604e-05, Train loss:0.18068024936583962, Train MRAE:0.18067935389457362, Train RMSE:0.0388953038076363, Train SAM:6.648600219499947, Train SID:0.07819920106984601\n",
            "Test loss:0.1694122719414094, Test MRAE:0.16941176179577322, Test RMSE:0.03709660733447356, Test SAM:6.7261367311664655, Test SID:0.07258308956436083\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:38:03.293977\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:133, Iter:13433, Time:5.95907187461853, learning rate:9.604e-05, Train loss:0.1893051517481851, Train MRAE:0.18930434944606064, Train RMSE:0.038844981350668585, Train SAM:6.831629337650715, Train SID:0.08467939805866469\n",
            "Test loss:0.18318235581996395, Test MRAE:0.18318122493870118, Test RMSE:0.03901553847918324, Test SAM:7.81274601057464, Test SID:0.09908985616821869\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:38:12.686512\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:134, Iter:13534, Time:5.949883699417114, learning rate:9.604e-05, Train loss:0.18122330853844634, Train MRAE:0.181222634076482, Train RMSE:0.03865666016198621, Train SAM:6.596677213612169, Train SID:0.07922702290043973\n",
            "Test loss:0.17195765133581908, Test MRAE:0.17195729721410602, Test RMSE:0.03866051320059627, Test SAM:6.60960428387511, Test SID:0.06997339033028659\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:38:22.037877\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:135, Iter:13635, Time:5.946953058242798, learning rate:9.604e-05, Train loss:0.17804500512262383, Train MRAE:0.17804430993181644, Train RMSE:0.03819758832307145, Train SAM:6.503071407280346, Train SID:0.07639807731945916\n",
            "Test loss:0.1757730013307403, Test MRAE:0.17577089924438327, Test RMSE:0.038402575962975916, Test SAM:7.527321619146011, Test SID:0.09054668846667982\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:38:33.320974\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:136, Iter:13736, Time:5.951288938522339, learning rate:9.604e-05, Train loss:0.18086222249387515, Train MRAE:0.18086128888448866, Train RMSE:0.03843964330188119, Train SAM:6.711105115342848, Train SID:0.08011804089540302\n",
            "Test loss:0.18101741154404247, Test MRAE:0.18101689321737663, Test RMSE:0.03886075106029417, Test SAM:6.929393992704504, Test SID:0.079680632653774\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:38:42.759224\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:137, Iter:13837, Time:5.954683542251587, learning rate:9.604e-05, Train loss:0.1944943810305973, Train MRAE:0.19449363405456638, Train RMSE:0.03969435187259523, Train SAM:6.992084989453306, Train SID:0.09058520835962626\n",
            "Test loss:0.17064986640916152, Test MRAE:0.1706484966710502, Test RMSE:0.035820515972434304, Test SAM:6.372729301452637, Test SID:0.06751935593053407\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:38:52.177806\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:138, Iter:13938, Time:5.952338218688965, learning rate:9.604e-05, Train loss:0.1724146378069821, Train MRAE:0.17241378866209842, Train RMSE:0.03690791646442791, Train SAM:6.357853488166733, Train SID:0.07460426955972568\n",
            "Test loss:0.17557636765288373, Test MRAE:0.17557531346877417, Test RMSE:0.037692217996307446, Test SAM:7.3130524111729045, Test SID:0.09245045490416826\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:39:01.569313\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.15684101073180928 updated_ts_MRAE 0.1568401403286878 updated_ts_RMSE 0.034625929177683944 updated_ts_SAM 6.214045814439362 updated_ts_SID 0.06467533257662081 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.15684101073180928 updated_ts_MRAE 0.1568401403286878 updated_ts_RMSE 0.034625929177683944 updated_ts_SAM 6.214045814439362 updated_ts_SID 0.06467533257662081 LR 9.604e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.15684101073180928 updated_ts_MRAE 0.1568401403286878 updated_ts_RMSE 0.034625929177683944 updated_ts_SAM 6.214045814439362 updated_ts_SID 0.06467533257662081 LR 9.604e-05\n",
            "fold: 0, Epoch:139, Iter:14039, Time:5.951058387756348, learning rate:9.604e-05, Train loss:0.1731571876028977, Train MRAE:0.17315616857002278, Train RMSE:0.037281711392178396, Train SAM:6.416124183352631, Train SID:0.07471658400084713\n",
            "Test loss:0.15684101073180928, Test MRAE:0.1568401403286878, Test RMSE:0.034625929177683944, Test SAM:6.214045814439362, Test SID:0.06467533257662081\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:39:11.045529\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:140, Iter:14140, Time:5.949328660964966, learning rate:9.604e-05, Train loss:0.17424351168741095, Train MRAE:0.1742428366353016, Train RMSE:0.03767139542073306, Train SAM:6.57288130675212, Train SID:0.07777356537114276\n",
            "Test loss:0.1707375961191514, Test MRAE:0.1707360133820889, Test RMSE:0.03603156440544362, Test SAM:6.558648988312366, Test SID:0.0712211070691838\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:39:20.424340\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.17136670751314537 updated_ts_MRAE 0.17136563419126996 updated_ts_RMSE 0.03497171533458373 updated_ts_SAM 6.29485699709724 updated_ts_SID 0.06443716421285096 LR 9.604e-05\n",
            "fold: 0, Epoch:141, Iter:14241, Time:5.956250190734863, learning rate:9.604e-05, Train loss:0.18276498437222868, Train MRAE:0.18276392587340703, Train RMSE:0.037885724778960246, Train SAM:6.7104106893633855, Train SID:0.07793832269709298\n",
            "Test loss:0.17136670751314537, Test MRAE:0.17136563419126996, Test RMSE:0.03497171533458373, Test SAM:6.29485699709724, Test SID:0.06443716421285096\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:39:29.872823\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.1607655964645685 updated_ts_MRAE 0.16076441518231935 updated_ts_RMSE 0.034523822915028125 updated_ts_SAM 6.136207253325219 updated_ts_SID 0.058776092733822616 LR 9.604e-05\n",
            "fold: 0, Epoch:142, Iter:14342, Time:5.96093225479126, learning rate:9.604e-05, Train loss:0.18030887178265223, Train MRAE:0.18030792976369953, Train RMSE:0.03786311034887734, Train SAM:6.583491391474658, Train SID:0.07743083523346646\n",
            "Test loss:0.1607655964645685, Test MRAE:0.16076441518231935, Test RMSE:0.034523822915028125, Test SAM:6.136207253325219, Test SID:0.058776092733822616\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:39:39.363796\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.1637042592845711 updated_ts_MRAE 0.16370341941422106 updated_ts_RMSE 0.03541860488407752 updated_ts_SAM 6.37773759692323 updated_ts_SID 0.05722264876114387 LR 9.604e-05\n",
            "fold: 0, Epoch:143, Iter:14443, Time:5.962537050247192, learning rate:9.604e-05, Train loss:0.17288008066687252, Train MRAE:0.17287880565860483, Train RMSE:0.03712606221658758, Train SAM:6.177903977951201, Train SID:0.06202611113243764\n",
            "Test loss:0.1637042592845711, Test MRAE:0.16370341941422106, Test RMSE:0.03541860488407752, Test SAM:6.37773759692323, Test SID:0.05722264876114387\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:39:48.861531\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.16202472413287444 updated_ts_MRAE 0.1620239844801379 updated_ts_RMSE 0.035140309592380244 updated_ts_SAM 6.1864600275077075 updated_ts_SID 0.05046173444419515 LR 9.604e-05\n",
            "fold: 0, Epoch:144, Iter:14544, Time:5.9664626121521, learning rate:9.604e-05, Train loss:0.16720633105476304, Train MRAE:0.16720529689942257, Train RMSE:0.03622527237944674, Train SAM:6.171711633701135, Train SID:0.056775921937262656\n",
            "Test loss:0.16202472413287444, Test MRAE:0.1620239844801379, Test RMSE:0.035140309592380244, Test SAM:6.1864600275077075, Test SID:0.05046173444419515\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:39:58.290642\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:145, Iter:14645, Time:5.970077037811279, learning rate:9.604e-05, Train loss:0.17012448638382524, Train MRAE:0.17012329584006036, Train RMSE:0.03725121684815034, Train SAM:6.268320697368962, Train SID:0.058168238194862214\n",
            "Test loss:0.16215221715324066, Test MRAE:0.1621511330207189, Test RMSE:0.03527437183348572, Test SAM:6.505507160635555, Test SID:0.05789062534185017\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:40:07.679393\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:146, Iter:14746, Time:5.975848197937012, learning rate:9.604e-05, Train loss:0.17609461824787725, Train MRAE:0.17609392113909864, Train RMSE:0.03711106655842597, Train SAM:6.453422607761799, Train SID:0.06306076390982264\n",
            "Test loss:0.17344212254472807, Test MRAE:0.17344164570756987, Test RMSE:0.03426550010986188, Test SAM:5.996779002395331, Test SID:0.05153044996162256\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:40:17.139153\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.1570401764383503 updated_ts_MRAE 0.1570396876218272 updated_ts_RMSE 0.034678174621042085 updated_ts_SAM 6.160956868938372 updated_ts_SID 0.048569390988525224 LR 9.604e-05\n",
            "fold: 0, Epoch:147, Iter:14847, Time:5.961034059524536, learning rate:9.604e-05, Train loss:0.1831355172366199, Train MRAE:0.18313422577806038, Train RMSE:0.03719090203763825, Train SAM:6.4316032334129405, Train SID:0.06394379049318262\n",
            "Test loss:0.1570401764383503, Test MRAE:0.1570396876218272, Test RMSE:0.034678174621042085, Test SAM:6.160956868938372, Test SID:0.048569390988525224\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:40:26.579473\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.15299097258670658 updated_ts_MRAE 0.1529903987459108 updated_ts_RMSE 0.03339029611179642 updated_ts_SAM 5.754113505868351 updated_ts_SID 0.0444638991326678 LR 9.604e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.15299097258670658 updated_ts_MRAE 0.1529903987459108 updated_ts_RMSE 0.03339029611179642 updated_ts_SAM 5.754113505868351 updated_ts_SID 0.0444638991326678 LR 9.604e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.15299097258670658 updated_ts_MRAE 0.1529903987459108 updated_ts_RMSE 0.03339029611179642 updated_ts_SAM 5.754113505868351 updated_ts_SID 0.0444638991326678 LR 9.604e-05\n",
            "fold: 0, Epoch:148, Iter:14948, Time:5.963868618011475, learning rate:9.604e-05, Train loss:0.1745923607066126, Train MRAE:0.17459145099809853, Train RMSE:0.036138953198448266, Train SAM:6.197004785632143, Train SID:0.058849101461986505\n",
            "Test loss:0.15299097258670658, Test MRAE:0.1529903987459108, Test RMSE:0.03339029611179642, Test SAM:5.754113505868351, Test SID:0.0444638991326678\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:40:36.081995\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:149, Iter:15049, Time:5.963822603225708, learning rate:9.604e-05, Train loss:0.1730615809412286, Train MRAE:0.17306069306807942, Train RMSE:0.0365414115056248, Train SAM:6.346934788297899, Train SID:0.06266766028607836\n",
            "Test loss:0.158577841870925, Test MRAE:0.15857669053708806, Test RMSE:0.034192908226567155, Test SAM:6.03694122912837, Test SID:0.04943082587537812\n",
            "w1: 1, w2: 0\n",
            "('updated learning rate', 9.41192e-05)\n",
            "num.101\n",
            "2022-08-05 17:40:45.544372\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:150, Iter:15150, Time:5.977944374084473, learning rate:9.41192e-05, Train loss:0.16960301552668655, Train MRAE:0.16960199582989854, Train RMSE:0.037172460209320086, Train SAM:6.111305227374086, Train SID:0.05701280398165236\n",
            "Test loss:0.16314473806643018, Test MRAE:0.16314294744356006, Test RMSE:0.03471490565468283, Test SAM:5.870028692133286, Test SID:0.04813092884918054\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:40:55.019528\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:151, Iter:15251, Time:5.9586310386657715, learning rate:9.41192e-05, Train loss:0.17344741877352837, Train MRAE:0.17344623051657534, Train RMSE:0.03689546104852516, Train SAM:6.261539029602957, Train SID:0.059177623176486185\n",
            "Test loss:0.1542629514839135, Test MRAE:0.15426184485356012, Test RMSE:0.033419079092495585, Test SAM:5.820917615703508, Test SID:0.047457404252068666\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:41:04.413447\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:152, Iter:15352, Time:5.963279485702515, learning rate:9.41192e-05, Train loss:0.16309892570618356, Train MRAE:0.16309795794215534, Train RMSE:0.03551967301876238, Train SAM:5.953597854859758, Train SID:0.05340012682989092\n",
            "Test loss:0.15574707266162424, Test MRAE:0.1557461373946246, Test RMSE:0.033940684817293114, Test SAM:5.952072489495371, Test SID:0.04717315660387862\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:41:13.840596\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:153, Iter:15453, Time:5.9802165031433105, learning rate:9.41192e-05, Train loss:0.16905471239939773, Train MRAE:0.16905382924740858, Train RMSE:0.035741945193840725, Train SAM:6.152083545628161, Train SID:0.056386875136211365\n",
            "Test loss:0.157785699355836, Test MRAE:0.1577840239978304, Test RMSE:0.033339873577157654, Test SAM:5.875514815835392, Test SID:0.04965537479695152\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:41:23.311080\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:154, Iter:15554, Time:5.972261667251587, learning rate:9.41192e-05, Train loss:0.17438015998295037, Train MRAE:0.17437838962172517, Train RMSE:0.03702645435339153, Train SAM:6.528931985987295, Train SID:0.06444843100512972\n",
            "Test loss:0.16042583947088204, Test MRAE:0.16042538644636378, Test RMSE:0.03560034627569657, Test SAM:6.245514346104042, Test SID:0.053750638174367885\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:41:32.765390\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.1528422939134579 updated_ts_MRAE 0.15284186601638794 updated_ts_RMSE 0.03437869889917327 updated_ts_SAM 6.0523905099606985 updated_ts_SID 0.04837474215994863 LR 9.41192e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.1528422939134579 updated_ts_MRAE 0.15284186601638794 updated_ts_RMSE 0.03437869889917327 updated_ts_SAM 6.0523905099606985 updated_ts_SID 0.04837474215994863 LR 9.41192e-05\n",
            "fold: 0, Epoch:155, Iter:15655, Time:5.977534294128418, learning rate:9.41192e-05, Train loss:0.1626476321627598, Train MRAE:0.16264670600395392, Train RMSE:0.03567788820012961, Train SAM:6.0272645855894185, Train SID:0.05355880780163968\n",
            "Test loss:0.1528422939134579, Test MRAE:0.15284186601638794, Test RMSE:0.03437869889917327, Test SAM:6.0523905099606985, Test SID:0.04837474215994863\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:41:42.280375\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.15152182985170215 updated_ts_MRAE 0.15152064184932149 updated_ts_RMSE 0.033110048914072564 updated_ts_SAM 5.7648444830202585 updated_ts_SID 0.046514053262916265 LR 9.41192e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.15152182985170215 updated_ts_MRAE 0.15152064184932149 updated_ts_RMSE 0.033110048914072564 updated_ts_SAM 5.7648444830202585 updated_ts_SID 0.046514053262916265 LR 9.41192e-05\n",
            "fold: 0, Epoch:156, Iter:15756, Time:5.97667670249939, learning rate:9.41192e-05, Train loss:0.17693553466608028, Train MRAE:0.17693475331410324, Train RMSE:0.03684204446133411, Train SAM:6.196030550664014, Train SID:0.06208850994779922\n",
            "Test loss:0.15152182985170215, Test MRAE:0.15152064184932149, Test RMSE:0.033110048914072564, Test SAM:5.7648444830202585, Test SID:0.046514053262916265\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:41:51.774788\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:157, Iter:15857, Time:5.9816060066223145, learning rate:9.41192e-05, Train loss:0.16187069476536003, Train MRAE:0.16186972655872306, Train RMSE:0.035222834400316275, Train SAM:5.727977632295968, Train SID:0.050805154321069766\n",
            "Test loss:0.20553061585215962, Test MRAE:0.20552999876877842, Test RMSE:0.0412153799963348, Test SAM:7.370149864869959, Test SID:0.08863976612394932\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:42:01.205968\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.15686370315505008 updated_ts_MRAE 0.15686323157712526 updated_ts_RMSE 0.032559450094898544 updated_ts_SAM 5.489270257014854 updated_ts_SID 0.04155435785651207 LR 9.41192e-05\n",
            "fold: 0, Epoch:158, Iter:15958, Time:5.975492477416992, learning rate:9.41192e-05, Train loss:0.1726118094555222, Train MRAE:0.17261092460686617, Train RMSE:0.03659711857464644, Train SAM:6.276537408923159, Train SID:0.06115542656493069\n",
            "Test loss:0.15686370315505008, Test MRAE:0.15686323157712526, Test RMSE:0.032559450094898544, Test SAM:5.489270257014854, Test SID:0.04155435785651207\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:42:10.681554\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:159, Iter:16059, Time:5.99285101890564, learning rate:9.41192e-05, Train loss:0.17164706151084144, Train MRAE:0.1716459268833151, Train RMSE:0.036666512876601505, Train SAM:6.107191246334869, Train SID:0.05749139890517339\n",
            "Test loss:0.1897752957016814, Test MRAE:0.18977349645951214, Test RMSE:0.0373980896730049, Test SAM:7.408919278313132, Test SID:0.08738724894675554\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:42:20.142413\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:160, Iter:16160, Time:5.971672773361206, learning rate:9.41192e-05, Train loss:0.17883157184218415, Train MRAE:0.1788305004339407, Train RMSE:0.03630218138494114, Train SAM:6.235098706613673, Train SID:0.060432579110164454\n",
            "Test loss:0.16642668624134624, Test MRAE:0.1664251797631675, Test RMSE:0.03527548620659931, Test SAM:6.651424594953949, Test SID:0.05969897416584632\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:42:29.623965\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:161, Iter:16261, Time:5.984935998916626, learning rate:9.41192e-05, Train loss:0.16965173413552861, Train MRAE:0.1696507936657065, Train RMSE:0.03594673240538871, Train SAM:6.131029917462038, Train SID:0.056889842292017274\n",
            "Test loss:0.16007876995147444, Test MRAE:0.16007787252173705, Test RMSE:0.03324511607049727, Test SAM:5.8947047626270965, Test SID:0.044518025884149125\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:42:39.111048\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:162, Iter:16362, Time:5.987332582473755, learning rate:9.41192e-05, Train loss:0.16665581805576193, Train MRAE:0.16665441860066782, Train RMSE:0.03527620386811766, Train SAM:5.980629429958834, Train SID:0.053805695385626046\n",
            "Test loss:0.15606506066579445, Test MRAE:0.15606435096147014, Test RMSE:0.03367347540516479, Test SAM:5.9449524037978225, Test SID:0.04914665784613759\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:42:48.553804\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:163, Iter:16463, Time:5.97707462310791, learning rate:9.41192e-05, Train loss:0.16298006283174646, Train MRAE:0.16297887287812657, Train RMSE:0.03508341135364948, Train SAM:5.889099841070648, Train SID:0.05258017612418326\n",
            "Test loss:0.15295154236110986, Test MRAE:0.15295004450223026, Test RMSE:0.033441231823435016, Test SAM:5.883067907071581, Test SID:0.046183514353983546\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:42:58.033090\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:164, Iter:16564, Time:5.9767632484436035, learning rate:9.41192e-05, Train loss:0.1649785946206291, Train MRAE:0.16497750506542697, Train RMSE:0.035260530483752196, Train SAM:6.024722590304838, Train SID:0.05493951295640799\n",
            "Test loss:0.17668658538776286, Test MRAE:0.17668429119329826, Test RMSE:0.03609730883994523, Test SAM:6.015910242118087, Test SID:0.04873583798168921\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:43:07.462312\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:165, Iter:16665, Time:5.980728626251221, learning rate:9.41192e-05, Train loss:0.16306020289954573, Train MRAE:0.16305920835768822, Train RMSE:0.03532732267043378, Train SAM:5.770708265871105, Train SID:0.051072590071523545\n",
            "Test loss:0.16766194078852148, Test MRAE:0.1676605105692265, Test RMSE:0.03462373794001691, Test SAM:6.560370735093659, Test SID:0.06677488091529585\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:43:16.843038\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:166, Iter:16766, Time:5.97761082649231, learning rate:9.41192e-05, Train loss:0.16934077751518475, Train MRAE:0.16933989967450058, Train RMSE:0.036045908393098576, Train SAM:6.008269706574997, Train SID:0.056246162198557714\n",
            "Test loss:0.1576960883012005, Test MRAE:0.15769361120228673, Test RMSE:0.03450316128631433, Test SAM:6.2479179700215655, Test SID:0.054758186470351966\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:43:26.294943\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:167, Iter:16867, Time:5.98116135597229, learning rate:9.41192e-05, Train loss:0.1612898768617375, Train MRAE:0.1612886889736251, Train RMSE:0.035141573407419836, Train SAM:5.726481371586866, Train SID:0.0498742664804553\n",
            "Test loss:0.1624127340083029, Test MRAE:0.16241208799913817, Test RMSE:0.03283213765597811, Test SAM:5.681048842037425, Test SID:0.044093722879302265\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:43:35.761515\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:168, Iter:16968, Time:5.9812257289886475, learning rate:9.41192e-05, Train loss:0.17640560061329663, Train MRAE:0.1764044546992472, Train RMSE:0.03612916505351515, Train SAM:6.203062461154295, Train SID:0.06132012220228662\n",
            "Test loss:0.15238722516041175, Test MRAE:0.1523869366330259, Test RMSE:0.033457894097356236, Test SAM:5.687055989807727, Test SID:0.04572153898576895\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:43:47.649122\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.14903594363553851 updated_ts_MRAE 0.1490351439398878 updated_ts_RMSE 0.03355573764180436 updated_ts_SAM 5.745266152363198 updated_ts_SID 0.04506698250770569 LR 9.41192e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.14903594363553851 updated_ts_MRAE 0.1490351439398878 updated_ts_RMSE 0.03355573764180436 updated_ts_SAM 5.745266152363198 updated_ts_SID 0.04506698250770569 LR 9.41192e-05\n",
            "fold: 0, Epoch:169, Iter:17069, Time:5.998839855194092, learning rate:9.41192e-05, Train loss:0.16379518629890857, Train MRAE:0.16379382461309433, Train RMSE:0.035091786285733235, Train SAM:5.90863031443983, Train SID:0.05318619349584131\n",
            "Test loss:0.14903594363553851, Test MRAE:0.1490351439398878, Test RMSE:0.03355573764180436, Test SAM:5.745266152363198, Test SID:0.04506698250770569\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:43:57.390621\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:170, Iter:17170, Time:5.981727123260498, learning rate:9.41192e-05, Train loss:0.16129663712022327, Train MRAE:0.161295357021955, Train RMSE:0.03478423203572188, Train SAM:5.781738625894679, Train SID:0.0504123141417409\n",
            "Test loss:0.15174006787585279, Test MRAE:0.15173971872119343, Test RMSE:0.03421685165342163, Test SAM:5.645543673459222, Test SID:0.04334589992375935\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:44:06.810744\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:171, Iter:17271, Time:5.986638307571411, learning rate:9.41192e-05, Train loss:0.15790875049510805, Train MRAE:0.15790742798016802, Train RMSE:0.03479414641635843, Train SAM:5.657310181325025, Train SID:0.04790168323803066\n",
            "Test loss:0.14937456741052516, Test MRAE:0.14937399152447195, Test RMSE:0.03340715593567081, Test SAM:5.7751036812277405, Test SID:0.044016773830734046\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:44:16.294949\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.14670005177750306 updated_ts_MRAE 0.1466993536142742 updated_ts_RMSE 0.032397988380170335 updated_ts_SAM 5.615156842213051 updated_ts_SID 0.04166719364915408 LR 9.41192e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.14670005177750306 updated_ts_MRAE 0.1466993536142742 updated_ts_RMSE 0.032397988380170335 updated_ts_SAM 5.615156842213051 updated_ts_SID 0.04166719364915408 LR 9.41192e-05\n",
            "fold: 0, Epoch:172, Iter:17372, Time:5.981874227523804, learning rate:9.41192e-05, Train loss:0.15639253654102286, Train MRAE:0.1563912132884016, Train RMSE:0.03404584147109844, Train SAM:5.575525043034317, Train SID:0.04787150377610532\n",
            "Test loss:0.14670005177750306, Test MRAE:0.1466993536142742, Test RMSE:0.032397988380170335, Test SAM:5.615156842213051, Test SID:0.04166719364915408\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:44:25.811525\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.1439848271947281 updated_ts_MRAE 0.14398411442251766 updated_ts_RMSE 0.0322381839813555 updated_ts_SAM 5.352142520979339 updated_ts_SID 0.03900388281281088 LR 9.41192e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.1439848271947281 updated_ts_MRAE 0.14398411442251766 updated_ts_RMSE 0.0322381839813555 updated_ts_SAM 5.352142520979339 updated_ts_SID 0.03900388281281088 LR 9.41192e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.1439848271947281 updated_ts_MRAE 0.14398411442251766 updated_ts_RMSE 0.0322381839813555 updated_ts_SAM 5.352142520979339 updated_ts_SID 0.03900388281281088 LR 9.41192e-05\n",
            "fold: 0, Epoch:173, Iter:17473, Time:5.985839605331421, learning rate:9.41192e-05, Train loss:0.15861115825943428, Train MRAE:0.15861029229541818, Train RMSE:0.0347502888503051, Train SAM:5.7340090203993395, Train SID:0.04989463808291619\n",
            "Test loss:0.1439848271947281, Test MRAE:0.14398411442251766, Test RMSE:0.0322381839813555, Test SAM:5.352142520979339, Test SID:0.03900388281281088\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:44:35.388670\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:174, Iter:17574, Time:5.995126724243164, learning rate:9.41192e-05, Train loss:0.1617051419792789, Train MRAE:0.16170399636030197, Train RMSE:0.03512103999457737, Train SAM:5.961714196913313, Train SID:0.052321304816125645\n",
            "Test loss:0.14439812068845712, Test MRAE:0.14439689105047898, Test RMSE:0.03210767475413341, Test SAM:5.434029443591249, Test SID:0.040937885569006786\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:44:44.884001\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:175, Iter:17675, Time:5.992360830307007, learning rate:9.41192e-05, Train loss:0.15747384771262066, Train MRAE:0.15747286563757623, Train RMSE:0.034540057200754044, Train SAM:5.630185500229939, Train SID:0.04827992201294049\n",
            "Test loss:0.1458925582906779, Test MRAE:0.1458907837376875, Test RMSE:0.0322052804643617, Test SAM:5.548526590945674, Test SID:0.04232760412874175\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:44:54.361222\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:176, Iter:17776, Time:5.985258102416992, learning rate:9.41192e-05, Train loss:0.15557939640366206, Train MRAE:0.15557833805237667, Train RMSE:0.03470001816011892, Train SAM:5.580869889495396, Train SID:0.0465477285630042\n",
            "Test loss:0.1598498597741127, Test MRAE:0.15984821421842949, Test RMSE:0.034414192004238855, Test SAM:6.57028715283263, Test SID:0.05472740512706485\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:45:03.823894\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:177, Iter:17877, Time:6.0083818435668945, learning rate:9.41192e-05, Train loss:0.15907087051632382, Train MRAE:0.15906937007266697, Train RMSE:0.034340696649091076, Train SAM:5.825461633134596, Train SID:0.051094525631996665\n",
            "Test loss:0.14905648766195073, Test MRAE:0.1490562358031086, Test RMSE:0.033124125471302106, Test SAM:5.657793891196158, Test SID:0.042396891825631555\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:45:13.334813\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.1428138812383016 updated_ts_MRAE 0.14281251471416623 updated_ts_RMSE 0.031345950950886686 updated_ts_SAM 5.266126090405034 updated_ts_SID 0.03731927812537726 LR 9.41192e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.1428138812383016 updated_ts_MRAE 0.14281251471416623 updated_ts_RMSE 0.031345950950886686 updated_ts_SAM 5.266126090405034 updated_ts_SID 0.03731927812537726 LR 9.41192e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.1428138812383016 updated_ts_MRAE 0.14281251471416623 updated_ts_RMSE 0.031345950950886686 updated_ts_SAM 5.266126090405034 updated_ts_SID 0.03731927812537726 LR 9.41192e-05\n",
            "fold: 0, Epoch:178, Iter:17978, Time:5.990985631942749, learning rate:9.41192e-05, Train loss:0.160593382940434, Train MRAE:0.1605920437538978, Train RMSE:0.03472255718073632, Train SAM:5.75319258765419, Train SID:0.05060033724265228\n",
            "Test loss:0.1428138812383016, Test MRAE:0.14281251471416623, Test RMSE:0.031345950950886686, Test SAM:5.266126090405034, Test SID:0.03731927812537726\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:45:22.873518\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:179, Iter:18079, Time:6.002735614776611, learning rate:9.41192e-05, Train loss:0.15930369422577395, Train MRAE:0.15930229159865048, Train RMSE:0.03463581654399928, Train SAM:5.732506279898162, Train SID:0.050362964262171545\n",
            "Test loss:0.14690266973247715, Test MRAE:0.146902241981497, Test RMSE:0.03322285567136372, Test SAM:5.548649498060638, Test SID:0.040856998596413466\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:45:32.414662\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:180, Iter:18180, Time:6.008517742156982, learning rate:9.41192e-05, Train loss:0.1598401363975931, Train MRAE:0.1598387587778639, Train RMSE:0.034209711495602486, Train SAM:5.764244572951062, Train SID:0.049872884084240045\n",
            "Test loss:0.14517930325339823, Test MRAE:0.14517897557394177, Test RMSE:0.03283708491453938, Test SAM:5.280585298351213, Test SID:0.03843949614640545\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:45:41.974394\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.14767196628392912 updated_ts_MRAE 0.14767075403063906 updated_ts_RMSE 0.03199675960429743 updated_ts_SAM 5.265356624827666 updated_ts_SID 0.03729286771632877 LR 9.41192e-05\n",
            "fold: 0, Epoch:181, Iter:18281, Time:6.009968042373657, learning rate:9.41192e-05, Train loss:0.1549691500345079, Train MRAE:0.15496814656670732, Train RMSE:0.03411956208914813, Train SAM:5.5210691253737645, Train SID:0.046974537451521005\n",
            "Test loss:0.14767196628392912, Test MRAE:0.14767075403063906, Test RMSE:0.03199675960429743, Test SAM:5.265356624827666, Test SID:0.03729286771632877\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:45:51.522575\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:182, Iter:18382, Time:5.997674942016602, learning rate:9.41192e-05, Train loss:0.15504123845903, Train MRAE:0.1550400875287481, Train RMSE:0.03402415948855405, Train SAM:5.416766858336949, Train SID:0.04502152140704122\n",
            "Test loss:0.1500018817536971, Test MRAE:0.15000109432959088, Test RMSE:0.031131689601084766, Test SAM:5.237813061358882, Test SID:0.03781310114644322\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:46:01.079095\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:183, Iter:18483, Time:5.992197275161743, learning rate:9.41192e-05, Train loss:0.15643286373060528, Train MRAE:0.1564317999942468, Train RMSE:0.03375553251198023, Train SAM:5.61358361905164, Train SID:0.04809907032637903\n",
            "Test loss:0.1516875144021184, Test MRAE:0.1516867937410579, Test RMSE:0.03243532645351747, Test SAM:5.5356578639909335, Test SID:0.04487618939111046\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:46:10.597398\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:184, Iter:18584, Time:6.001195192337036, learning rate:9.41192e-05, Train loss:0.1573548598749803, Train MRAE:0.15735405594995705, Train RMSE:0.03440470951102158, Train SAM:5.612530838144888, Train SID:0.04889548242571625\n",
            "Test loss:0.14682470232832665, Test MRAE:0.1468235781671954, Test RMSE:0.03180430931787865, Test SAM:5.407839503942752, Test SID:0.04272462701534524\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:46:20.068692\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:185, Iter:18685, Time:6.00343656539917, learning rate:9.41192e-05, Train loss:0.15902857733244943, Train MRAE:0.15902694314718246, Train RMSE:0.0340483605972316, Train SAM:5.6314961060439, Train SID:0.04924159593843293\n",
            "Test loss:0.15240484549134387, Test MRAE:0.15240424813008777, Test RMSE:0.033123025378467987, Test SAM:6.226744548947203, Test SID:0.048021025079138135\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:46:29.563062\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:186, Iter:18786, Time:5.997813701629639, learning rate:9.41192e-05, Train loss:0.15217829185842288, Train MRAE:0.15217702902189575, Train RMSE:0.03349982919448083, Train SAM:5.5253837745968655, Train SID:0.04569202739892915\n",
            "Test loss:0.14496575734194586, Test MRAE:0.14496536450643166, Test RMSE:0.033052280216532594, Test SAM:5.224696561401966, Test SID:0.037506380410609295\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:46:39.028966\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:187, Iter:18887, Time:6.006130933761597, learning rate:9.41192e-05, Train loss:0.1685410815447864, Train MRAE:0.16853990959058893, Train RMSE:0.0347961577780471, Train SAM:5.913727408588523, Train SID:0.05461172594476749\n",
            "Test loss:0.16942960754329084, Test MRAE:0.1694285117235838, Test RMSE:0.03609837883827733, Test SAM:6.367706214680391, Test SID:0.05846924214240383\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:46:48.543177\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:188, Iter:18988, Time:6.002360582351685, learning rate:9.41192e-05, Train loss:0.16539435649272238, Train MRAE:0.1653931513428688, Train RMSE:0.03409215887215468, Train SAM:5.800721000916887, Train SID:0.05193070656075926\n",
            "Test loss:0.14768803134268405, Test MRAE:0.14768708321978063, Test RMSE:0.03197063461822622, Test SAM:5.569470255982642, Test SID:0.043548165911845134\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:46:58.028225\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.1396089315998788 updated_ts_MRAE 0.13960823475145825 updated_ts_RMSE 0.0312532773424013 updated_ts_SAM 5.185982026305854 updated_ts_SID 0.03756881340899888 LR 9.41192e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.1396089315998788 updated_ts_MRAE 0.13960823475145825 updated_ts_RMSE 0.0312532773424013 updated_ts_SAM 5.185982026305854 updated_ts_SID 0.03756881340899888 LR 9.41192e-05\n",
            "fold: 0, Epoch:189, Iter:19089, Time:5.992730379104614, learning rate:9.41192e-05, Train loss:0.15213429632753428, Train MRAE:0.15213306203927143, Train RMSE:0.03321213926197869, Train SAM:5.3991720039065525, Train SID:0.044922257930335434\n",
            "Test loss:0.1396089315998788, Test MRAE:0.13960823475145825, Test RMSE:0.0312532773424013, Test SAM:5.185982026305854, Test SID:0.03756881340899888\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:47:07.611284\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:190, Iter:19190, Time:6.0188727378845215, learning rate:9.41192e-05, Train loss:0.15647816547367832, Train MRAE:0.1564770210349914, Train RMSE:0.03353216080456087, Train SAM:5.466850696223797, Train SID:0.04520337211687376\n",
            "Test loss:0.15194382810709522, Test MRAE:0.15194308246467628, Test RMSE:0.03124696582409681, Test SAM:5.2939164685268025, Test SID:0.03748219135199107\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:47:17.192964\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:191, Iter:19291, Time:6.0039074420928955, learning rate:9.41192e-05, Train loss:0.15682603868812617, Train MRAE:0.15682467582202195, Train RMSE:0.03435557571673157, Train SAM:5.590156550454621, Train SID:0.047013020603963646\n",
            "Test loss:0.15241127212842306, Test MRAE:0.15241097556609734, Test RMSE:0.034366952839727495, Test SAM:5.338191317576988, Test SID:0.039547826400866695\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:47:26.706439\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:192, Iter:19392, Time:6.024500608444214, learning rate:9.41192e-05, Train loss:0.15244138122785209, Train MRAE:0.15244049992006603, Train RMSE:0.033871578071081994, Train SAM:5.386038874635602, Train SID:0.04449971031286929\n",
            "Test loss:0.14203990411524678, Test MRAE:0.14203958943778394, Test RMSE:0.03174440423939742, Test SAM:5.468637728223614, Test SID:0.03915965045784034\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:47:36.227348\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:193, Iter:19493, Time:6.00842809677124, learning rate:9.41192e-05, Train loss:0.15538058688144873, Train MRAE:0.15537955324248512, Train RMSE:0.03391880501467403, Train SAM:5.628411632953304, Train SID:0.04762656878827527\n",
            "Test loss:0.14787413705797756, Test MRAE:0.14787299156773323, Test RMSE:0.033245898089280315, Test SAM:5.750926901312435, Test SID:0.0457053162595805\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:47:45.745856\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:194, Iter:19594, Time:6.004406690597534, learning rate:9.41192e-05, Train loss:0.15853338651727922, Train MRAE:0.15853237766440553, Train RMSE:0.03368553294256182, Train SAM:5.464231753113246, Train SID:0.046442674795663594\n",
            "Test loss:0.15263640456924252, Test MRAE:0.15263579566689098, Test RMSE:0.033697636688456815, Test SAM:5.725117066327264, Test SID:0.04308238154386773\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:47:55.250204\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.14866066446491316 updated_ts_MRAE 0.14865957521924786 updated_ts_RMSE 0.03156610625777759 updated_ts_SAM 5.212547909979727 updated_ts_SID 0.036038044493134115 LR 9.41192e-05\n",
            "fold: 0, Epoch:195, Iter:19695, Time:6.009316921234131, learning rate:9.41192e-05, Train loss:0.1472077604272578, Train MRAE:0.14720675681192097, Train RMSE:0.03297982763240833, Train SAM:5.299540137300397, Train SID:0.04194019969995364\n",
            "Test loss:0.14866066446491316, Test MRAE:0.14865957521924786, Test RMSE:0.03156610625777759, Test SAM:5.212547909979727, Test SID:0.036038044493134115\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:48:04.814004\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:196, Iter:19796, Time:6.003427982330322, learning rate:9.41192e-05, Train loss:0.14990840335883718, Train MRAE:0.14990754514047416, Train RMSE:0.03314169444660149, Train SAM:5.309134617890462, Train SID:0.044290595226875035\n",
            "Test loss:0.14800696235661412, Test MRAE:0.14800483178274304, Test RMSE:0.03187467457324851, Test SAM:5.428737238341687, Test SID:0.040087532580775374\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:48:14.328855\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:197, Iter:19897, Time:6.0089805126190186, learning rate:9.41192e-05, Train loss:0.15043971466250938, Train MRAE:0.15043844754743105, Train RMSE:0.0331410870214205, Train SAM:5.380774082523762, Train SID:0.044827510031069266\n",
            "Test loss:0.14352669669132606, Test MRAE:0.14352641634496988, Test RMSE:0.03366457459096815, Test SAM:5.16089253799588, Test SID:0.03688092224811222\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:48:23.819555\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:198, Iter:19998, Time:6.016686916351318, learning rate:9.41192e-05, Train loss:0.15923964910873092, Train MRAE:0.15923878764457042, Train RMSE:0.03403182287174877, Train SAM:5.52323058572146, Train SID:0.0469583747908473\n",
            "Test loss:0.14179629496499604, Test MRAE:0.14179551542973987, Test RMSE:0.03157919922880098, Test SAM:5.420125157225366, Test SID:0.03872636694680242\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:48:33.336373\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:199, Iter:20099, Time:6.0027289390563965, learning rate:9.41192e-05, Train loss:0.14833552073134054, Train MRAE:0.14833418766755868, Train RMSE:0.03284854462167414, Train SAM:5.306830007250946, Train SID:0.04304034625281497\n",
            "Test loss:0.15561175857688866, Test MRAE:0.15561121877501993, Test RMSE:0.03238370086924702, Test SAM:5.389942734849219, Test SID:0.03920759273437308\n",
            "w1: 1, w2: 0\n",
            "('updated learning rate', 9.2236816e-05)\n",
            "num.101\n",
            "2022-08-05 17:48:42.783994\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:200, Iter:20200, Time:6.005702257156372, learning rate:9.2236816e-05, Train loss:0.15339507295353577, Train MRAE:0.1533941825722704, Train RMSE:0.033907242506594944, Train SAM:5.32967747792159, Train SID:0.04396368805687911\n",
            "Test loss:0.15111486233916938, Test MRAE:0.15111278275064394, Test RMSE:0.03354976830237052, Test SAM:5.953796279196646, Test SID:0.048880626538804935\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:48:52.293762\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:201, Iter:20301, Time:6.012391805648804, learning rate:9.2236816e-05, Train loss:0.15114881563009602, Train MRAE:0.15114777675359556, Train RMSE:0.03336420743772299, Train SAM:5.260732062972418, Train SID:0.0422488063444743\n",
            "Test loss:0.14647624524785022, Test MRAE:0.14647560186830222, Test RMSE:0.03242958249414668, Test SAM:5.498143093258727, Test SID:0.04098077198746158\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:49:01.833413\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:202, Iter:20402, Time:6.018571615219116, learning rate:9.2236816e-05, Train loss:0.1586441110708926, Train MRAE:0.15864315001976373, Train RMSE:0.03369092865671852, Train SAM:5.45776895013186, Train SID:0.04606066287338439\n",
            "Test loss:0.16322056116426692, Test MRAE:0.16321919858455658, Test RMSE:0.03202043692855274, Test SAM:5.482098308264041, Test SID:0.042046707507003754\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:49:11.429013\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:203, Iter:20503, Time:6.031985759735107, learning rate:9.2236816e-05, Train loss:0.15237586119092336, Train MRAE:0.15237459769048314, Train RMSE:0.033197794146466966, Train SAM:5.421705715727098, Train SID:0.04565143442279337\n",
            "Test loss:0.14899375231242648, Test MRAE:0.14899330060271657, Test RMSE:0.03126445120456172, Test SAM:5.289489951788211, Test SID:0.03810544750269722\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:49:20.958042\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.13645452991420148 updated_ts_MRAE 0.1364540270730561 updated_ts_RMSE 0.030505958908036642 updated_ts_SAM 4.89711638992908 updated_ts_SID 0.03340770231158126 LR 9.2236816e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.13645452991420148 updated_ts_MRAE 0.1364540270730561 updated_ts_RMSE 0.030505958908036642 updated_ts_SAM 4.89711638992908 updated_ts_SID 0.03340770231158126 LR 9.2236816e-05\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.13645452991420148 updated_ts_MRAE 0.1364540270730561 updated_ts_RMSE 0.030505958908036642 updated_ts_SAM 4.89711638992908 updated_ts_SID 0.03340770231158126 LR 9.2236816e-05\n",
            "fold: 0, Epoch:204, Iter:20604, Time:6.014325380325317, learning rate:9.2236816e-05, Train loss:0.15047291341689553, Train MRAE:0.15047179546096537, Train RMSE:0.032539566573087535, Train SAM:5.251675572725806, Train SID:0.04383121476978949\n",
            "Test loss:0.13645452991420148, Test MRAE:0.1364540270730561, Test RMSE:0.030505958908036642, Test SAM:4.89711638992908, Test SID:0.03340770231158126\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:49:30.589782\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:205, Iter:20705, Time:6.022479295730591, learning rate:9.2236816e-05, Train loss:0.1535943357071074, Train MRAE:0.15359327351987953, Train RMSE:0.03340689795515915, Train SAM:5.335692965158142, Train SID:0.04363584306349259\n",
            "Test loss:0.14141130520432604, Test MRAE:0.14141062252661762, Test RMSE:0.03106921005482767, Test SAM:5.2192195817536, Test SID:0.036622678623626045\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:49:40.181002\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:206, Iter:20806, Time:6.030067682266235, learning rate:9.2236816e-05, Train loss:0.1651073234653709, Train MRAE:0.16510603613782637, Train RMSE:0.03419460431847832, Train SAM:5.6801347496485945, Train SID:0.0495941109475818\n",
            "Test loss:0.14953339114493014, Test MRAE:0.14953268202496509, Test RMSE:0.032110558421004055, Test SAM:5.4167027520198445, Test SID:0.04004158609199757\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:49:49.728692\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:207, Iter:20907, Time:6.0186848640441895, learning rate:9.2236816e-05, Train loss:0.15066410261805696, Train MRAE:0.15066280562688808, Train RMSE:0.03292530066896193, Train SAM:5.2155047053157695, Train SID:0.0424189769875\n",
            "Test loss:0.16181764386448205, Test MRAE:0.161816904650015, Test RMSE:0.03449002620490158, Test SAM:5.79624288222369, Test SID:0.04623738614221414\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:49:59.334405\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:208, Iter:21008, Time:6.02489447593689, learning rate:9.2236816e-05, Train loss:0.14503411048709755, Train MRAE:0.14503303089059225, Train RMSE:0.032464470271721925, Train SAM:5.223476924518548, Train SID:0.04144894951345897\n",
            "Test loss:0.15133943204201905, Test MRAE:0.15133867310542687, Test RMSE:0.031241318831841152, Test SAM:5.114239917081945, Test SID:0.03583301895973729\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:50:11.865796\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:209, Iter:21109, Time:6.028508901596069, learning rate:9.2236816e-05, Train loss:0.14805413922756025, Train MRAE:0.14805305166409746, Train RMSE:0.032475262824999226, Train SAM:5.183015504685959, Train SID:0.04189034007342145\n",
            "Test loss:0.1465225095550219, Test MRAE:0.14652205054082124, Test RMSE:0.03116771425394451, Test SAM:5.221646196701947, Test SID:0.038787690959140364\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:50:21.412714\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:210, Iter:21210, Time:6.0278639793396, learning rate:9.2236816e-05, Train loss:0.1440419103987146, Train MRAE:0.14404059393276084, Train RMSE:0.03248756222131819, Train SAM:5.116077850360681, Train SID:0.04009079838190043\n",
            "Test loss:0.1497521932218589, Test MRAE:0.14975147460605584, Test RMSE:0.03277642498998081, Test SAM:5.397399542378444, Test SID:0.03928203318341106\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:50:30.979788\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:211, Iter:21311, Time:6.017699241638184, learning rate:9.2236816e-05, Train loss:0.15013391863886671, Train MRAE:0.1501322028660538, Train RMSE:0.032319430981089574, Train SAM:5.209605261831, Train SID:0.041171162140251384\n",
            "Test loss:0.1426004188902238, Test MRAE:0.1426000001968122, Test RMSE:0.031739370641755124, Test SAM:5.1533187931659175, Test SID:0.036337403160538156\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:50:40.519503\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:212, Iter:21412, Time:6.035454511642456, learning rate:9.2236816e-05, Train loss:0.1490654296951719, Train MRAE:0.14906408054993883, Train RMSE:0.03279822400890955, Train SAM:5.232200705178894, Train SID:0.0428145694832253\n",
            "Test loss:0.14621669243948132, Test MRAE:0.14621630311012268, Test RMSE:0.03212390142474689, Test SAM:5.2426914794772275, Test SID:0.03912395450706575\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:50:50.036776\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.13987208917444827 updated_ts_MRAE 0.1398714029905843 updated_ts_RMSE 0.030099489233073068 updated_ts_SAM 4.906094064899519 updated_ts_SID 0.03310163284414539 LR 9.2236816e-05\n",
            "fold: 0, Epoch:213, Iter:21513, Time:6.041285991668701, learning rate:9.2236816e-05, Train loss:0.144900813329928, Train MRAE:0.14489957837775203, Train RMSE:0.03240822550683919, Train SAM:5.232131415074415, Train SID:0.04218063641819033\n",
            "Test loss:0.13987208917444827, Test MRAE:0.1398714029905843, Test RMSE:0.030099489233073068, Test SAM:4.906094064899519, Test SID:0.03310163284414539\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:50:59.676351\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.13412168271401348 updated_ts_MRAE 0.13412132800794116 updated_ts_RMSE 0.031003278296659973 updated_ts_SAM 4.935999683305329 updated_ts_SID 0.0336913461523021 LR 9.2236816e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.13412168271401348 updated_ts_MRAE 0.13412132800794116 updated_ts_RMSE 0.031003278296659973 updated_ts_SAM 4.935999683305329 updated_ts_SID 0.0336913461523021 LR 9.2236816e-05\n",
            "fold: 0, Epoch:214, Iter:21614, Time:6.031980752944946, learning rate:9.2236816e-05, Train loss:0.14763367544896533, Train MRAE:0.1476327020785596, Train RMSE:0.03236305645418049, Train SAM:5.252039220073436, Train SID:0.04307821532250336\n",
            "Test loss:0.13412168271401348, Test MRAE:0.13412132800794116, Test RMSE:0.031003278296659973, Test SAM:4.935999683305329, Test SID:0.0336913461523021\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:51:09.281240\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.1419004324604483 updated_ts_MRAE 0.1418990701729176 updated_ts_RMSE 0.03010970865394555 updated_ts_SAM 4.758489524616914 updated_ts_SID 0.03116023175272287 LR 9.2236816e-05\n",
            "fold: 0, Epoch:215, Iter:21715, Time:6.024693965911865, learning rate:9.2236816e-05, Train loss:0.1416675503006076, Train MRAE:0.1416662809724855, Train RMSE:0.031835560198172484, Train SAM:5.059207441783188, Train SID:0.039515826307089615\n",
            "Test loss:0.1419004324604483, Test MRAE:0.1418990701729176, Test RMSE:0.03010970865394555, Test SAM:4.758489524616914, Test SID:0.03116023175272287\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:51:18.885705\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:216, Iter:21816, Time:6.025730848312378, learning rate:9.2236816e-05, Train loss:0.14589543906178806, Train MRAE:0.14589450773921345, Train RMSE:0.03268001075502079, Train SAM:5.03864589540085, Train SID:0.03962373885667265\n",
            "Test loss:0.13916358819194868, Test MRAE:0.13916288666865406, Test RMSE:0.03159921101349242, Test SAM:5.146554119446698, Test SID:0.035885954494861996\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:51:28.438538\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:217, Iter:21917, Time:6.039209365844727, learning rate:9.2236816e-05, Train loss:0.14941762653317783, Train MRAE:0.14941623939736054, Train RMSE:0.032250133628892425, Train SAM:5.209642693547919, Train SID:0.04116267116168643\n",
            "Test loss:0.1446732732595182, Test MRAE:0.14467250891760283, Test RMSE:0.029814312161476005, Test SAM:4.837826649347941, Test SID:0.033607350589305744\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:51:38.012889\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:218, Iter:22018, Time:6.025802373886108, learning rate:9.2236816e-05, Train loss:0.14755105190347917, Train MRAE:0.1475500216578493, Train RMSE:0.03216230603727964, Train SAM:5.190578033428381, Train SID:0.04183053041780644\n",
            "Test loss:0.15979228563168468, Test MRAE:0.15979102867491105, Test RMSE:0.03500652704022679, Test SAM:6.479805076823515, Test SID:0.06205560917071268\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:51:47.538571\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:219, Iter:22119, Time:6.036012649536133, learning rate:9.2236816e-05, Train loss:0.14309835832307835, Train MRAE:0.14309753595602395, Train RMSE:0.032203564881393226, Train SAM:4.981728334238033, Train SID:0.0385885819479233\n",
            "Test loss:0.13417857140302658, Test MRAE:0.13417756133804135, Test RMSE:0.029844587778343874, Test SAM:4.7782366042043645, Test SID:0.03116912455024088\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:51:57.049650\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:220, Iter:22220, Time:6.037309408187866, learning rate:9.2236816e-05, Train loss:0.13996400529205208, Train MRAE:0.13996297216946535, Train RMSE:0.03176494859306529, Train SAM:4.920779476071348, Train SID:0.03738085440553651\n",
            "Test loss:0.1344855160397642, Test MRAE:0.13448399495260388, Test RMSE:0.029954636652095645, Test SAM:4.8485144493626615, Test SID:0.032818520039904354\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:52:06.691762\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:221, Iter:22321, Time:6.02829122543335, learning rate:9.2236816e-05, Train loss:0.14759979935565798, Train MRAE:0.14759893184251124, Train RMSE:0.03263317909792508, Train SAM:5.133533444735083, Train SID:0.04068757003486747\n",
            "Test loss:0.14315586274161057, Test MRAE:0.14315536851976432, Test RMSE:0.030980704680961722, Test SAM:5.181004832772648, Test SID:0.03601093613999147\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:52:16.276334\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:222, Iter:22422, Time:6.042670965194702, learning rate:9.2236816e-05, Train loss:0.14382437579702623, Train MRAE:0.1438231517625327, Train RMSE:0.03223941772069672, Train SAM:5.117101485186284, Train SID:0.040609356947243214\n",
            "Test loss:0.14582247988266103, Test MRAE:0.1458215858129894, Test RMSE:0.030602090681592625, Test SAM:5.034237539066988, Test SID:0.03434509517369317\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:52:25.893030\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:223, Iter:22523, Time:6.056218385696411, learning rate:9.2236816e-05, Train loss:0.15282923824126177, Train MRAE:0.15282829577970033, Train RMSE:0.032996582199293786, Train SAM:5.3587965611184, Train SID:0.044684746736052015\n",
            "Test loss:0.14242286089004255, Test MRAE:0.142421057119089, Test RMSE:0.03181709918905707, Test SAM:5.746917322570202, Test SID:0.041687356636804694\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:52:35.446233\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:224, Iter:22624, Time:6.033002614974976, learning rate:9.2236816e-05, Train loss:0.15419309888735855, Train MRAE:0.1541920519701325, Train RMSE:0.0327611186606164, Train SAM:5.312088241671572, Train SID:0.04512259961649923\n",
            "Test loss:0.14038416582579707, Test MRAE:0.14038379607247373, Test RMSE:0.030476188433228756, Test SAM:5.02962885650934, Test SID:0.035598116178138586\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:52:45.052869\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_SID\n",
            "Saving SID updated_ts_loss 0.138283959060323 updated_ts_MRAE 0.13828221679318184 updated_ts_RMSE 0.03015664822476752 updated_ts_SAM 4.603702993953929 updated_ts_SID 0.0297226177565023 LR 9.2236816e-05\n",
            "fold: 0, Epoch:225, Iter:22725, Time:6.05485200881958, learning rate:9.2236816e-05, Train loss:0.1446603402347848, Train MRAE:0.14465875466271202, Train RMSE:0.03200521072981381, Train SAM:5.147603740786562, Train SID:0.041099515393155046\n",
            "Test loss:0.138283959060323, Test MRAE:0.13828221679318184, Test RMSE:0.03015664822476752, Test SAM:4.603702993953929, Test SID:0.0297226177565023\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:52:54.613500\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:226, Iter:22826, Time:6.042399168014526, learning rate:9.2236816e-05, Train loss:0.14345529255005393, Train MRAE:0.14345387708727675, Train RMSE:0.031694921379042146, Train SAM:5.030392512236491, Train SID:0.038849177004972306\n",
            "Test loss:0.1388186692607169, Test MRAE:0.13881757387927934, Test RMSE:0.030680904968404303, Test SAM:5.176764656515682, Test SID:0.03835881682222381\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:53:04.211405\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:227, Iter:22927, Time:6.035024881362915, learning rate:9.2236816e-05, Train loss:0.14422721010033446, Train MRAE:0.14422600517178527, Train RMSE:0.031818858338612144, Train SAM:5.098060482799417, Train SID:0.040851396702156206\n",
            "Test loss:0.13802092259421067, Test MRAE:0.13802076876163483, Test RMSE:0.0316886039560332, Test SAM:4.951042647455253, Test SID:0.03468692423227955\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:53:13.887290\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "ts_loss\n",
            "Saving total loss updated_ts_loss 0.13331400676100863 updated_ts_MRAE 0.13331309676754707 updated_ts_RMSE 0.02976323017740951 updated_ts_SAM 4.822667939990175 updated_ts_SID 0.033268894835868305 LR 9.2236816e-05\n",
            "ts_MRAE\n",
            "Saving MRAE updated_ts_loss 0.13331400676100863 updated_ts_MRAE 0.13331309676754707 updated_ts_RMSE 0.02976323017740951 updated_ts_SAM 4.822667939990175 updated_ts_SID 0.033268894835868305 LR 9.2236816e-05\n",
            "fold: 0, Epoch:228, Iter:23028, Time:6.047902345657349, learning rate:9.2236816e-05, Train loss:0.1416509611034157, Train MRAE:0.14165008363157217, Train RMSE:0.03213041825312199, Train SAM:4.981422485691486, Train SID:0.03795422526943212\n",
            "Test loss:0.13331400676100863, Test MRAE:0.13331309676754707, Test RMSE:0.02976323017740951, Test SAM:4.822667939990175, Test SID:0.033268894835868305\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:53:23.493114\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:229, Iter:23129, Time:6.062833786010742, learning rate:9.2236816e-05, Train loss:0.15232838070628665, Train MRAE:0.1523277521280959, Train RMSE:0.03260987246464385, Train SAM:5.080024249482863, Train SID:0.04126274247312605\n",
            "Test loss:0.13427309794168846, Test MRAE:0.13427228334487654, Test RMSE:0.029881763071113943, Test SAM:4.932814411088532, Test SID:0.032814394299160034\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:53:33.020618\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:230, Iter:23230, Time:6.048227071762085, learning rate:9.2236816e-05, Train loss:0.14412315175084783, Train MRAE:0.1441215433106564, Train RMSE:0.032020494581596684, Train SAM:4.97726042671959, Train SID:0.03905900780775464\n",
            "Test loss:0.14882086217403412, Test MRAE:0.14882070045260823, Test RMSE:0.03256348224685473, Test SAM:5.589730646096024, Test SID:0.04095696265279662\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:53:42.624107\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:231, Iter:23331, Time:6.057429313659668, learning rate:9.2236816e-05, Train loss:0.15278518406471403, Train MRAE:0.15278406758414637, Train RMSE:0.03249431709473086, Train SAM:5.257330386945517, Train SID:0.04199405664596522\n",
            "Test loss:0.13739196707805, Test MRAE:0.13739086278513366, Test RMSE:0.030320730279473698, Test SAM:5.032930944480148, Test SID:0.03517368802910342\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:53:52.182529\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:232, Iter:23432, Time:6.062414646148682, learning rate:9.2236816e-05, Train loss:0.14518479781575722, Train MRAE:0.14518405637233564, Train RMSE:0.03197281036784153, Train SAM:5.050334304866224, Train SID:0.03995805724274994\n",
            "Test loss:0.13854552323327346, Test MRAE:0.13854489884540147, Test RMSE:0.030674041924523374, Test SAM:5.136641553803986, Test SID:0.0353478730528378\n",
            "w1: 1, w2: 0\n",
            "num.101\n",
            "2022-08-05 17:54:01.775468\n",
            "val_num: 1\n",
            "val_num: 2\n",
            "val_num: 3\n",
            "val_num: 4\n",
            "val_num: 5\n",
            "fold: 0, Epoch:233, Iter:23533, Time:6.052911996841431, learning rate:9.2236816e-05, Train loss:0.13868911173379067, Train MRAE:0.138688260818472, Train RMSE:0.03186609254836446, Train SAM:4.860175366448884, Train SID:0.03626203082391236\n",
            "Test loss:0.14074013338369482, Test MRAE:0.14073829382073647, Test RMSE:0.02989417822191528, Test SAM:5.00308830130334, Test SID:0.03277147893666053\n",
            "w1: 1, w2: 0\n",
            "num.101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7sakvJCYwiQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xDSJQBg8GxY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}